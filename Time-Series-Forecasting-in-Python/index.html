<html>

<head>
    <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
    <link href="style.css" rel="stylesheet" type="text/css" />
    <title>Time Series Forecasting in Python</title>
</head>

<body>
    <div class="calibre" id="calibre_link-0">
        <p><a id="calibre_link-308"></a><img src="images/000124.png" alt="Image 1" class="calibre2" />
        </p>
        <p><img src="images/000139.png" alt="Image 2" class="calibre2" /></p>
        <p>Marco Peixeiro</p>
        <p>M A N N I N G</p>
        <p><a id="calibre_link-320"></a> <i><b>Core concepts for time series forecasting</b></>
        </p>
        <p>Core concept</p>
        <p>Chapter</p>
        <p>Section</p>
        <p>Defining time series</p>
        <p>1</p>
        <p>1.1</p>
        <p>Time series decomposition</p>
        <p>1</p>
        <p>1.1</p>
        <p>Forecasting project lifecycle</p>
        <p>1</p>
        <p>1.2</p>
        <p>Baseline models</p>
        <p>2</p>
        <p>2.1</p>
        <p>Random walk model</p>
        <p>3</p>
        <p>3.1</p>
        <p>Stationarity</p>
        <p>3</p>
        <p>3.2.1</p>
        <p>Differencing</p>
        <p>3</p>
        <p>3.2.1</p>
        <p>Autocorrelation function (ACF)</p>
        <p>3</p>
        <p>3.2.3</p>
        <p>Forecasting a random walk</p>
        <p>3</p>
        <p>3.3</p>
        <p>Moving average model: MA( <i>q</i>)</p>
        <p>4</p>
        <p>4.1</p>
        <p>Reading the ACF plot</p>
        <p>4</p>
        <p>4.1.1</p>
        <p>Forecasting with MA( <i>q</i>)</p>
        <p>4</p>
        <p>4.2</p>
        <p>Autoregressive model: AR( <i>p</i>)</p>
        <p>5</p>
        <p>5.2</p>
        <p>Partial autocorrelation function (PACF)</p>
        <p>5</p>
        <p>5.3.1</p>
        <p>Forecasting with AR( <i>p</i>)</p>
        <p>5</p>
        <p>5.4</p>
        <p>ARMA( <i>p</i>, <i>q</i>) model</p>
        <p>6</p>
        <p>6.2</p>
        <p>General modeling procedure</p>
        <p>6</p>
        <p>6.4</p>
        <p>Akaike Information Criterion (AIC)</p>
        <p>6</p>
        <p>6.4.1</p>
        <p>Q-Q plot</p>
        <p>6</p>
        <p>6.4.3</p>
        <p>Ljung-Box test</p>
        <p>6</p>
        <p>6.4.3</p>
        <p>Residual analysis</p>
        <p>6</p>
        <p>6.4.4</p>
        <p>Forecasting with ARMA( <i>p</i>, <i>q</i>)</p>
        <p>6</p>
        <p>6.6</p>
        <p>ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            model</p>
        <p>7</p>
        <p>7.1</p>
        <p>Forecasting with ARIMA</p>
        <p>7</p>
        <p>7.3</p>
        <p> <i>Continues on inside back cover</i></p>
        <p><a id="calibre_link-321"></a> <i>Time Series Forecasting in Python</i></p>
        <p><a id="calibre_link-322"></a>
            <a id="calibre_link-323"></a> <i>Time Series</i></p>
        <p> <i>Forecasting</i></p>
        <p> <i>in Python</i></p>
        <p>MARCO PEIXEIRO</p>
        <p>M A N N I N G</p>
        <p>SHELTER ISLAND</p>
        <p><a id="calibre_link-324"></a>For online information and ordering of this and other Manning
            books, please visit</p>
        <p><a href="http://www.manning.com">www.manning.com</a>. The publisher offers discounts on this
            book when ordered in quantity. </p>
        <p>For more information, please contact</p>
        <p>Special Sales Department</p>
        <p>Manning Publications Co. </p>
        <p>20 Baldwin Road</p>
        <p>PO Box 761</p>
        <p>Shelter Island, NY 11964</p>
        <p>Email: orders@manning.com</p>
        <p>©2022 by Manning Publications Co. All rights reserved. </p>
        <p>No part of this publication may be reproduced, stored in a retrieval system, or transmitted,
            in any form or by means electronic, mechanical, photocopying, or otherwise, without prior written permission
            of the publisher. </p>
        <p>Many of the designations used by manufacturers and sellers to distinguish their products are
            claimed as trademarks. Where those designations appear in the book, and Manning Publications was aware of a
            trademark claim, the designations have been printed in initial caps or all caps. </p>
        <p>Recognizing the importance of preserving what has been written, it is Manning's policy to
            have the books we publish printed on acid-free paper, and we exert our best efforts to that end. </p>
        <p>Recognizing also our responsibility to conserve the resources of our planet, Manning books
            are printed on paper that is at least 15 percent recycled and processed without the use of elemental
            chlorine. </p>
        <p>The author and publisher have made every effort to ensure that the information in this book
            was correct at press time. The author and publisher do not assume and hereby disclaim any liability to any
            party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions
            result from negligence, accident, or any other cause, or from any usage of the information herein. </p>
        <p>Manning Publications Co. </p>
        <p>Development editor: Bobbie Jennings</p>
        <p>20 Baldwin Road</p>
        <p>Technical development editor: Al Krinker</p>
        <p>PO Box 761</p>
        <p>Review editor: Adriana Sabo</p>
        <p>Shelter Island, NY 11964</p>
        <p>Production editor: Andy Marinkovich</p>
        <p>Copy editor: Andy Carroll</p>
        <p>Proofreader: Katie Tennant</p>
        <p>Technical proofreader: Karsten Strøbaek</p>
        <p>Typesetter: Dennis Dalinnik</p>
        <p>Cover designer: Marija Tudor</p>
        <p>ISBN: 9781617299889</p>
        <p>Printed in the United States of America</p>
        <p><a id="calibre_link-325"></a> <i> To my wife, my parents, and my sister,
            </i></p>
        <p> <i>even though you will probably never read it. </i></p>
        <p><a id="calibre_link-326"></a> <i> </i></p>
        <p><a id="calibre_link-309"></a> <i>brief contents</i></p>
        <p><a href="#calibre_link-1"><b>PART 1</b></a></p>
        <p><a href="#calibre_link-1"><b>TIME WAITS FOR NO ONE
                    ...............................................1</b></a></p>
        <p> <i>1</i></p>
        <p>■</p>
        <p><a href="#calibre_link-2">Understanding time series forecasting</a></p>
        <p><a href="#calibre_link-2">3</a></p>
        <p> <i>2</i></p>
        <p>■</p>
        <p><a href="#calibre_link-3">A naive prediction of the future</a></p>
        <p><a href="#calibre_link-3">14</a></p>
        <p> <i>3</i></p>
        <p>■</p>
        <p><a href="#calibre_link-4">Going on a random walk</a></p>
        <p><a href="#calibre_link-4">30</a></p>
        <p><a href="#calibre_link-5"><b>PART 2</b></a></p>
        <p><a href="#calibre_link-5"><b>FORECASTING WITH STATISTICAL MODELS
                    ....................59</b></a></p>
        <p> <i>4</i></p>
        <p>■</p>
        <p><a href="#calibre_link-6">Modeling a moving average process</a></p>
        <p><a href="#calibre_link-6">61</a></p>
        <p> <i>5</i></p>
        <p>■</p>
        <p><a href="#calibre_link-7">Modeling an autoregressive process</a></p>
        <p><a href="#calibre_link-7">81</a></p>
        <p> <i>6</i></p>
        <p>■</p>
        <p><a href="#calibre_link-8">Modeling complex time series</a></p>
        <p><a href="#calibre_link-8">101</a></p>
        <p> <i>7</i></p>
        <p>■</p>
        <p><a href="#calibre_link-9">Forecasting non-stationary time series</a></p>
        <p><a href="#calibre_link-9">140</a></p>
        <p> <i>8</i></p>
        <p>■</p>
        <p><a href="#calibre_link-10">Accounting for seasonality</a></p>
        <p><a href="#calibre_link-10">156</a></p>
        <p> <i>9</i></p>
        <p>■</p>
        <p><a href="#calibre_link-11">Adding external variables to our model</a></p>
        <p><a href="#calibre_link-11">180</a></p>
        <p> <i>10</i></p>
        <p>■</p>
        <p><a href="#calibre_link-12">Forecasting multiple time series</a></p>
        <p><a href="#calibre_link-12">197</a></p>
        <p> <i>11</i></p>
        <p>■</p>
        <p><a href="#calibre_link-13">Capstone: Forecasting the number of antidiabetic drug </a></p>
        <p><a href="#calibre_link-13">prescriptions in Australia</a></p>
        <p><a href="#calibre_link-13">216</a></p>
        <p><b>vii</b></p>
        <p><a id="calibre_link-327"></a><b>viii</b></p>
        <p>BRIEF CONTENTS</p>
        <p><a href="#calibre_link-14"><b>PART 3</b></a></p>
        <p><a href="#calibre_link-14"><b>LARGE-SCALE FORECASTING WITH DEEP LEARNING
                    ......231</b></a></p>
        <p> <i>12</i></p>
        <p>■</p>
        <p><a href="#calibre_link-15">Introducing deep learning for time series </a></p>
        <p><a href="#calibre_link-15">forecasting</a></p>
        <p><a href="#calibre_link-15">233</a></p>
        <p> <i>13</i></p>
        <p>■</p>
        <p><a href="#calibre_link-16">Data windowing and creating baselines </a></p>
        <p><a href="#calibre_link-16">for deep learning</a></p>
        <p><a href="#calibre_link-16">248</a></p>
        <p> <i>14</i></p>
        <p>■</p>
        <p><a href="#calibre_link-17">Baby steps with deep learning</a></p>
        <p><a href="#calibre_link-17">270</a></p>
        <p> <i>15</i></p>
        <p>■</p>
        <p><a href="#calibre_link-18">Remembering the past with LSTM</a></p>
        <p><a href="#calibre_link-18">287</a></p>
        <p> <i>16</i></p>
        <p>■</p>
        <p><a href="#calibre_link-19">Filtering a time series with CNN</a></p>
        <p><a href="#calibre_link-19">305</a></p>
        <p> <i>17</i></p>
        <p>■</p>
        <p><a href="#calibre_link-20">Using predictions to make more predictions</a></p>
        <p><a href="#calibre_link-20">320</a></p>
        <p> <i>18</i></p>
        <p>■</p>
        <p><a href="#calibre_link-21">Capstone: Forecasting the electric power </a></p>
        <p><a href="#calibre_link-21">consumption of a household</a></p>
        <p><a href="#calibre_link-21">329</a></p>
        <p><a href="#calibre_link-22"><b>PART 4</b></a></p>
        <p><a href="#calibre_link-22"><b>AUTOMATING FORECASTING AT
                    SCALE........................359</b></a></p>
        <p> <i>19</i></p>
        <p>■</p>
        <p><a href="#calibre_link-23">Automating time series forecasting with Prophet</a></p>
        <p><a href="#calibre_link-23">361</a></p>
        <p> <i>20</i></p>
        <p>■</p>
        <p><a href="#calibre_link-24">Capstone: Forecasting the monthly average retail </a></p>
        <p><a href="#calibre_link-24">price of steak in Canada</a></p>
        <p><a href="#calibre_link-24">396</a></p>
        <p> <i>21</i></p>
        <p>■</p>
        <p><a href="#calibre_link-25">Going above and beyond</a></p>
        <p><a href="#calibre_link-25">410</a></p>
        <p><a id="calibre_link-310"></a> <i>contents</i></p>
        <p><a href="#calibre_link-26"> <i>preface</i></a></p>
        <p><a href="#calibre_link-26"> <i>xvii</i></a></p>
        <p><a href="#calibre_link-27"> <i>acknowledgments</i></a></p>
        <p><a href="#calibre_link-27"> <i>xix</i></a></p>
        <p><a href="#calibre_link-28"> <i>about this book</i></a></p>
        <p><a href="#calibre_link-28"> <i>xx</i></a></p>
        <p><a href="#calibre_link-29"> <i>about the author</i></a></p>
        <p><a href="#calibre_link-29"> <i>xxiv</i></a></p>
        <p><a href="#calibre_link-30"> <i>about the cover illustration</i></a></p>
        <p><a href="#calibre_link-30"> <i>xxv</i></a></p>
        <p><a href="#calibre_link-1"><b>PART 1</b></a></p>
        <p><a href="#calibre_link-1"><b>TIME WAITS FOR NO ONE
                    .....................................1</b></a></p>
        <p><a href="#calibre_link-2"> <i>1 <b>Understanding time
                        series forecasting 3</b></i></a></p>
        <p><a href="#calibre_link-2">1.1</a></p>
        <p><a href="#calibre_link-2">Introducing time series</a></p>
        <p><a href="#calibre_link-2">4</a></p>
        <p><a href="#calibre_link-31"> <i>Components of a time series</i></a></p>
        <p><a href="#calibre_link-31"> <i>5</i></a></p>
        <p><a href="#calibre_link-32">1.2</a></p>
        <p><a href="#calibre_link-32">Bird's-eye view of time series forecasting</a></p>
        <p><a href="#calibre_link-32">8</a></p>
        <p><a href="#calibre_link-33"> <i>Setting a goal</i></a></p>
        <p><a href="#calibre_link-33"> <i>9 </i></a>■ <a href="#calibre_link-33"> <i class="calibre3">Determining what
                    must be forecast to achieve </i></a></p>
        <p><a href="#calibre_link-33"> <i>your goal</i></a></p>
        <p><a href="#calibre_link-33"> <i>9 </i></a>■ <a href="#calibre_link-34"> <i class="calibre3">Setting the
                    horizon of the forecast</i></a></p>
        <p><a href="#calibre_link-34"> <i>10 </i></a>■ <a href="#calibre_link-34"> <i class="calibre3">Gathering
                </i></a></p>
        <p><a href="#calibre_link-34"> <i>the data</i></a></p>
        <p><a href="#calibre_link-34"> <i>10 </i></a>■ <a href="#calibre_link-34"> <i class="calibre3">Developing a
                    forecasting model</i></a></p>
        <p><a href="#calibre_link-34"> <i>10 </i></a>■ <a href="#calibre_link-35"> <i class="calibre3">Deploying to
                </i></a></p>
        <p><a href="#calibre_link-35"> <i>production</i></a></p>
        <p><a href="#calibre_link-35"> <i>11 </i></a>■ <a href="#calibre_link-35"> <i
                    class="calibre3">Monitoring</i></a></p>
        <p><a href="#calibre_link-35"> <i>11 </i></a>■ <a href="#calibre_link-35"> <i class="calibre3">Collecting new
                    data</i></a></p>
        <p><a href="#calibre_link-35"> <i>11</i></a></p>
        <p><a href="#calibre_link-36">1.3</a></p>
        <p><a href="#calibre_link-36">How time series forecasting is different from </a></p>
        <p><a href="#calibre_link-36">other regression tasks</a></p>
        <p><a href="#calibre_link-36">12</a></p>
        <p><a href="#calibre_link-36"> <i>Time series have an order</i></a></p>
        <p><a href="#calibre_link-36"> <i>12 </i></a>■ <a href="#calibre_link-37"> <i class="calibre3">Time series
                    sometimes do not </i></a></p>
        <p><a href="#calibre_link-37"> <i>have features</i></a></p>
        <p><a href="#calibre_link-37"> <i>13</i></a></p>
        <p>1.4</p>
        <p><a href="#calibre_link-37">Next steps</a></p>
        <p><a href="#calibre_link-37">13</a></p>
        <p><b>ix</b></p>
        <p><a id="calibre_link-328"></a><b>x</b></p>
        <p>CONTENTS</p>
        <p><a href="#calibre_link-3"> <i>2 <b>A naive prediction of
                        the future 14</b></i></a></p>
        <p><a href="#calibre_link-3">2.1</a></p>
        <p><a href="#calibre_link-3">Defining a baseline model</a></p>
        <p><a href="#calibre_link-3">16</a></p>
        <p><a href="#calibre_link-38">2.2</a></p>
        <p><a href="#calibre_link-38">Forecasting the historical mean</a></p>
        <p><a href="#calibre_link-38">17</a></p>
        <p><a href="#calibre_link-38"> <i>Setup for baseline implementations</i></a>
        </p>
        <p><a href="#calibre_link-38"> <i>17 </i></a>■ <a href="#calibre_link-39"> <i class="calibre3">Implementing the
                </i></a></p>
        <p><a href="#calibre_link-39"> <i>historical mean baseline</i></a></p>
        <p><a href="#calibre_link-39"> <i>19</i></a></p>
        <p><a href="#calibre_link-40">2.3</a></p>
        <p><a href="#calibre_link-40">Forecasting last year's mean</a></p>
        <p><a href="#calibre_link-40">23</a></p>
        <p><a href="#calibre_link-41">2.4</a></p>
        <p><a href="#calibre_link-41">Predicting using the last known value</a></p>
        <p><a href="#calibre_link-41">25</a></p>
        <p><a href="#calibre_link-42">2.5</a></p>
        <p><a href="#calibre_link-42">Implementing the naive seasonal forecast</a></p>
        <p><a href="#calibre_link-42">26</a></p>
        <p><a href="#calibre_link-43">2.6</a></p>
        <p><a href="#calibre_link-43">Next steps</a></p>
        <p><a href="#calibre_link-43">28</a></p>
        <p><a href="#calibre_link-4"> <i>3 <b>Going on a random walk
                        30</b></i></a></p>
        <p><a href="#calibre_link-4">3.1</a></p>
        <p><a href="#calibre_link-4">The random walk process</a></p>
        <p><a href="#calibre_link-4">31</a></p>
        <p><a href="#calibre_link-44"> <i>Simulating a random walk process</i></a></p>
        <p><a href="#calibre_link-44"> <i>32</i></a></p>
        <p><a href="#calibre_link-45">3.2</a></p>
        <p><a href="#calibre_link-45">Identifying a random walk</a></p>
        <p><a href="#calibre_link-45">35</a></p>
        <p><a href="#calibre_link-46"> <i>Stationarity</i></a></p>
        <p><a href="#calibre_link-46"> <i>36 </i></a>■ <a href="#calibre_link-47"> <i class="calibre3">Testing for
                    stationarity</i></a></p>
        <p><a href="#calibre_link-47"> <i>38 </i></a>■ <a href="#calibre_link-48"> <i class="calibre3">The
                    autocorrelation </i></a></p>
        <p><a href="#calibre_link-48"> <i>function</i></a></p>
        <p><a href="#calibre_link-48"> <i>41 </i></a>■ <a href="#calibre_link-49"> <i class="calibre3">Putting it all
                    together</i></a></p>
        <p><a href="#calibre_link-49"> <i>42 </i></a>■ <a href="#calibre_link-50"> <i class="calibre3">Is GOOGL a random
                </i></a></p>
        <p><a href="#calibre_link-50"> <i>walk? </i></a></p>
        <p><a href="#calibre_link-50"> <i>45</i></a></p>
        <p><a href="#calibre_link-51">3.3</a></p>
        <p><a href="#calibre_link-51">Forecasting a random walk</a></p>
        <p><a href="#calibre_link-51">47</a></p>
        <p><a href="#calibre_link-52"> <i>Forecasting on a long horizon</i></a></p>
        <p><a href="#calibre_link-52"> <i>48 </i></a>■ <a href="#calibre_link-53"> <i class="calibre3">Forecasting the
                    next </i></a></p>
        <p><a href="#calibre_link-53"> <i>timestep</i></a></p>
        <p><a href="#calibre_link-53"> <i>52</i></a></p>
        <p><a href="#calibre_link-54">3.4</a></p>
        <p><a href="#calibre_link-54">Next steps</a></p>
        <p><a href="#calibre_link-54">55</a></p>
        <p><a href="#calibre_link-55">3.5</a></p>
        <p><a href="#calibre_link-55">Exercises</a></p>
        <p><a href="#calibre_link-55">56</a></p>
        <p><a href="#calibre_link-55"> <i>Simulate and forecast a random walk</i></a>
        </p>
        <p><a href="#calibre_link-55"> <i>56 </i></a>■ <a href="#calibre_link-56"> <i class="calibre3">Forecast the
                    daily </i></a></p>
        <p><a href="#calibre_link-56"> <i>closing price of GOOGL</i></a></p>
        <p><a href="#calibre_link-56"> <i>57 </i></a>■ <a href="#calibre_link-56"> <i class="calibre3">Forecast the
                    daily closing price of a </i></a></p>
        <p><a href="#calibre_link-56"> <i>stock of your choice</i></a></p>
        <p><a href="#calibre_link-56"> <i>57</i></a></p>
        <p><a href="#calibre_link-5"><b>PART 2</b></a></p>
        <p><a href="#calibre_link-5"><b>FORECASTING WITH STATISTICAL MODELS
                    ..........59</b></a></p>
        <p><a href="#calibre_link-6"> <i>4 <b>Modeling a moving
                        average process 61</b></i></a></p>
        <p><a href="#calibre_link-57">4.1</a></p>
        <p><a href="#calibre_link-57">Defining a moving average process</a></p>
        <p><a href="#calibre_link-57">63</a></p>
        <p><a href="#calibre_link-58"> <i>Identifying the order of a moving average
                    process</i></a></p>
        <p><a href="#calibre_link-58"> <i>64</i></a></p>
        <p><a href="#calibre_link-59">4.2</a></p>
        <p><a href="#calibre_link-59">Forecasting a moving average process</a></p>
        <p><a href="#calibre_link-59">69</a></p>
        <p><a href="#calibre_link-60">4.3</a></p>
        <p><a href="#calibre_link-60">Next steps</a></p>
        <p><a href="#calibre_link-60">78</a></p>
        <p><a href="#calibre_link-61">4.4</a></p>
        <p><a href="#calibre_link-61">Exercises</a></p>
        <p><a href="#calibre_link-61">79</a></p>
        <p><a href="#calibre_link-61"> <i>Simulate an MA(2) process and make
                    forecasts</i></a></p>
        <p><a href="#calibre_link-61"> <i>79 </i></a>■ <a href="#calibre_link-62"> <i class="calibre3">Simulate an
                </i></a></p>
        <p><a href="#calibre_link-62"> <i>MA(q) process and make forecasts</i></a></p>
        <p><a href="#calibre_link-62"> <i>80</i></a></p>
        <p><a id="calibre_link-329"></a>CONTENTS</p>
        <p><b>xi</b></p>
        <p><a href="#calibre_link-7"> <i>5 <b>Modeling an
                        autoregressive process 81</b></i></a></p>
        <p><a href="#calibre_link-7">5.1</a></p>
        <p><a href="#calibre_link-7">Predicting the average weekly foot traffic in a retail </a></p>
        <p><a href="#calibre_link-63">store</a></p>
        <p><a href="#calibre_link-63">82</a></p>
        <p><a href="#calibre_link-64">5.2</a></p>
        <p><a href="#calibre_link-64">Defining the autoregressive process</a></p>
        <p><a href="#calibre_link-64">84</a></p>
        <p><a href="#calibre_link-65">5.3</a></p>
        <p><a href="#calibre_link-65">Finding the order of a stationary autoregressive process</a></p>
        <p><a href="#calibre_link-65">85</a></p>
        <p><a href="#calibre_link-66"> <i>The partial autocorrelation function
                    (PACF)</i></a></p>
        <p><a href="#calibre_link-66"> <i>89</i></a></p>
        <p><a href="#calibre_link-67">5.4</a></p>
        <p><a href="#calibre_link-67">Forecasting an autoregressive process</a></p>
        <p><a href="#calibre_link-67">92</a></p>
        <p><a href="#calibre_link-68">5.5</a></p>
        <p><a href="#calibre_link-68">Next steps</a></p>
        <p><a href="#calibre_link-68">98</a></p>
        <p><a href="#calibre_link-69">5.6</a></p>
        <p><a href="#calibre_link-69">Exercises</a></p>
        <p><a href="#calibre_link-69">99</a></p>
        <p><a href="#calibre_link-69"> <i>Simulate an AR(2) process and make
                    forecasts</i></a></p>
        <p><a href="#calibre_link-69"> <i>99 </i></a>■ <a href="#calibre_link-70"> <i class="calibre3">Simulate an
                </i></a></p>
        <p><a href="#calibre_link-70"> <i>AR(p) process and make forecasts</i></a></p>
        <p><a href="#calibre_link-70"> <i>100</i></a></p>
        <p><a href="#calibre_link-8"> <i>6 <b>Modeling complex time
                        series 101</b></i></a></p>
        <p><a href="#calibre_link-8">6.1</a></p>
        <p><a href="#calibre_link-8">Forecasting bandwidth usage for data centers</a></p>
        <p><a href="#calibre_link-8">102</a></p>
        <p><a href="#calibre_link-71">6.2</a></p>
        <p><a href="#calibre_link-71">Examining the autoregressive moving average process</a></p>
        <p><a href="#calibre_link-71">105</a></p>
        <p><a href="#calibre_link-72">6.3</a></p>
        <p><a href="#calibre_link-72">Identifying a stationary ARMA process</a></p>
        <p><a href="#calibre_link-72">106</a></p>
        <p><a href="#calibre_link-73">6.4</a></p>
        <p><a href="#calibre_link-73">Devising a general modeling procedure</a></p>
        <p><a href="#calibre_link-73">111</a></p>
        <p><a href="#calibre_link-74"> <i>Understanding the Akaike information
                    criterion (AIC)</i></a></p>
        <p><a href="#calibre_link-74"> <i>113</i></a></p>
        <p><a href="#calibre_link-75"> <i>Selecting a model using the AIC</i></a></p>
        <p><a href="#calibre_link-75"> <i>114 </i></a>■ <a href="#calibre_link-76"> <i class="calibre3">Understanding
                    residual </i></a></p>
        <p><a href="#calibre_link-76"> <i>analysis</i></a></p>
        <p><a href="#calibre_link-76"> <i>116 </i></a>■ <a href="#calibre_link-77"> <i class="calibre3">Performing
                    residual analysis</i></a></p>
        <p><a href="#calibre_link-77"> <i>121</i></a></p>
        <p><a href="#calibre_link-78">6.5</a></p>
        <p><a href="#calibre_link-78">Applying the general modeling procedure</a></p>
        <p><a href="#calibre_link-78">125</a></p>
        <p><a href="#calibre_link-79">6.6</a></p>
        <p><a href="#calibre_link-79">Forecasting bandwidth usage</a></p>
        <p><a href="#calibre_link-79">132</a></p>
        <p><a href="#calibre_link-80">6.7</a></p>
        <p><a href="#calibre_link-80">Next steps</a></p>
        <p><a href="#calibre_link-80">136</a></p>
        <p><a href="#calibre_link-81">6.8</a></p>
        <p><a href="#calibre_link-81">Exercises</a></p>
        <p><a href="#calibre_link-81">137</a></p>
        <p><a href="#calibre_link-81"> <i>Make predictions on the simulated ARMA(1,1)
                    process</i></a></p>
        <p><a href="#calibre_link-81"> <i>137</i></a></p>
        <p><a href="#calibre_link-81"> <i>Simulate an ARMA(2,2) process and make
                    forecasts</i></a></p>
        <p><a href="#calibre_link-81"> <i>137</i></a></p>
        <p><a href="#calibre_link-9"> <i>7 <b>Forecasting
                        non-stationary time series 140</b></i></a></p>
        <p><a href="#calibre_link-9">7.1</a></p>
        <p><a href="#calibre_link-9">Defining the autoregressive integrated moving </a></p>
        <p><a href="#calibre_link-82">average model</a></p>
        <p><a href="#calibre_link-82">142</a></p>
        <p><a href="#calibre_link-83">7.2</a></p>
        <p><a href="#calibre_link-83">Modifying the general modeling procedure to account for </a></p>
        <p><a href="#calibre_link-83">non-stationary series</a></p>
        <p><a href="#calibre_link-83">143</a></p>
        <p><a href="#calibre_link-84">7.3</a></p>
        <p><a href="#calibre_link-84">Forecasting a non-stationary times series</a></p>
        <p><a href="#calibre_link-84">145</a></p>
        <p><a href="#calibre_link-85">7.4</a></p>
        <p><a href="#calibre_link-85">Next steps</a></p>
        <p><a href="#calibre_link-85">154</a></p>
        <p><a href="#calibre_link-85">7.5</a></p>
        <p><a href="#calibre_link-85">Exercises</a></p>
        <p><a href="#calibre_link-85">154</a></p>
        <p><a href="#calibre_link-85"> <i>Apply the ARIMA(p,d,q) model on the datasets
                    from chapters 4, 5, </i></a></p>
        <p><a href="#calibre_link-85"> <i>and 6</i></a></p>
        <p><a href="#calibre_link-85"> <i>154</i></a></p>
        <p><a id="calibre_link-330"></a><b>xii</b></p>
        <p>CONTENTS</p>
        <p><a href="#calibre_link-10"> <i>8 <b>Accounting for
                        seasonality 156</b></i></a></p>
        <p><a href="#calibre_link-10">8.1</a></p>
        <p><a href="#calibre_link-10">Examining the SARIMA(p,d,q)(P,D,Q)</a>m<a href="#calibre_link-10"> model 157</a>
        </p>
        <p><a href="#calibre_link-86">8.2</a></p>
        <p><a href="#calibre_link-86">Identifying seasonal patterns in a time series</a></p>
        <p><a href="#calibre_link-86">160</a></p>
        <p><a href="#calibre_link-87">8.3</a></p>
        <p><a href="#calibre_link-87">Forecasting the number of monthly air passengers</a></p>
        <p><a href="#calibre_link-87">163</a></p>
        <p><a href="#calibre_link-88"> <i>Forecasting with an ARIMA(p,d,q)
                    model</i></a></p>
        <p><a href="#calibre_link-88"> <i>165 </i></a>■ <a href="#calibre_link-89"> <i class="calibre3">Forecasting with
                </i></a></p>
        <p><a href="#calibre_link-89"> <i>a SARIMA(p,d,q)(P,D,Q)m</i></a> <i> model
                171 </i>■ <a href="#calibre_link-90"> <i>Comparing the </i></a>
        </p>
        <p><a href="#calibre_link-90"> <i>performance of each forecasting
                    method</i></a></p>
        <p><a href="#calibre_link-90"> <i>176</i></a></p>
        <p><a href="#calibre_link-91">8.4</a></p>
        <p><a href="#calibre_link-91">Next steps</a></p>
        <p><a href="#calibre_link-91">178</a></p>
        <p><a href="#calibre_link-91">8.5</a></p>
        <p><a href="#calibre_link-91">Exercises</a></p>
        <p><a href="#calibre_link-91">178</a></p>
        <p><a href="#calibre_link-91"> <i>Apply the SARIMA(p,d,q)(P,D,Q)</i></a> <i>m
                model on the Johnson &amp;</i> <i>Johnson dataset</i></p>
        <p> <i>178</i></p>
        <p><a href="#calibre_link-11"> <i>9 <b>Adding external
                        variables to our model 180</b></i></a></p>
        <p><a href="#calibre_link-11">9.1</a></p>
        <p><a href="#calibre_link-11">Examining the SARIMAX model</a></p>
        <p><a href="#calibre_link-11">182</a></p>
        <p><a href="#calibre_link-92"> <i>Exploring the exogenous variables of the US
                    macroeconomics </i></a></p>
        <p><a href="#calibre_link-92"> <i>dataset</i></a></p>
        <p><a href="#calibre_link-92"> <i>183 </i></a>■ <a href="#calibre_link-93"> <i class="calibre3">Caveat for using
                    SARIMAX</i></a></p>
        <p><a href="#calibre_link-93"> <i>185</i></a></p>
        <p><a href="#calibre_link-94">9.2</a></p>
        <p><a href="#calibre_link-94">Forecasting the real GDP using the SARIMAX model</a></p>
        <p><a href="#calibre_link-94">186</a></p>
        <p><a href="#calibre_link-95">9.3</a></p>
        <p><a href="#calibre_link-95">Next steps</a></p>
        <p><a href="#calibre_link-95">195</a></p>
        <p><a href="#calibre_link-95">9.4</a></p>
        <p><a href="#calibre_link-95">Exercises</a></p>
        <p><a href="#calibre_link-95">195</a></p>
        <p><a href="#calibre_link-95"> <i>Use all exogenous variables in a SARIMAX
                    model to predict </i></a></p>
        <p><a href="#calibre_link-95"> <i>the real GDP</i></a></p>
        <p><a href="#calibre_link-95"> <i>195</i></a></p>
        <p><a href="#calibre_link-12"> <i>10 <b>Forecasting multiple
                        time series 197</b></i></a></p>
        <p><a href="#calibre_link-12">10.1</a></p>
        <p><a href="#calibre_link-12">Examining the VAR model</a></p>
        <p><a href="#calibre_link-12">199</a></p>
        <p><a href="#calibre_link-96">10.2</a></p>
        <p><a href="#calibre_link-96">Designing a modeling procedure for the VAR(p) </a></p>
        <p><a href="#calibre_link-96">model</a></p>
        <p><a href="#calibre_link-96">201</a></p>
        <p><a href="#calibre_link-96"> <i>Exploring the Granger causality test</i></a>
        </p>
        <p><a href="#calibre_link-96"> <i>201</i></a></p>
        <p><a href="#calibre_link-97">10.3</a></p>
        <p><a href="#calibre_link-97">Forecasting real disposable income and real </a></p>
        <p><a href="#calibre_link-97">consumption</a></p>
        <p><a href="#calibre_link-97">203</a></p>
        <p><a href="#calibre_link-98">10.4</a></p>
        <p><a href="#calibre_link-98">Next steps</a></p>
        <p><a href="#calibre_link-98">214</a></p>
        <p><a href="#calibre_link-98">10.5</a></p>
        <p><a href="#calibre_link-98">Exercises</a></p>
        <p><a href="#calibre_link-98">214</a></p>
        <p><a href="#calibre_link-98"> <i>Use a VARMA model to predict realdpi and
                    realcons</i></a></p>
        <p><a href="#calibre_link-98"> <i>214</i></a></p>
        <p><a href="#calibre_link-99"> <i>Use a VARMAX model to predict realdpi and
                    realcons</i></a></p>
        <p><a href="#calibre_link-99"> <i>215</i></a></p>
        <p><a href="#calibre_link-13"> <i>11 <b>Capstone: Forecasting
                        the number of antidiabetic drug </b></i></a></p>
        <p><a href="#calibre_link-13"> <i><b>prescriptions in
                        Australia</b></i></a></p>
        <p><a href="#calibre_link-13"> <i><b>216</b></i></a></p>
        <p><a href="#calibre_link-100">11.1</a></p>
        <p><a href="#calibre_link-100">Importing the required libraries and loading the data</a></p>
        <p><a href="#calibre_link-100">218</a></p>
        <p>11.2</p>
        <p><a href="#calibre_link-101">Visualizing the series and its components</a></p>
        <p><a href="#calibre_link-101">219</a></p>
        <p><a id="calibre_link-331"></a>CONTENTS</p>
        <p><b>xiii</b></p>
        <p><a href="#calibre_link-102">11.3</a></p>
        <p><a href="#calibre_link-102">Modeling the data</a></p>
        <p><a href="#calibre_link-102">220</a></p>
        <p><a href="#calibre_link-103"> <i>Performing model selection</i></a></p>
        <p><a href="#calibre_link-103"> <i>222 </i></a>■ <a href="#calibre_link-104">
                <i>Conducting residual </i></a></p>
        <p><a href="#calibre_link-104"> <i>analysis</i></a></p>
        <p><a href="#calibre_link-104"> <i>224</i></a></p>
        <p><a href="#calibre_link-105">11.4</a></p>
        <p><a href="#calibre_link-105">Forecasting and evaluating the model's performance</a></p>
        <p><a href="#calibre_link-105">225</a></p>
        <p><a href="#calibre_link-106">11.5</a></p>
        <p><a href="#calibre_link-106">Next steps</a></p>
        <p><a href="#calibre_link-106">229</a></p>
        <p><a href="#calibre_link-14"><b>PART 3</b></a></p>
        <p><a href="#calibre_link-14"><b>LARGE-SCALE FORECASTING WITH DEEP </b></a>
        </p>
        <p><a href="#calibre_link-14"><b>LEARNING
                    ....................................................... 231</b></a></p>
        <p><a href="#calibre_link-15"> <i>12 <b>Introducing deep
                        learning </b></i></a></p>
        <p><a href="#calibre_link-15"> <i><b>for time series
                        forecasting</b></i></a></p>
        <p><a href="#calibre_link-15"> <i><b>233</b></i></a></p>
        <p><a href="#calibre_link-107">12.1</a></p>
        <p><a href="#calibre_link-107">When to use deep learning for time series </a></p>
        <p><a href="#calibre_link-107">forecasting</a></p>
        <p><a href="#calibre_link-107">234</a></p>
        <p><a href="#calibre_link-107">12.2</a></p>
        <p><a href="#calibre_link-107">Exploring the different types of deep learning </a></p>
        <p><a href="#calibre_link-107">models</a></p>
        <p><a href="#calibre_link-107">234</a></p>
        <p><a href="#calibre_link-108">12.3</a></p>
        <p><a href="#calibre_link-108">Getting ready to apply deep learning for forecasting</a></p>
        <p><a href="#calibre_link-108">237</a></p>
        <p><a href="#calibre_link-108"> <i>Performing data exploration</i></a></p>
        <p><a href="#calibre_link-108"> <i>237 </i></a>■ <a href="#calibre_link-109">
                <i>Feature engineering and data </i></a></p>
        <p><a href="#calibre_link-109"> <i>splitting</i></a></p>
        <p><a href="#calibre_link-109"> <i>241</i></a></p>
        <p><a href="#calibre_link-110">12.4</a></p>
        <p><a href="#calibre_link-110">Next steps</a></p>
        <p><a href="#calibre_link-110">246</a></p>
        <p><a href="#calibre_link-110">12.5</a></p>
        <p><a href="#calibre_link-110">Exercise</a></p>
        <p><a href="#calibre_link-110">246</a></p>
        <p><a href="#calibre_link-16"> <i>13 <b>Data windowing and
                        creating baselines for deep learning 248</b></i></a></p>
        <p><a href="#calibre_link-16">13.1</a></p>
        <p><a href="#calibre_link-16">Creating windows of data</a></p>
        <p><a href="#calibre_link-16">249</a></p>
        <p><a href="#calibre_link-111"> <i>Exploring how deep learning models are
                    trained for time series </i></a></p>
        <p><a href="#calibre_link-111"> <i>forecasting</i></a></p>
        <p><a href="#calibre_link-111"> <i>249 </i></a>■ <a href="#calibre_link-112">
                <i>Implementing the DataWindow class</i></a></p>
        <p><a href="#calibre_link-112"> <i>253</i></a></p>
        <p><a href="#calibre_link-113">13.2</a></p>
        <p><a href="#calibre_link-113">Applying baseline models</a></p>
        <p><a href="#calibre_link-113">260</a></p>
        <p><a href="#calibre_link-113"> <i>Single-step baseline model</i></a></p>
        <p><a href="#calibre_link-113"> <i>260 </i></a>■ <a href="#calibre_link-114">
                <i>Multi-step baseline models</i></a></p>
        <p><a href="#calibre_link-114"> <i>263</i></a></p>
        <p><a href="#calibre_link-115"> <i>Multi-output baseline model</i></a></p>
        <p><a href="#calibre_link-115"> <i>266</i></a></p>
        <p><a href="#calibre_link-116">13.3</a></p>
        <p><a href="#calibre_link-116">Next steps</a></p>
        <p><a href="#calibre_link-116">268</a></p>
        <p><a href="#calibre_link-117">13.4</a></p>
        <p><a href="#calibre_link-117">Exercises</a></p>
        <p><a href="#calibre_link-117">269</a></p>
        <p><a href="#calibre_link-17"> <i>14 <b>Baby steps with deep
                        learning 270</b></i></a></p>
        <p><a href="#calibre_link-118">14.1</a></p>
        <p><a href="#calibre_link-118">Implementing a linear model</a></p>
        <p><a href="#calibre_link-118">271</a></p>
        <p><a href="#calibre_link-119"> <i>Implementing a single-step linear
                    model</i></a></p>
        <p><a href="#calibre_link-119"> <i>272 </i></a>■ <a href="#calibre_link-120">
                <i>Implementing a </i></a></p>
        <p><a href="#calibre_link-120"> <i>multi-step linear model</i></a></p>
        <p><a href="#calibre_link-120"> <i>274 </i></a>■ <a href="#calibre_link-121">
                <i>Implementing a multi-output linear </i></a></p>
        <p><a href="#calibre_link-121"> <i>model</i></a></p>
        <p><a href="#calibre_link-121"> <i>275</i></a></p>
        <p><a id="calibre_link-332"></a><b>xiv</b></p>
        <p>CONTENTS</p>
        <p><a href="#calibre_link-122">14.2</a></p>
        <p><a href="#calibre_link-122">Implementing a deep neural network</a></p>
        <p><a href="#calibre_link-122">276</a></p>
        <p><a href="#calibre_link-123"> <i>Implementing a deep neural network as a
                    single-step model</i></a></p>
        <p><a href="#calibre_link-123"> <i>278</i></a></p>
        <p><a href="#calibre_link-124"> <i>Implementing a deep neural network as a
                    multi-step model</i></a></p>
        <p><a href="#calibre_link-124"> <i>281</i></a></p>
        <p><a href="#calibre_link-125"> <i>Implementing a deep neural network as a
                    multi-output model</i></a></p>
        <p><a href="#calibre_link-125"> <i>282</i></a></p>
        <p><a href="#calibre_link-126">14.3</a></p>
        <p><a href="#calibre_link-126">Next steps</a></p>
        <p><a href="#calibre_link-126">284</a></p>
        <p><a href="#calibre_link-127">14.4</a></p>
        <p><a href="#calibre_link-127">Exercises</a></p>
        <p><a href="#calibre_link-127">285</a></p>
        <p><a href="#calibre_link-18"> <i>15 <b>Remembering the past
                        with LSTM 287</b></i></a></p>
        <p><a href="#calibre_link-18">15.1</a></p>
        <p><a href="#calibre_link-18">Exploring the recurrent neural network (RNN)</a></p>
        <p><a href="#calibre_link-18">288</a></p>
        <p><a href="#calibre_link-128">15.2</a></p>
        <p><a href="#calibre_link-128">Examining the LSTM architecture</a></p>
        <p><a href="#calibre_link-128">290</a></p>
        <p><a href="#calibre_link-129"> <i>The forget gate</i></a></p>
        <p><a href="#calibre_link-129"> <i>291 </i></a>■ <a href="#calibre_link-130">
                <i>The input gate</i></a></p>
        <p><a href="#calibre_link-130"> <i>292 </i></a>■ <a href="#calibre_link-131">
                <i>The output gate</i></a></p>
        <p><a href="#calibre_link-131"> <i>294</i></a></p>
        <p><a href="#calibre_link-132">15.3</a></p>
        <p><a href="#calibre_link-132">Implementing the LSTM architecture</a></p>
        <p><a href="#calibre_link-132">295</a></p>
        <p><a href="#calibre_link-132"> <i>Implementing an LSTM as a single-step
                    model</i></a></p>
        <p><a href="#calibre_link-132"> <i>295</i></a></p>
        <p><a href="#calibre_link-133"> <i>Implementing an LSTM as a multi-step
                    model</i></a></p>
        <p><a href="#calibre_link-133"> <i>297</i></a></p>
        <p><a href="#calibre_link-134"> <i>Implementing an LSTM as a multi-output
                    model</i></a></p>
        <p><a href="#calibre_link-134"> <i>299</i></a></p>
        <p><a href="#calibre_link-135">15.4</a></p>
        <p><a href="#calibre_link-135">Next steps</a></p>
        <p><a href="#calibre_link-135">302</a></p>
        <p><a href="#calibre_link-136">15.5</a></p>
        <p><a href="#calibre_link-136">Exercises</a></p>
        <p><a href="#calibre_link-136">303</a></p>
        <p><a href="#calibre_link-19"> <i>16 <b>Filtering a time
                        series with CNN 305</b></i></a></p>
        <p><a href="#calibre_link-137">16.1</a></p>
        <p><a href="#calibre_link-137">Examining the convolutional neural network (CNN)</a></p>
        <p><a href="#calibre_link-137">306</a></p>
        <p><a href="#calibre_link-138">16.2</a></p>
        <p><a href="#calibre_link-138">Implementing a CNN</a></p>
        <p><a href="#calibre_link-138">309</a></p>
        <p><a href="#calibre_link-139"> <i>Implementing a CNN as a single-step
                    model</i></a></p>
        <p><a href="#calibre_link-139"> <i>310 </i></a>■ <a href="#calibre_link-140">
                <i>Implementing a </i></a></p>
        <p><a href="#calibre_link-140"> <i>CNN as a multi-step model</i></a></p>
        <p><a href="#calibre_link-140"> <i>314 </i></a>■ <a href="#calibre_link-141">
                <i>Implementing a CNN as a </i></a></p>
        <p><a href="#calibre_link-141"> <i>multi-output model</i></a></p>
        <p><a href="#calibre_link-141"> <i>315</i></a></p>
        <p><a href="#calibre_link-142">16.3</a></p>
        <p><a href="#calibre_link-142">Next steps</a></p>
        <p><a href="#calibre_link-142">317</a></p>
        <p><a href="#calibre_link-143">16.4</a></p>
        <p><a href="#calibre_link-143">Exercises</a></p>
        <p><a href="#calibre_link-143">318</a></p>
        <p><a href="#calibre_link-20"> <i>17 <b>Using predictions to
                        make more predictions 320</b></i></a></p>
        <p><a href="#calibre_link-144">17.1</a></p>
        <p><a href="#calibre_link-144">Examining the ARLSTM architecture</a></p>
        <p><a href="#calibre_link-144">321</a></p>
        <p><a href="#calibre_link-145">17.2</a></p>
        <p><a href="#calibre_link-145">Building an autoregressive LSTM model</a></p>
        <p><a href="#calibre_link-145">322</a></p>
        <p><a href="#calibre_link-146">17.3</a></p>
        <p><a href="#calibre_link-146">Next steps</a></p>
        <p><a href="#calibre_link-146">327</a></p>
        <p><a href="#calibre_link-147">17.4</a></p>
        <p><a href="#calibre_link-147">Exercises</a></p>
        <p><a href="#calibre_link-147">328</a></p>
        <p><a href="#calibre_link-21"> <i>18 <b>Capstone: Forecasting
                        the electric power consumption of a </b></i></a></p>
        <p><a href="#calibre_link-21"> <i><b>household</b></i></a>
        </p>
        <p><a href="#calibre_link-21"> <i><b>329</b></i></a></p>
        <p><a href="#calibre_link-148">18.1</a></p>
        <p><a href="#calibre_link-148">Understanding the capstone project</a></p>
        <p><a href="#calibre_link-148">330</a></p>
        <p><a href="#calibre_link-149"> <i>Objective of this capstone project</i></a>
        </p>
        <p><a href="#calibre_link-149"> <i>331</i></a></p>
        <p><a id="calibre_link-333"></a>CONTENTS</p>
        <p><b>xv</b></p>
        <p><a href="#calibre_link-150">18.2</a></p>
        <p><a href="#calibre_link-150">Data wrangling and preprocessing</a></p>
        <p><a href="#calibre_link-150">333</a></p>
        <p><a href="#calibre_link-151"> <i>Dealing with missing data</i></a></p>
        <p><a href="#calibre_link-151"> <i>334 </i></a>■ <a href="#calibre_link-152">
                <i>Data conversion</i></a></p>
        <p><a href="#calibre_link-152"> <i>335</i></a></p>
        <p><a href="#calibre_link-152"> <i>Data resampling</i></a></p>
        <p><a href="#calibre_link-152"> <i>335</i></a></p>
        <p><a href="#calibre_link-153">18.3</a></p>
        <p><a href="#calibre_link-153">Feature engineering</a></p>
        <p><a href="#calibre_link-153">338</a></p>
        <p><a href="#calibre_link-153"> <i>Removing unnecessary columns</i></a></p>
        <p><a href="#calibre_link-153"> <i>338 </i></a>■ <a href="#calibre_link-154">
                <i>Identifying the seasonal </i></a></p>
        <p><a href="#calibre_link-154"> <i>period</i></a></p>
        <p><a href="#calibre_link-154"> <i>339 </i></a>■ <a href="#calibre_link-155">
                <i>Splitting and scaling the data</i></a></p>
        <p><a href="#calibre_link-155"> <i>341</i></a></p>
        <p><a href="#calibre_link-156">18.4</a></p>
        <p><a href="#calibre_link-156">Preparing for modeling with deep learning</a></p>
        <p><a href="#calibre_link-156">342</a></p>
        <p><a href="#calibre_link-156"> <i>Initial setup</i></a></p>
        <p><a href="#calibre_link-156"> <i>342 </i></a>■ <a href="#calibre_link-157">
                <i>Defining the DataWindow class</i></a></p>
        <p><a href="#calibre_link-157"> <i>343</i></a></p>
        <p><a href="#calibre_link-158"> <i>Utility function to train our
                    models</i></a></p>
        <p><a href="#calibre_link-158"> <i>346</i></a></p>
        <p><a href="#calibre_link-158">18.5</a></p>
        <p><a href="#calibre_link-158">Modeling with deep learning</a></p>
        <p><a href="#calibre_link-158">346</a></p>
        <p><a href="#calibre_link-158"> <i>Baseline models</i></a></p>
        <p><a href="#calibre_link-158"> <i>346 </i></a>■ <a href="#calibre_link-159">
                <i>Linear model</i></a></p>
        <p><a href="#calibre_link-159"> <i>349 </i></a>■ <a href="#calibre_link-160">
                <i>Deep neural </i></a></p>
        <p><a href="#calibre_link-160"> <i>network</i></a></p>
        <p><a href="#calibre_link-160"> <i>350 </i></a>■ <a href="#calibre_link-161">
                <i>Long short-term memory (LSTM) model</i></a></p>
        <p><a href="#calibre_link-161"> <i>351</i></a></p>
        <p><a href="#calibre_link-161"> <i>Convolutional neural network (CNN)</i></a>
        </p>
        <p><a href="#calibre_link-161"> <i>351 </i></a>■ <a href="#calibre_link-162">
                <i>Combining a CNN </i></a></p>
        <p><a href="#calibre_link-162"> <i>with an LSTM</i></a></p>
        <p><a href="#calibre_link-162"> <i>354 </i></a>■ <a href="#calibre_link-163">
                <i>The autoregressive LSTM model</i></a></p>
        <p><a href="#calibre_link-163"> <i>355</i></a></p>
        <p><a href="#calibre_link-164"> <i>Selecting the best model</i></a></p>
        <p><a href="#calibre_link-164"> <i>356</i></a></p>
        <p><a href="#calibre_link-165">18.6</a></p>
        <p><a href="#calibre_link-165">Next steps</a></p>
        <p><a href="#calibre_link-165">358</a></p>
        <p><a href="#calibre_link-22"><b>PART 4</b></a></p>
        <p><a href="#calibre_link-22"><b>AUTOMATING FORECASTING AT
                    SCALE..............359</b></a></p>
        <p><a href="#calibre_link-23"> <i>19 <b>Automating time
                        series forecasting with Prophet 361</b></i></a></p>
        <p><a href="#calibre_link-166">19.1</a></p>
        <p><a href="#calibre_link-166">Overview of the automated forecasting libraries</a></p>
        <p><a href="#calibre_link-166">362</a></p>
        <p><a href="#calibre_link-167">19.2</a></p>
        <p><a href="#calibre_link-167">Exploring Prophet</a></p>
        <p><a href="#calibre_link-167">363</a></p>
        <p><a href="#calibre_link-168">19.3</a></p>
        <p><a href="#calibre_link-168">Basic forecasting with Prophet</a></p>
        <p><a href="#calibre_link-168">365</a></p>
        <p><a href="#calibre_link-169">19.4</a></p>
        <p><a href="#calibre_link-169">Exploring Prophet's advanced functionality</a></p>
        <p><a href="#calibre_link-169">370</a></p>
        <p><a href="#calibre_link-169"> <i>Visualization capabilities</i></a></p>
        <p><a href="#calibre_link-169"> <i>370 </i></a>■ <a href="#calibre_link-170">
                <i>Cross-validation and </i></a></p>
        <p><a href="#calibre_link-170"> <i>performance metrics</i></a></p>
        <p><a href="#calibre_link-170"> <i>374 </i></a>■ <a href="#calibre_link-171">
                <i>Hyperparameter tuning</i></a></p>
        <p><a href="#calibre_link-171"> <i>379</i></a></p>
        <p><a href="#calibre_link-172">19.5</a></p>
        <p><a href="#calibre_link-172">Implementing a robust forecasting process </a></p>
        <p><a href="#calibre_link-172">with Prophet</a></p>
        <p><a href="#calibre_link-172">381</a></p>
        <p><a href="#calibre_link-172"> <i>Forecasting project: Predicting the
                    popularity of “chocolate” searches </i></a></p>
        <p><a href="#calibre_link-172"> <i>on Google</i></a></p>
        <p><a href="#calibre_link-172"> <i>381 </i></a>■ <a href="#calibre_link-173">
                <i>Experiment: Can SARIMA do better? </i></a></p>
        <p><a href="#calibre_link-173"> <i>389</i></a></p>
        <p><a href="#calibre_link-174">19.6</a></p>
        <p><a href="#calibre_link-174">Next steps</a></p>
        <p><a href="#calibre_link-174">393</a></p>
        <p><a href="#calibre_link-175">19.7</a></p>
        <p><a href="#calibre_link-175">Exercises</a></p>
        <p><a href="#calibre_link-175">394</a></p>
        <p><a href="#calibre_link-175"> <i>Forecast the number of air
                    passengers</i></a></p>
        <p><a href="#calibre_link-175"> <i>394 </i></a>■ <a href="#calibre_link-175">
                <i>Forecast the volume of </i></a></p>
        <p><a href="#calibre_link-175"> <i>antidiabetic drug prescriptions</i></a></p>
        <p><a href="#calibre_link-175"> <i>394 </i></a>■ <a href="#calibre_link-175">
                <i>Forecast the popularity of a </i></a></p>
        <p><a href="#calibre_link-175"> <i>keyword on Google Trends</i></a></p>
        <p><a href="#calibre_link-175"> <i>394</i></a></p>
        <p><a id="calibre_link-334"></a><b>xvi</b></p>
        <p>CONTENTS</p>
        <p><a href="#calibre_link-24"> <i>20 <b>Capstone: Forecasting
                        the monthly average retail </b></i></a></p>
        <p><a href="#calibre_link-24"> <i><b>price of steak in
                        Canada</b></i></a></p>
        <p><a href="#calibre_link-24"> <i><b>396</b></i></a></p>
        <p><a href="#calibre_link-176">20.1</a></p>
        <p><a href="#calibre_link-176">Understanding the capstone project</a></p>
        <p><a href="#calibre_link-176">397</a></p>
        <p><a href="#calibre_link-176"> <i>Objective of the capstone project</i></a>
        </p>
        <p><a href="#calibre_link-176"> <i>397</i></a></p>
        <p><a href="#calibre_link-177">20.2</a></p>
        <p><a href="#calibre_link-177">Data preprocessing and visualization</a></p>
        <p><a href="#calibre_link-177">398</a></p>
        <p><a href="#calibre_link-178">20.3</a></p>
        <p><a href="#calibre_link-178">Modeling with Prophet</a></p>
        <p><a href="#calibre_link-178">400</a></p>
        <p><a href="#calibre_link-179">20.4</a></p>
        <p><a href="#calibre_link-179">Optional: Develop a SARIMA model</a></p>
        <p><a href="#calibre_link-179">404</a></p>
        <p><a href="#calibre_link-180">20.5</a></p>
        <p><a href="#calibre_link-180">Next steps</a></p>
        <p><a href="#calibre_link-180">409</a></p>
        <p><a href="#calibre_link-25"> <i>21 <b>Going above and
                        beyond 410</b></i></a></p>
        <p><a href="#calibre_link-25">21.1</a></p>
        <p><a href="#calibre_link-25">Summarizing what you've learned</a></p>
        <p><a href="#calibre_link-25">411</a></p>
        <p><a href="#calibre_link-181"> <i>Statistical methods for forecasting</i></a>
        </p>
        <p><a href="#calibre_link-181"> <i>411 </i></a>■ <a href="#calibre_link-182">
                <i>Deep learning methods for </i></a></p>
        <p><a href="#calibre_link-182"> <i>forecasting</i></a></p>
        <p><a href="#calibre_link-182"> <i>412 </i></a>■ <a href="#calibre_link-183">
                <i>Automating the forecasting process</i></a></p>
        <p><a href="#calibre_link-183"> <i>413</i></a></p>
        <p><a href="#calibre_link-183">21.2</a></p>
        <p><a href="#calibre_link-183">What if forecasting does not work? </a></p>
        <p><a href="#calibre_link-183">413</a></p>
        <p><a href="#calibre_link-184">21.3</a></p>
        <p><a href="#calibre_link-184">Other applications of time series data</a></p>
        <p><a href="#calibre_link-184">415</a></p>
        <p><a href="#calibre_link-185">21.4</a></p>
        <p><a href="#calibre_link-185">Keep practicing</a></p>
        <p><a href="#calibre_link-185">416</a></p>
        <p><a href="#calibre_link-186"> <i>appendix</i></a></p>
        <p><a href="#calibre_link-186"> <i>Installation instructions</i></a></p>
        <p><a href="#calibre_link-186"> <i>418</i></a></p>
        <p><a href="#calibre_link-187"> <i>index</i></a></p>
        <p><a href="#calibre_link-187"> <i>421</i></a></p>
        <p><a id="calibre_link-26"></a> <i>preface</i></p>
        <p>Working at a bank, I quickly realized how time is an important factor. Interest rates vary
            over time, people's spending varies over time, asset prices vary over time. Yet I found most people,
            including me, were uncomfortable with time series. So I decided to learn time series forecasting. </p>
        <p>It turned out to be harder than expected because every resource I found was in R. </p>
        <p>I am comfortable with Python, and Python is undoubtedly the most popular language for data
            science in the industry. While R constrains you to statistical computing, Python allows you to code
            websites, perform machine learning, deploy models, build servers, and more. Therefore, I had to translate a
            lot of R code into Python to learn time series forecasting. That's when I recognized the gap, and I was
            lucky enough to be given the opportunity to write a book about it. </p>
        <p>With this book, I hope to create a one-stop reference for time series forecasting with
            Python. It covers both statistical and machine learning models, and it also discusses automated forecasting
            libraries, as they are widely used in the industry and often act as baseline models. This book greatly
            emphasizes a hands-on, practical approach, with various real-life scenarios. In real life, data is messy,
            dirty, and sometimes missing, and I wanted to give readers a safe space to experiment with those
            difficulties, learn from them, and easily transpose those skills into their own projects. </p>
        <p>This book focuses on time series forecasting. Of course, with time series data, we can also
            perform classification or anomaly detection, but this book addresses only forecasting to keep the scope
            manageable. </p>
        <p><b>xvii</b></p>
        <p><a id="calibre_link-335"></a><b>xviii</b></p>
        <p>PREFACE</p>
        <p>In each chapter, you will find exercises you can use to practice and hone your skills. Each
            exercise comes with a full solution on GitHub. I strongly suggest that you take the time to complete them,
            as you will gain important practical skills. They offer a great way to test your knowledge, see what you
            need to revisit in a given chapter, and apply modeling techniques in new scenarios. </p>
        <p>After reading the chapters and completing the exercises, you will have all the necessary
            tools to tackle any forecasting project with confidence and great results. Hopefully, you will also gain the
            curiosity and motivation to go beyond this book and become a time series expert. </p>
        <p><a id="calibre_link-27"></a> <i>acknowledgments</i></p>
        <p>First, I would like to thank my wife, Lina. Thank you for listening when I struggled, for
            your feedback on large parts of the book, and for correcting my grammar. You supported me from the very
            beginning and ultimately made this possible. </p>
        <p>Next, I want to acknowledge my editor, Bobbie Jennings. You made the entire pro-</p>
        <p>cess of writing my first book so easy, it makes me want to write a second one! You taught me
            a lot about writing and keeping my audience in mind, and you weren't scared to challenge parts of the book,
            which greatly improved it. </p>
        <p>To all the reviewers&mdash;Amaresh Rajasekharan, Ariel Andres, Biswanath Chowdhury, </p>
        <p>Claudiu Schiller, Dan Sheikh, David Paccoud, David R King, Dinesh Ghanta, Dirk Gomez, Gary
            Bake, Gustavo Patino, Helder C. R. Oliveira, Howard Bandy, Igor Vieira, Kathrin Björkelund, Lokesh Kumar,
            Mary Anne Thygesen, Mikael Dautrey, Naftali Cohen, Oliver Korten, Paul Silisteanu, Raymond Cheung, Richard
            Meinsen, Richard</p>
        <p>Vaughan, Rohit Goswami, Sadhana Ganapathiraju, Shabie Iqbal, Shankar Swamy, Shreesha
            Jagadeesh, Simone Sguazza, Sriram Macharla, Thomas Joseph Heiman, Vincent Vandenborne, and Walter Alexander
            Mata López&mdash;thank you. You all helped</p>
        <p>make this a better book. </p>
        <p>Finally, a special thank you goes to Brian Sawyer. I guess you saw something in me. </p>
        <p>You gave me this incredible opportunity to write a book, and you trusted me the entire time.
            Writing a book is a dream come true for me, and it's happened because you</p>
        <p>started this entire process. I am very grateful for that. </p>
        <p><b>xix</b></p>
        <p><a id="calibre_link-28"></a> <i>about this book</i></p>
        <p>This book was written to help data scientists master time series forecasting and help
            professionals transition from R to Python for time series analysis. It starts off by defining time series
            data and highlighting the uniqueness of working with that type of data (for example, you cannot shuffle the
            data). It then walks through developing baseline models and explores when forecasting does not make sense.
        </p>
        <p>Subsequent chapters dive deep into forecasting techniques and gradually increase the
            complexity of the models, from statistical models to deep learning models. Finally, the book covers
            automated forecasting libraries, which can greatly speed up the forecasting process. This will give you a
            sense of what is being done in the industry. </p>
        <p> <i><b>Who should read this book? </b></i></p>
        <p>This book is for data scientists who know how to perform traditional regression and
            classification tasks but find themselves stuck when it comes to time series. If you have been dropping the
            date column up until now, this book is definitely for you! </p>
        <p>The book is also for professionals proficient in R looking to transition to Python. R</p>
        <p>is a great language for time series forecasting, and many methods have been implemented in
            R. However, Python is the most popular language for data science, and it has the advantage of being applied
            to deep learning models, which is something R</p>
        <p>can't do. </p>
        <p></p>
        <p><b>xx</b></p>
        <p><a id="calibre_link-311"></a>ABOUT THIS BOOK</p>
        <p><b>xxi</b></p>
        <p> <i><b>How this book is organized: A roadmap</b></i></p>
        <p>The book has 4 parts and 21 chapters. </p>
        <p>Part 1 is an introduction to time series forecasting. We'll formalize the concept of time
            series data, develop baseline models, and see when forecasting is not a reasonable avenue:</p>
        <p> Chapter 1 defines time series data and explores the lifecycle of a forecasting project.
        </p>
        <p> In chapter 2 we'll develop baseline models, as a model can only be evaluated in relation
            to another model. It is therefore important to first have a simple forecasting model before moving on to
            more complex techniques. </p>
        <p> In chapter 3 we'll study the random walk model, which is a special scenario where
            forecasting cannot reasonably be performed with advanced models, and</p>
        <p>we must resort to simple baseline models. </p>
        <p>Part 2 focuses on forecasting with statistical models:</p>
        <p> In chapter 4 we'll develop the moving average model, MA( <i>q</i>), one
            of the building blocks of more complex forecasting techniques. </p>
        <p> In chapter 5 we'll develop the autoregressive model, AR( <i>p</i>), the
            other foundational model for more complicated scenarios. </p>
        <p> In chapter 6 we'll combine the AR( <i>p</i>) and MA( <i>q</i>) models to
            form the ARMA( <i>p</i>, <i>q</i>)
            model and design a new forecasting procedure. </p>
        <p> In chapter 7 we'll build on the previous chapter to model non-stationary time</p>
        <p>series with the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            model. </p>
        <p> In chapter 8 we'll add yet another layer of complexity and model seasonal time series with
            the SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model. </p>
        <p> In chapter 9 we'll add the last layer of complexity and reach the SARIMAX</p>
        <p>model, allowing us to use external variables to forecast our data. </p>
        <p> In chapter 10 we'll explore vector autoregression, VAR( <i>p</i>),
            models, which allow us to forecast many time series simultaneously. </p>
        <p> Chapter 11 concludes part 2 with a capstone project, giving us the chance to apply what we
            learned since chapter 4. </p>
        <p>Part 3 covers forecasting with deep learning. When your dataset becomes very large, with
            nonlinear relationships and high dimensionality, deep learning is the most appropriate tool for forecasting:
        </p>
        <p> Chapter 12 introduces deep learning and the types of models we can build. </p>
        <p> Chapter 13 explores the data windowing step, which is crucial to ensuring the</p>
        <p>success of forecasting using deep learning models. </p>
        <p> In chapter 14 we'll develop our first simple deep learning models. </p>
        <p> In chapter 15 we'll use the LSTM architecture for forecasting. This architecture is
            specifically built to process sequential data, just like time series. </p>
        <p><a id="calibre_link-312"></a><b>xxii</b></p>
        <p>ABOUT THIS BOOK</p>
        <p> In chapter 16 we'll explore the CNN architecture, which can effectively filter the noise
            in a time series with the convolution operation. We'll also combine the CNN with the LSTM architecture. </p>
        <p> In chapter 17 we'll develop an autoregressive deep learning model, which is an
            architecture that is proven to generate state-of-the-art results, as the model's output is fed back in as an
            input to produce the next forecast. </p>
        <p> In chapter 18 we'll conclude part 3 with a capstone project. </p>
        <p>Part 4 explores the use of automated forecasting libraries, especially Prophet, as it is one
            of the most widely used libraries in the industry:</p>
        <p> Chapter 19 explores the ecosystem of automated forecasting libraries, and we'll work
            through a project using Prophet. We'll also use a SARIMAX model to compare the performance of both methods.
        </p>
        <p> Chapter 20 is a capstone project where you are invited to use Prophet and a SARIMAX model
            and see which performs best in that situation. </p>
        <p> Chapter 21 concludes the book and aims to inspire you to go above and beyond</p>
        <p>and explore what else can be done with time series data. </p>
        <p> <i><b>About the code</b></i></p>
        <p>This book contains many examples of source code both in numbered listings and in line with
            normal text. In both cases, source code is formatted in a fixed-width font like this to separate it from
            ordinary text. </p>
        <p>In many cases, the original source code has been reformatted; we've added line breaks and
            reworked indentation to accommodate the available page space in the book. In some cases, even this was not
            enough, and listings include line-continuation markers (➥). Additionally, comments in the source code have
            often been removed</p>
        <p>from the listings when the code is described in the text. Code annotations accompany many of
            the listings, highlighting important concepts. </p>
        <p>You can get executable snippets of code from the liveBook (online) version of this book <a
                href="https://livebook.manning.com/book/time-series-forecasting-in-python-book/">at
                https://livebook.manning.com/book/time-series-forecasting-in-python-book/. </a></p>
        <p>The entire source code for this book is available on GitHub at <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython">https://github.com/</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython">marcopeix/TimeSeriesForecastingInPython.
            </a> You can also find the solutions to all the exercises there, and the code for the figures is also
            included. Creating visualizations is sometimes an overlooked skill, but I believe it is an important one.
        </p>
        <p>All the code was run on Windows using Jupyter Notebooks in Anaconda. I used Python 3.7, but
            any later release should work as well. </p>
        <p> <i><b>liveBook discussion forum</b></i></p>
        <p>Purchase of <i>Time Series Forecasting in Python</i> includes free access
            to liveBook, Manning's online reading platform. Using liveBook's exclusive discussion features, you can
            attach comments to the book globally or to specific sections or paragraphs. It's a snap to make notes for
            yourself, ask and answer technical questions, and receive help from</p>
        <p><a id="calibre_link-313"></a>ABOUT THIS BOOK</p>
        <p><b>xxiii</b></p>
        <p>the author and other users. To access the forum, go to <a
                href="https://livebook.manning.com/book/time-series-forecasting-in-python-book/discussion">https://livebook.manning.com/</a>
        </p>
        <p><a
                href="https://livebook.manning.com/book/time-series-forecasting-in-python-book/discussion">book/time-series-forecasting-in-python-book/discussion</a>.
            You can also learn more about Manning's forums and the rules of conduct <a
                href="https://livebook.manning.com/discussion">at https://livebook.manning.com/discussion</a>. </p>
        <p>Manning's commitment to our readers is to provide a venue where a meaningful</p>
        <p>dialogue between individual readers and between readers and the author can take place. It is
            not a commitment to any specific amount of participation on the part of the author, whose contribution to
            the forum remains voluntary (and unpaid). We suggest you try asking the author some challenging questions
            lest his interest stray! The forum and the archives of previous discussions will be accessible from the
            publisher's website as long as the book is in print. </p>
        <p> <i><b>Author online</b></i></p>
        <p>You can follow me on Medium for more articles on data science <a
                href="https://medium.com/@marcopeixeiro">(https://medium</a></p>
        <p><a href="https://medium.com/@marcopeixeiro">.com/@marcopeixeiro)</a>. My approach to
            blogging is similar to how I approached this book: theory first and a hands-on project second. You can also
            reach out to me on LinkedIn (<a
                href="https://www.linkedin.com/in/marco-peixeiro/">https://www.linkedin.com/in/marco-peixeiro/</a>).
        </p>
        <p><a id="calibre_link-29"></a><img src="images/000019.png" alt="Image 3" class="calibre2" />
        </p>
        <p> <i>about the author</i></p>
        <p><b>MARCO PEIXEIRO</b> is a senior data scientist at one of Canada's</p>
        <p>largest banks. Being self-taught, he is especially aware of what</p>
        <p>one needs to know to land a job and work in the industry. </p>
        <p>Marco is a big proponent of hands-on approaches to learning, </p>
        <p>which is the approach taken in his Medium blog, his freeCode-</p>
        <p>Camp crash course on data science, and in his Udemy course. </p>
        <p><b>xxiv</b></p>
        <p><a id="calibre_link-30"></a> <i>about the cover illustration</i></p>
        <p>The figure on the cover of <i>Time Series Forecasting in Python</i> is
            captioned “Homme de Kamtschatka,” or “Kamchatka man,” taken from a collection by Jacques Grasset de
            Saint-Sauveur, published in 1797. Each illustration is finely drawn and colored by hand. </p>
        <p>In those days, it was easy to identify where people lived and what their trade or station in
            life was just by their dress. Manning celebrates the inventiveness and initiative of the computer business
            with book covers based on the rich diversity of regional cul-ture centuries ago, brought back to life by
            pictures from collections such as this one. </p>
        <p><b>xxv</b></p>
        <p><a id="calibre_link-336"></a> </p>
        <p><a id="calibre_link-1"></a> <i>Part 1</i></p>
        <p> <i>Time waits for no one</i></p>
        <p>Very few phenomena are unaffected by time, which in itself is enough to jus-</p>
        <p>tify the importance of understanding what time series are. In this first part of the book,
            we'll define time series and explore the particularities of working with them. We'll also develop our very
            first forecasting models using naive methods. </p>
        <p>These will serve as baseline models, and we'll reuse these techniques throughout the book.
            Finally, we'll study a situation where forecasting is not possible, so that we identify and avoid falling
            into that trap. </p>
        <p><a id="calibre_link-337"></a> </p>
        <p><a id="calibre_link-2"></a> <i>Understanding</i></p>
        <p> <i>time series forecasting</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Introducing time series</p>
        <p> Understanding the three main components of </p>
        <p>a time series</p>
        <p> The steps necessary for a successful forecasting </p>
        <p>project</p>
        <p> How forecasting time series is different from </p>
        <p>other regression tasks</p>
        <p>Time series exist in a variety of fields from meteorology to finance, econometrics, and
            marketing. By recording data and analyzing it, we can study time series to analyze industrial processes or
            track business metrics, such as sales or engagement. </p>
        <p>Also, with large amounts of data available, data scientists can apply their expertise to
            techniques for time series forecasting. </p>
        <p>You might have come across other courses, books, or articles on time series that implement
            their solutions in R, a programming language specifically made for statistical computing. Many forecasting
            techniques make use of statistical models, as you will learn in chapter 3 and onwards. Thus, a lot of work
            was done to develop packages to make time series analysis and forecasting seamless using R. However, <b
                class="calibre4">3</b></p>
        <p><a id="calibre_link-304"></a><b>4</b></p>
        <p>CHAPTER 1</p>
        <p> <i><b>Understanding time series forecasting</b></i></p>
        <p>most data scientists are required to be proficient with Python, as it is the most
            wide-spread language in the field of machine learning. In recent years, the community and large companies
            have developed powerful libraries that leverage Python to perform statistical computing and machine learning
            tasks, develop websites, and much more. </p>
        <p>While Python is far from being a perfect programming language, its versatility is a strong
            benefit to its users, as we can develop models, perform statistical tests, and possibly serve our models
            through an API or develop a web interface, all while using the same programming language. This book will
            show you how to implement both statistical learning techniques and machine learning techniques for time
            series forecasting using only Python. </p>
        <p>This book will focus entirely on time series forecasting. You will first learn how to make
            simple forecasts that will serve as benchmarks for more complex models. </p>
        <p>Then we will use two statistical learning techniques, the moving average model and the
            autoregressive model, to make forecasts. These will serve as the foundation for the more complex modeling
            techniques we will cover that will allow us to account for non-stationarity, seasonality effects, and the
            impact of exogenous variables. </p>
        <p>Afterwards, we'll switch from statistical learning techniques to deep learning methods, in
            order to forecast very large time series with a high dimensionality, a scenario in which statistical
            learning often does not perform as well as its deep learning counterpart. </p>
        <p>For now, this chapter will examine the basic concepts of time series forecasting. I'll start
            by defining time series so that you can recognize one. Then, we will move on and discuss the purpose of time
            series forecasting. Finally, you will learn why forecasting a time series is different from other regression
            problems, and thus why the subject deserves its own book. </p>
        <p> <i><b>1.1</b></i></p>
        <p> <i><b>Introducing time series</b></i></p>
        <p>The first step in understanding and performing time series forecasting is learning what a
            time series is. In short, a <i>time series</i> is simply a set of data points ordered in
            time. </p>
        <p>Furthermore, the data is often equally spaced in time, meaning that equal intervals separate
            each data point. In simpler terms, the data can be recorded at every hour or every minute, or it could be
            averaged over every month or year. Some typical examples of time series include the closing value of a
            particular stock, a household's electricity consumption, or the temperature outside. </p>
        <p>Time series</p>
        <p>A time series is a set of data points ordered in time. </p>
        <p>The data is equally spaced in time, meaning that it was recorded at every hour, minute,
            month, or quarter. Typical examples of time series include the closing value of a stock, a household's
            electricity consumption, or the temperature outside. </p>
        <p><a id="calibre_link-31"></a><img src="images/000098.jpg" alt="Image 4" class="calibre2" />
        </p>
        <p> <i><b>1.1</b></i></p>
        <p> <i><b>Introducing time series</b></i></p>
        <p><b>5</b></p>
        <p>Let's consider a dataset representing the quarterly earnings per share in US dollars of
            Johnson &amp; Johnson stock from 1960 to 1980, shown in figure 1.1. We will use this dataset often
            throughout this book, as it has many interesting properties that will help you learn advanced techniques for
            more complex forecasting problems. </p>
        <p>As you can see, figure 1.1 clearly represents a time series. The data is indexed by time, as
            marked on the horizontal axis. Also, the data is equally spaced in time, since it was recorded at the end of
            every quarter of each year. We can see that the data has a trend, since the values are increasing over time.
            We also see the earnings going up and down over the course of each year, and the pattern repeats every year.
        </p>
        <p>Figure 1.1</p>
        <p>Quarterly earnings of Johnson &amp; Johnson in USD from 1960 to 1980 </p>
        <p>showing a positive trend and a cyclical behavior</p>
        <p> <i><b>1.1.1</b></i></p>
        <p> <i><b>Components of a time series</b></i></p>
        <p>We can further our understanding of time series by looking at their three components: a
            trend, a seasonal component, and residuals. In fact, all time series can be decomposed into these three
            elements. </p>
        <p>Visualizing the components of a time series is known as decomposition. <i>Decomposition</i>
            is defined as a statistical task that separates a time series into
            its different components. We can visualize each individual component, which will help us identify the trend
            and seasonal pattern in the data, which is not always straightforward just by looking at a dataset. </p>
        <p>Let's take a closer look at the decomposition of Johnson &amp; Johnson quarterly earnings
            per share, shown in figure 1.2. You can see how the Observed data was split into Trend, Seasonal, and
            Residuals. Let's study each piece of the graph in more detail. </p>
        <p><a id="calibre_link-338"></a><img src="images/000046.jpg" alt="Image 5" class="calibre2" />
        </p>
        <p><img src="images/000166.jpg" alt="Image 6" class="calibre2" /></p>
        <p><b>6</b></p>
        <p>CHAPTER 1</p>
        <p> <i><b>Understanding time series forecasting</b></i></p>
        <p>Figure 1.2</p>
        <p>Decomposition of quarterly earnings of Johnson &amp; Johnson from 1960 to </p>
        <p>1980</p>
        <p>First, the top graph, labeled as Observed, simply shows the time series as it was recorded
            (figure 1.3). The <i>y</i>-axis displays the value of the quarterly earnings per share for
            Johnson &amp; Johnson in US dollars, while the <i>x</i>-axis represents time. It is
            basically a recre-ation of figure 1.1, and it shows the result of combining the Trend, Seasonal, and
            Residuals graphs from figure 1.2. </p>
        <p>Figure 1.3</p>
        <p>Focusing on the Observed plot</p>
        <p>Then we have the trend component, as shown in figure 1.4. Again, keep in mind that the <i
                class="calibre3">y</i>-axis represents the value, while the <i>x</i>-axis still refers
            to time. The trend is defined as the slow-moving changes in a time series. We can see that it starts out
            flat and then steeply goes up, meaning that we have an increasing, or positive, trend in our data. </p>
        <p>The trend component is sometimes referred to as the <i>level</i>. We can
            think of the trend</p>
        <p><a id="calibre_link-339"></a><img src="images/000034.jpg" alt="Image 7" class="calibre2" />
        </p>
        <p><img src="images/000167.jpg" alt="Image 8" class="calibre2" /></p>
        <p> <i><b>1.1</b></i></p>
        <p> <i><b>Introducing time series</b></i></p>
        <p><b>7</b></p>
        <p>Figure 1.4</p>
        <p>Focusing on the trend component. We have a trend in our series, since the </p>
        <p>component is not flat. It indicates that we have increasing values over time. </p>
        <p>component as trying to draw a line through most of the data points to show the general
            direction of a time series. </p>
        <p>Next we see the seasonal component in figure 1.5. The seasonal component cap-</p>
        <p>tures the seasonal variation, which is a cycle that occurs over a fixed period of time. </p>
        <p>We can see that over the course of a year, or four quarters, the earnings per share start
            low, increase, and decrease again at the end of the year. </p>
        <p>Figure 1.5</p>
        <p>Focusing on the seasonal component. Here we have periodic fluctuations in </p>
        <p>our time series, which indicates that earnings go up and down every year. </p>
        <p>Notice how the <i>y</i>-axis shows negative values. Does this mean that the
            earnings per share are negative? Clearly, that cannot be, since our dataset strictly has positive values.
            Therefore, we can say that the seasonal component shows how we deviate from the trend. Sometimes we have a
            positive deviation, and we get a peak in the Observed graph. Other times, we have a negative deviation, and
            we see a trough in Observed. </p>
        <p>Finally, the last graph in figure 1.2 shows the residuals, which is what cannot be explained
            by either the trend or the seasonal components. We can think of the residuals as adding the Trend and
            Seasonal graphs together and comparing the value at each point in time to the Observed graph. For certain
            points, we might get the exact same value as in Observed, in which case the residual will be zero. In other
            cases, the value is different from the one in Observed, so the Residuals graph shows what value must be
            added to Trend and Seasonal in order to adjust the result and get the same value as in Observed. Residuals
            usually correspond to random errors, also termed <i>white noise</i>, as we will discuss in
            chapter 3. They represent information that we cannot model or predict, since it is completely random, as
            shown in figure 1.6. </p>
        <p><a id="calibre_link-32"></a><img src="images/000090.jpg" alt="Image 9" class="calibre2" />
        </p>
        <p><b>8</b></p>
        <p>CHAPTER 1</p>
        <p> <i><b>Understanding time series forecasting</b></i></p>
        <p>Figure 1.6</p>
        <p>Focusing on the residuals. The residuals are what cannot be explained by </p>
        <p>the trend and seasonal components. </p>
        <p>Time series decomposition</p>
        <p>Time series decomposition is a process by which we separate a time series into its
            components: trend, seasonality, and residuals. </p>
        <p>The trend represents the slow-moving changes in a time series. It is responsible for making
            the series gradually increase or decrease over time. </p>
        <p>The seasonality component represents the seasonal pattern in the series. The cycles occur
            repeatedly over a fixed period of time. </p>
        <p>The residuals represent the behavior that cannot be explained by the trend and seasonality
            components. They correspond to random errors, also termed white noise. </p>
        <p>Already we can intuitively see how each component affects our work when forecasting. </p>
        <p>If a time series exposes a certain trend, then we'll expect it to continue in the future.
        </p>
        <p>Similarly, if we observe a strong seasonality effect, this is likely going to continue, and
            our forecasts must reflect that. Later in the book, you'll see how to account for these components and
            include them in your models to forecast more complex time series. </p>
        <p> <i><b>1.2</b></i></p>
        <p> <i><b>Bird's-eye view of time series forecasting</b></i>
        </p>
        <p> <i>Forecasting</i> is predicting the future using historical data and
            knowledge of future events that might affect our forecasts. This definition is full of promises and, as data
            scientists, we are often very eager to start forecasting by using our scientific knowledge to showcase an
            incredible model with a near-perfect forecast accuracy. However, there are important steps that must be
            covered before reaching the point of forecasting. </p>
        <p>Figure 1.7 is a simplified diagram of what a complete forecasting project might look like in
            a professional setting. Note that these steps are not universal, and they may or may not be followed,
            depending on the organization and its maturity. These steps are nonetheless essential to ensure good
            cohesion between the data team and the business team, hence providing business value and avoiding friction
            and frustra-tion between the teams. </p>
        <p>Let's dive into a scenario that covers each step of a forecasting project roadmap in detail.
            Imagine you are planning a one-week camping trip one month from now, and</p>
        <p><a id="calibre_link-33"></a> <i><b>1.2</b></i></p>
        <p> <i><b>Bird's-eye view of time series forecasting</b></i>
        </p>
        <p><b>9</b></p>
        <p>Set a goal</p>
        <p>Determine what must be</p>
        <p>forecast to achieve our</p>
        <p>goal</p>
        <p>Set the horizon of the</p>
        <p>forecast</p>
        <p>Gather the data</p>
        <p>Figure 1.7</p>
        <p>Forecasting project roadmap. The </p>
        <p>first step is naturally to set a goal that justifies the </p>
        <p>Develop a forecasting</p>
        <p>need for forecasting. Then you must determine </p>
        <p>Collect new data</p>
        <p>model</p>
        <p>what needs to be forecast in order to achieve that </p>
        <p>goal. Then you set the horizon of the forecast. </p>
        <p>Once that's done, you can gather the data and </p>
        <p>develop a forecasting model. Then the model </p>
        <p>is deployed to production, its performance is </p>
        <p>monitored, and new data is collected in order </p>
        <p>Deploy to production</p>
        <p>Monitor</p>
        <p>to retrain the forecasting model and make sure </p>
        <p>it is still relevant. </p>
        <p>you want to know which sleeping bag to bring with you so you can sleep comfortably at night.
        </p>
        <p> <i><b>1.2.1</b></i></p>
        <p> <i><b>Setting a goal</b></i></p>
        <p>The very first step in any project roadmap is to set a goal. Here it is explicit in the
            scenario: you want to know which sleeping bag to bring to sleep comfortably at night. If the nights will be
            cold, a warm sleeping bag is the best choice. Of course, if nights are expected to be warm, then a light
            sleeping bag would be the better option. </p>
        <p> <i><b>1.2.2</b></i></p>
        <p> <i><b>Determining what must be forecast to achieve your
                    goal</b></i></p>
        <p>Then you move to determining what must be forecast in order for you to decide which sleeping
            bag to bring. In this case, you need to predict the temperature at night. To simplify things, let's consider
            that predicting the minimum temperature is sufficient to make a decision, and that the minimum temperature
            occurs at night. </p>
        <p><a id="calibre_link-34"></a><b>10</b></p>
        <p>CHAPTER 1</p>
        <p> <i><b>Understanding time series forecasting</b></i></p>
        <p> <i><b>1.2.3</b></i></p>
        <p> <i><b>Setting the horizon of the forecast</b></i></p>
        <p>Now you can set the horizon of your forecast. In this case, your camping trip is one month
            from now, and it will last for one week. Therefore, you have a horizon of one week, since you are only
            interested in predicting the minimum temperature during the camping trip. </p>
        <p> <i><b>1.2.4</b></i></p>
        <p> <i><b>Gathering the data</b></i></p>
        <p>You can now start gathering your data. For example, you could collect historical daily
            minimum temperature data. You could also gather data on possible factors that can influence temperature,
            such as humidity and wind speed. </p>
        <p>This is when the question of how much data is enough data arises. Ideally, you would collect
            more than 1 year of data. That way, you could determine if there is a yearly seasonal pattern or a trend. In
            the case of temperature, you can of course expect some seasonal pattern over the year, since different
            seasons bring different minimum temperatures. </p>
        <p>However, 1 year of data is not the ultimate answer to how much data is sufficient. It highly
            depends on the frequency of the forecasts. In this case, you will be creating daily forecasts, so 1 year of
            data should be enough. </p>
        <p>If you wanted to create hourly forecasts, a few months of training data would be enough, as
            it would contain a lot of data points. If you were creating monthly or yearly forecasts, you would need a
            much larger historical period to have enough data points to train with. </p>
        <p>In the end, there is no clear answer regarding the quantity of data required to train a
            model. Determining this is part of the experimentation process of building a model, assessing its
            performance, and testing whether more data improves the model's performance. </p>
        <p> <i><b>1.2.5</b></i></p>
        <p> <i><b>Developing a forecasting model</b></i></p>
        <p>With your historical data in hand, you are ready to develop a forecasting model. This part
            of the project roadmap is the focus of this entire book. This is when you get to study the data and
            determine whether there is a trend or a seasonal pattern. </p>
        <p>If you observe seasonality, then a SARIMA model would be relevant, because this</p>
        <p>model uses seasonal effects to produce forecasts. If you have information on wind speed and
            humidity, you could take that into account using the SARIMAX model, because you can feed it with information
            from exogenous variables, such as wind speed and humidity. We will explore these models in detail in
            chapters 8 and 9. </p>
        <p>If you managed to collect a large amount of data, such as the daily minimum tem-</p>
        <p>perature of the last 20 years, you could use neural networks to leverage this very large
            amount of training data. Unlike statistical learning methods, deep learning tends to produce better models,
            as more data is used for training. </p>
        <p>Whichever model you develop, you will use part of the training data as a test set to
            evaluate your model's performance. The test set will always be the most recent data points, and it must be
            representative of the forecasting horizon. </p>
        <p><a id="calibre_link-35"></a> <i><b>1.2</b></i></p>
        <p> <i><b>Bird's-eye view of time series forecasting</b></i>
        </p>
        <p><b>11</b></p>
        <p>In this case, since your horizon is one week, you can remove the last seven data points from
            your training set to place them in a test set. Then, when each model is trained, you can produce one-week
            forecasts and compare the results to the test set. </p>
        <p>The model's performance can be assessed by computing an error metric, such as the mean
            squared error (MSE). This is a way to evaluate how far your predictions are from the real values. The model
            with the lowest MSE will be your best-performing model, and it is the one that will move on to the next
            step. </p>
        <p> <i><b>1.2.6</b></i></p>
        <p> <i><b>Deploying to production</b></i></p>
        <p>Once you have your champion model, you must deploy it to production. This means</p>
        <p>that your model can take in data and return a prediction for the minimum daily temperature
            for the next 7 days. There are many ways to deploy a model to production, and this could be the subject of
            an entire book. Your model could be served as an API or integrated in a web application, or you could define
            your own Excel function to run your model. Ultimately, your model is considered deployed when you can feed
            in data and have forecasts returned without any manual manipulation of the data. At this point, your model
            can be monitored. </p>
        <p> <i><b>1.2.7</b></i></p>
        <p> <i><b>Monitoring</b></i></p>
        <p>Since the camping trip is 1 month from now, you can see how well your model per-</p>
        <p>forms. Every day, you can compare your model's forecast to the actual minimum temperature
            recorded for the day. This allows you to determine the quality of the model's forecasts. </p>
        <p>You can also look for unexpected events. For example, a heat wave can arise, degrading the
            quality of your model's forecasts. Closely monitoring your model and current events allows you to determine
            if the unexpected event results from a tempo-rary situation, or if it will last for the next 2 months, in
            which case it could impact your decision for the camping trip. </p>
        <p> <i><b>1.2.8</b></i></p>
        <p> <i><b>Collecting new data</b></i></p>
        <p>By monitoring your model, you necessarily collect new data as you compare the model's
            forecasts to the observed minimum temperature for the day. This new, more recent, data can then be used in
            retraining your model. That way, you have up-to-date data you can use to forecast the minimum temperature
            for the next 7 days. </p>
        <p>This cycle is repeated over the next month until you reach the day of the camping trip, as
            shown in figure 1.8. By that point, you will have made many forecasts, assessed their quality against newly
            observed data, and retrained your model with new daily minimum temperatures as you recorded them. That way,
            you make sure that your model is still performant and uses relevant data to forecast the temperature for
            your camping trip. </p>
        <p>Finally, based on your model's predictions, you can decide which sleeping bag to bring with
            you. </p>
        <p><a id="calibre_link-36"></a><b>12</b></p>
        <p>CHAPTER 1</p>
        <p> <i><b>Understanding time series forecasting</b></i></p>
        <p>Develop a forecasting</p>
        <p>Collect new data</p>
        <p>model</p>
        <p>Figure 1.8</p>
        <p>Visualizing the production loop. </p>
        <p>Once the model is in production, you enter a </p>
        <p>cycle where you monitor it, collect new data, </p>
        <p>Deploy to production</p>
        <p>Monitor</p>
        <p>and use that data to adjust the forecasting </p>
        <p>model before deploying it again. </p>
        <p> <i><b>1.3</b></i></p>
        <p> <i><b>How time series forecasting is different from
                </b></i></p>
        <p> <i><b>other regression tasks</b></i></p>
        <p>You probably have encountered regression tasks where you must predict some continuous target
            given a certain set of features. At first glance, time series forecasting seems like a typical regression
            problem: we have some historical data, and we wish to build a mathematical expression that will express
            future values as a function of past values. </p>
        <p>However, there are some key differences between time series forecasting and regression for
            time-independent scenarios that deserve to be addressed before we look at our very first forecasting
            technique. </p>
        <p> <i><b>1.3.1</b></i></p>
        <p> <i><b>Time series have an order</b></i></p>
        <p>The first concept to keep in mind is that time series have an order, and we cannot change
            that order when modeling. In time series forecasting, we express future values as a function of past values.
            Therefore, we must keep the data in order, so as to not vio-late this relationship. </p>
        <p>Also, it makes sense to keep the data in order because your model can only use</p>
        <p>information from the past up until the present&mdash;it will not know what will be observed
            in the future. Recall your camping trip. If you want to predict the temperature for Tuesday, you cannot
            possibly use the information from Wednesday, since it is in the future from the model's point of view. You
            would only be able to use the data from Monday and before. That is why the order of the data must remain the
            same throughout the modeling process. </p>
        <p>Other regression tasks in machine learning often do not have an order. For exam-</p>
        <p>ple, if you are tasked to predict revenue based on ad spend, it does not matter when a
            certain amount was spent on ads. Instead, you simply want to relate the amount of ad spend to the revenue.
            In fact, you might even randomly shuffle the data to make your model more robust. Here the regression task
            is to simply derive a function such that given an amount on ad spend, an estimate of revenue is returned.
        </p>
        <p>On the other hand, time series are indexed by time, and that order must be kept. </p>
        <p>Otherwise, you would be training your model with future information that it would not have
            at prediction time. This is called <i>look-ahead bias</i> in more formal terms. The
            resulting model would therefore not be reliable and would most probably perform poorly when you make future
            forecasts. </p>
        <p><a id="calibre_link-37"></a> <i><b>1.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>13</b></p>
        <p> <i><b>1.3.2</b></i></p>
        <p> <i><b>Time series sometimes do not have features</b></i>
        </p>
        <p>It is possible to forecast time series without the use of features other than the time
            series itself. </p>
        <p>As data scientists, we are used to having datasets with many columns, each representing a
            potential predictor for our target. For example, consider the task of predicting revenue based on ad spend,
            where the revenue is the target variable. As features, we could have the amount spent on Google ads,
            Facebook ads, and television ads. </p>
        <p>Using these three features, we would build a regression model to estimate revenue. </p>
        <p>However, with time series, it is quite common to be given a simple dataset with a time
            column and a value at that point in time. Without any other features, we must learn ways of using past
            values of the time series to forecast future values. This is when the moving average model (chapter 4) or
            autoregressive model (chapter 5) come into play, as they are ways to express future values as a function of
            past values. These models are foundational to the more complex models that then allow you to consider
            seasonal patterns and trends in time series. Starting in chapter 6, we will gradually build on those basic
            models to forecast more complex time series. </p>
        <p> <i><b>1.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>This book will cover different forecasting techniques in detail. We'll start with some very
            basic methods, such as the moving average model and autoregressive model, and we will gradually account for
            more factors in order to forecast time series with trends and seasonal patterns using the ARIMA, SARIMA, and
            SARIMAX models. We will also</p>
        <p>work with time series with high dimensionality, which will require us to use deep learning
            techniques for sequential data. Therefore, we will have to build neural networks using CNN (convolutional
            neural network) and LSTM (long short-term mem-</p>
        <p>ory). Finally, you will learn how to automate the work of forecasting time series. As
            mentioned, all implementations throughout the book will be done in Python. </p>
        <p>Now that you have learned what a time series is and how forecasting will be different than
            any traditional regression tasks you might have seen before, we are ready to move on and start forecasting.
            However, our first attempt at forecasting will focus on naive methods that will serve as baseline models.
        </p>
        <p> <i><b>Summary</b></i></p>
        <p> A time series is a set of data points ordered in time. </p>
        <p> Examples of time series are the closing price of a stock or the temperature outside. </p>
        <p> Time series can be decomposed into three components: a trend, a seasonal com-</p>
        <p>ponent, and residuals. </p>
        <p> It is important to have a goal when forecasting and to monitor the model once</p>
        <p>it's deployed. This will ensure the success and longevity of the project. </p>
        <p> Never change the order of a time series when modeling. Shuffling the data is</p>
        <p>not allowed. </p>
        <p><i>Chapter 2</i></p>
        <p><i>A naive prediction of the future</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Defining a baseline model </p>
        <p> Setting a baseline using the mean</p>
        <p> Building a baseline using the mean of the previous window of time</p>
        <p> Creating a baseline using the previous timestep</p>
        <p> Implementing the naive seasonal forecast</p>
        <p>In chapter 1 we covered what time series are and how forecasting a time series is different
            from a traditional regression task. You also learned the necessary steps in building a successful
            forecasting project, from defining a goal to building a model, deploying it, and updating it as new data is
            collected. Now you are ready to start forecasting a time series. </p>
        <p>You will first learn how to make a naive prediction of the future, which will serve as a
            baseline. The baseline model is a trivial solution that uses heuristics, or simple statistics, to compute a
            forecast. Developing a baseline model is not always an exact science. It will often require some intuition
            that we'll gain by visualizing the data and detecting patterns that can be used to make predictions. In any
            modeling project, it is important to have a baseline, as you can use it to compare the performanceof the
            more complex models you'll build down the road. The only way to know that a model is
            good or performant, is to compare it to a baseline. </p>


        <p>In this chapter, let's imagine that we wish to predict the quarterly earnings per share
            (EPS) of Johnson &amp; Johnson. We can look at the dataset in figure 2.1, which is identical to what you saw
            in chapter 1. Specifically, we will use the data from 1960 to the end of 1979 in order to predict the EPS
            for the four quarters of 1980. The forecasting period is illustrated by the gray zone in figure 2.1. </p>
        <p><a id="calibre_link-239"></a><img src="images/000150.jpg" alt="Image 10" class="calibre2" />
        </p>
        <p>Figure 2.1</p>
        <p>Quarterly earnings per share of Johnson &amp; Johnson in US dollars (USD) </p>
        <p>between 1960 and 1980. We will use the data from 1960 to the last quarter of 1979 </p>
        <p>to build a baseline model that will forecast the earnings per share for the quarters of 1980
            (as illustrated by the gray area). </p>
        <p>You can see in figure 2.1 that our data has a trend, since it is increasing over time. </p>
        <p>Also, we have a seasonal pattern, since over the course of a year, or four quarters, we can
            observe peaks and troughs repeatedly. This means that we have seasonality. </p>
        <p>Recall that we identified each of these components when we decomposed our time</p>
        <p>series in chapter 1. The components are shown in figure 2.2. We will study some of these
            components in detail later in the chapter, as they will help us gain some intuition about the behavior of
            the data, which in turn will help us develop a good baseline model. </p>
        <p>We will first define what a baseline model is, and then we will develop four different
            baselines to forecast the quarterly EPS of Johnson &amp; Johnson. This is the time when we'll finally get
            our hands dirty with Python and time series forecasting. </p>
        <p><a id="calibre_link-216"></a><img src="images/000126.jpg" alt="Image 11" class="calibre2" />
        </p>
        <p><b>16</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>Figure 2.2</p>
        <p>Decomposition of quarterly earnings of Johnson &amp; Johnson from 1960 to 1980</p>
        <p> <i><b>2.1</b></i></p>
        <p> <i><b>Defining a baseline model</b></i></p>
        <p>A <i>baseline model</i> is a trivial solution to our problem. It often uses
            heuristics, or simple statistics, to generate predictions. The baseline model is the simplest solution you
            can think of&mdash;it should not require any training, and the cost of implementation should be very low.
        </p>
        <p>Can you think of a baseline for our project? </p>
        <p>Knowing that we want to forecast the EPS for Johnson &amp; Johnson, what is the most basic,
            most naive, forecast you can make? </p>
        <p>In the context of time series, one simple statistic we can use to build a baseline is the
            arithmetic mean. We can simply compute the mean of the values over a certain period and assume that future
            values will be equal to that mean. In the context of predicting the EPS for Johnson &amp; Johnson, this is
            like saying</p>
        <p> <i>The average EPS between 1960 and 1979 was $4.31. Therefore, I expect
                the EPS over the</i> <i>next four quarters of 1980 to be equal to $4.31 per quarter.
            </i></p>
        <p>Another possible baseline is to naively forecast the last recorded data point. In our
            context, this would be like saying</p>
        <p> <i>If the EPS is $0.71 for this quarter, then the EPS will also be $0.71
                for next quarter. </i></p>
        <p><a id="calibre_link-38"></a> <i><b>2.2</b></i></p>
        <p> <i><b>Forecasting the historical mean</b></i></p>
        <p><b>17</b></p>
        <p>Or, if we see a cyclical pattern in our data, we can simply repeat that pattern into the
            future. Staying in the context of Johnson &amp; Johnson, this is like saying</p>
        <p> <i>If the EPS is $14.04 for the first quarter of 1979, then the EPS for
                the first quarter of</i> <i>1980 will also be $14.04. </i></p>
        <p>You can see these three possible baselines rely on simple statistics, heuristics, and
            patterns observed in our dataset. </p>
        <p>Baseline model</p>
        <p>A baseline model is a trivial solution to your forecasting problem. It relies on heuristics
            or simple statistics and is usually the simplest solution. It does not require model fitting, and it is easy
            to implement. </p>
        <p>You might wonder if those baseline models are any good. How well can those simple methods
            forecast the future? We can answer this question by forecasting for the year of 1980 and testing our
            forecasts against the observed data in 1980. This is called <i>out-of-sample</i>
            forecasting because we are making predictions for a period that was not taken into account when the model
            was developed. That way we can measure the performance of our models and see how they would perform when we
            forecast beyond</p>
        <p>the data we have, which in this case is 1981 and later. </p>
        <p>In the next sections, you will learn how to develop the different baselines mentioned here
            to predict the quarterly EPS of Johnson &amp; Johnson. </p>
        <p> <i><b>2.2</b></i></p>
        <p> <i><b>Forecasting the historical mean</b></i></p>
        <p>As mentioned at the beginning of the chapter, we are going to work with the quarterly EPS in
            US dollars (USD) of Johnson &amp; Johnson from 1960 to 1980. Our goal is to use the data from 1960 to the
            end of 1979 to predict the four quarters of 1980. The first baseline we'll discuss uses the historical mean,
            which is the arithmetic mean of past values. Its implementation is straightforward: calculate the mean of
            the training set, and it will be our prediction for the four quarters of 1980. First, though, we need to do
            some preliminary work that we'll use in all of our baseline implementations. </p>
        <p> <i><b>2.2.1</b></i></p>
        <p> <i><b>Setup for baseline implementations</b></i></p>
        <p>Our first step is to load the dataset. To do so, we will use the pandas library and load the
            dataset into a DataFrame using the read_csv method. You can either download the file on your local machine
            and pass the file's path to the read_csv method, or simply type in the URL where the CSV file is hosted on
            GitHub. In this case, we will work with the file:</p>
        <p>import pandas as pd</p>
        <p>df = pd.read_csv('../data/jj.csv')</p>
        <p><a id="calibre_link-236"></a><img src="images/000174.jpg" alt="Image 12" class="calibre2" />
        </p>
        <p><img src="images/000113.jpg" alt="Image 13" class="calibre2" /></p>
        <p><b>18</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>NOTE</p>
        <p>The entire code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH02">https://github</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH02">.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH02</a>.
        </p>
        <p>A DataFrame is the most-used data structure in pandas. It is a 2-dimensional labeled data
            structure with columns that can hold different types of data, such as strings, integers, floats, or dates.
        </p>
        <p>Our second step is to split the data into a train set for training and a test set for
            testing. Given that our horizon is 1 year, our train set will start in 1960 and go all the way to the end of
            1979. We will save the data collected in 1980 for our test set. You can think of a DataFrame as a table or a
            spreadsheet with column names and row indices. </p>
        <p>With our dataset in a DataFrame, we can display the first five entries by running df.head()
        </p>
        <p>This will give us the output shown in figure 2.3. </p>
        <p>Figure 2.3</p>
        <p>The first five entries of quarterly </p>
        <p>earnings per share for the Johnson &amp; Johnson </p>
        <p>dataset. Notice how our <b>DataFrame</b> has two </p>
        <p>columns: date and data. It also has row indices </p>
        <p>starting at 0. </p>
        <p>Figure 2.3 will help you better understand what type of data our DataFrame is holding. </p>
        <p>We have the date column, which specifies the end of each quarter, when the EPS is
            calculated. The data column holds the value of the EPS in US dollars (USD). </p>
        <p>We can optionally display the last five entries of our dataset and obtain the output in
            figure 2.4:</p>
        <p>df.tail()</p>
        <p>Figure 2.4</p>
        <p>The last five entries of our dataset. Here </p>
        <p>we can see the four quarters of 1980 that we will try </p>
        <p>to predict using different baseline models. We will </p>
        <p>compare our forecasts to the observed data in 1980 </p>
        <p>to evaluate the performance of each baseline. </p>
        <p>In figure 2.4 we see the four quarters of 1980, which is what we will be trying to forecast
            using our baseline models. We will evaluate the performance of our baselines by</p>
        <p><a id="calibre_link-39"></a> <i><b>2.2</b></i></p>
        <p> <i><b>Forecasting the historical mean</b></i></p>
        <p><b>19</b></p>
        <p>comparing our forecasts to the values in the data column for the four quarters of 1980. The
            closer our forecasts are to the observed values, the better. </p>
        <p>The final step before developing our baseline models is to split the dataset into the train
            and test sets. As mentioned earlier, the train set will consist of the data from 1960</p>
        <p>to the end of 1979, and the test set will consist of the four quarters of 1980. The train
            set will be the only information we use to develop our models. Once a model is built, we will forecast the
            next four timesteps, which will correspond to the four quarters of 1980 in our test set. That way, we can
            compare our forecasts to the observed data and evaluate the performance of our baselines. </p>
        <p>To make the split, we'll specify that our train set will contain all the data held in df
            except the last four entries. The test set will be composed of only the last four entries. </p>
        <p>This is what the next code block does:</p>
        <p>train = df[:-4]</p>
        <p>test = df[-4:]</p>
        <p> <i><b>2.2.2</b></i></p>
        <p> <i><b>Implementing the historical mean baseline</b></i>
        </p>
        <p>Now we are ready to implement our baseline. We will first use the arithmetic mean of the
            entire train set. To compute the mean, we'll use the numpy library, as it is a very fast package for
            scientific computing in Python that plays really well with DataFrames: import numpy as np</p>
        <p><b>Compute the arithmetic </b></p>
        <p>historical_mean = np.mean(train['data']) </p>
        <p><b>mean of the data column </b></p>
        <p><b>in the train set. </b></p>
        <p>print(historical_mean)</p>
        <p>In the preceding code block, we first import the numpy library and then compute the average
            of the EPS over the entire train set and print it out on the screen. This gives a value of 4.31 USD. This
            means that from 1960 to the end of 1979, the quarterly EPS of Johnson &amp; Johnson is on average 4.31 USD.
        </p>
        <p>Now we will naively forecast this value for each quarter of 1980. To do so, we'll simply
            create a new column, pred_mean, that holds the historical mean of the training set as a forecast:</p>
        <p><b>Set the historical </b></p>
        <p>test.loc[:, 'pred_mean'] = historical_mean </p>
        <p><b>mean as a forecast. </b></p>
        <p>Next, we need to define and calculate an error metric in order to evaluate the performance
            of our forecasts on the test set. In this case, we will use the <i>mean absolute percentage
                error</i> (MAPE). It is a measure of prediction accuracy for forecasting methods that is easy to
            interpret and independent of the scale of our data. This means that whether we are working with two-digit
            values or six-digit values, the MAPE will always be expressed as a percentage. Thus, the MAPE returns the
            percentage of how much the forecast values</p>
        <p><a id="calibre_link-262"></a><b>20</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>deviate from the observed or actual values on average, whether the prediction was higher or
            lower than the observed values. The MAPE is defined in equation 2.1. </p>
        <p>Equation 2.1</p>
        <p>MAPE</p>
        <p>In equation 2.1, <i>Ai</i> is the actual value at point <i>i</i> in time,
            and <i>Fi</i> is the forecast value at point <i>i</i> in time; <i class="calibre3">n</i> is simply the
            number of forecasts. In our
            case, because we are forecasting the four quarters of 1980, <i>n</i> = 4. Inside the
            summation, the forecast value is subtracted from the actual value, and that result is divided by the actual
            value, which gives us the percentage error. Then we take the absolute value of the percentage error. This
            operation is repeated for each of the <i>n</i> points in time, and the results are added
            together. Finally, we divide the sum by <i>n</i>, the number of points in time, which
            effectively gives us the mean absolute percentage error. </p>
        <p>Let's implement this function in Python. We'll define a mape function that takes in two
            vectors: y_true for the actual values observed in the test set and y_pred for the forecast values. In this
            case, because numpy allows us to work with arrays, we will not need a loop to sum all the values. We can
            simply subtract the y_pred array from the y_true array and divide by y_true to get the percentage error.
            Then we can take the absolute value. </p>
        <p>After that, we take the mean of the result, which will take care of summing up each value in
            the vector and dividing by the number of predictions. Finally, we'll multiply the result by 100 so the
            output is expressed as a percentage instead of a decimal number: def mape(y_true, y_pred):</p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p>Now we can calculate the MAPE of our baseline. Our actual values are in the data column of
            test, so it will be the first parameter passed to the mape function. Our forecasts are in the pred_mean
            column of test, so it will be our second parameter for the function:</p>
        <p>mape_hist_mean = mape(test['data'], test['pred_mean'])</p>
        <p>print(mape_hist_mean)</p>
        <p>Running the function gives a MAPE of 70.00%. This means that our baseline deviates by 70% on
            average from the observed quarterly EPS of Johnson &amp; Johnson in 1980. </p>
        <p>Let's visualize our forecasts to better understand our MAPE of 70%. </p>
        <p>Listing 2.1</p>
        <p>Visualizing our forecasts</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(train['date'], train['data'], 'g-.', label='Train')</p>
        <p>ax.plot(test['date'], test['data'], 'b-', label='Test')</p>
        <p><a id="calibre_link-206"></a><img src="images/000012.jpg" alt="Image 14" class="calibre2" />
        </p>
        <p> <i><b>2.2</b></i></p>
        <p> <i><b>Forecasting the historical mean</b></i></p>
        <p><b>21</b></p>
        <p>ax.plot(test['date'], test['pred_mean'], 'r--', label='Predicted')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Earnings per share (USD)')</p>
        <p>ax.axvspan(80, 83, color='#808080', alpha=0.2)</p>
        <p>ax.legend(loc=2)</p>
        <p>plt.xticks(np.arange(0, 85, 8), [1960, 1962, 1964, 1966, 1968, 1970, 1972, </p>
        <p>1974, 1976, 1978, 1980])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In listing 2.1, we use the matplotlib library, which is the most popular library for
            generating visualizations in Python, to generate a graph showing the training data, the forecast horizon,
            the observed values of the test set, and the predictions for each quarter of 1980. </p>
        <p>First, we initialize a figure and an ax object. A figure can contain many ax objects, which
            allows us to create a figure with two, three, or more plots. In this case, we are creating a figure with a
            single plot, so we only need one ax. </p>
        <p>Second, we plot our data on the ax object. We plot the train data using a green</p>
        <p>dashed and dotted line and give this curve a label of “Train.” The label will later be
            useful for generating a legend for the graph. We then plot the test data and use a blue continuous line with
            a label of “Test.” Finally, we plot our predictions using a red dashed line with a label of “Predicted.”
        </p>
        <p>Third, we label our <i>x-</i> axis and <i>y</i>-axis and
            draw a rectangular area to illustrate the forecast horizon. Since our forecast horizon is the four quarters
            of 1980, the area should start at index 80 and end at index 83, spanning the entire year of 1980. Remember
            that we obtained the indices of the last quarter of 1980 by running df.tail(), which resulted in figure 2.5.
        </p>
        <p>Figure 2.5</p>
        <p>The last five </p>
        <p>entries of our dataset</p>
        <p>We give this area a gray color and specify the opacity using the alpha parameter. </p>
        <p>When alpha is 1, the shape is completely opaque; when alpha is 0, it is completely
            transparent. In our case, we'll use an opacity of 20%, or 0.2. </p>
        <p>Then we specify the labels for the ticks on the <i>x</i>-axis. By default,
            the labels would show the data for each quarter of the dataset, which would create a crowded <i
                class="calibre3">x</i>-axis with unreadable labels. Instead, we'll display the year every 2 years. To do
            so, we'll gen-</p>
        <p><a id="calibre_link-247"></a><img src="images/000064.jpg" alt="Image 15" class="calibre2" />
        </p>
        <p>erate an array specifying the index at which the label must appear. That's what np.arange(0,
            81, 8) does: it generates an array starting at 0, finishing at 80, because the end index (81) is not
            included, with steps of 8, because there are 8 quarters in 2</p>
        <p>years. This will effectively generate the following array: [0,8,16,…72,80]. Then we specify
            an array containing the labels at each index, so it must start with 1960 and end with 1980, just like our
            dataset. </p>
        <p>Finally, we use fig.automft_xdate() to automatically format the tick labels on the <i
                class="calibre3">x</i>-axis. It will slightly rotate them and make sure that they are legible. The final
            touch-up is using plt.tight_layout() to remove any excess white space around the figure. </p>
        <p>The end result is figure 2.6. Clearly, this baseline did not yield accurate predictions,
            since the Predicted line is very far from the Test line. Now we know that our forecasts are, on average, 70%
            below the actual EPS for each quarter in 1980. Whereas the EPS in 1980 was consistently above $10, we
            predicted only $4.31 for each quarter. </p>
        <p>Figure 2.6</p>
        <p>Predicting the historical mean as a baseline. You can see that the prediction </p>
        <p>is far from the actual values in the test set. This baseline gives a MAPE of 70%. </p>
        <p>Still, what can we learn from it? Looking at our training set, we can see a positive trend,
            as the EPS is increasing over time. This is further supported by the trend component coming from the
            decomposition of our dataset, shown in figure 2.7. </p>
        <p>As you we can see, not only do we have a trend, but the trend is not constant between 1960
            and 1980&mdash;it is getting steeper. Therefore, it might be that the EPS observed in 1960 is not predictive
            of the EPS in 1980, because we have a positive trend, and
            EPS values are increasing with time and are doing so at a faster rate. </p>
        <p><a id="calibre_link-40"></a><img src="images/000063.jpg" alt="Image 16" class="calibre2" />
        </p>
        <p> <i><b>2.3</b></i></p>
        <p> <i><b>Forecasting last year's mean</b></i></p>
        <p><b>23</b></p>
        <p>Figure 2.7</p>
        <p>Trend component of our time series. You can see that we have a positive </p>
        <p>trend in our data, as it increases over time. </p>
        <p>Can you improve our baseline? </p>
        <p>Before moving on to the next section, can you think of a way to improve our baseline while
            still using the mean? Do you think that taking the mean of a shorter and more recent period of time would
            help (from 1970 to 1979, for example)? </p>
        <p> <i><b>2.3</b></i></p>
        <p> <i><b>Forecasting last year's mean</b></i></p>
        <p>The lesson learned from the previous baseline is that earlier values do not seem to be
            predictive of future values in the long term because of the positive trend component in our dataset. Earlier
            values seem to be too small to be representative of the new level the EPS reaches toward the end of 1979 and
            onwards into 1980. </p>
        <p>What if we use the mean of the last year in our training set to forecast the following year?
            This means that we would compute the average EPS in 1979 and forecast it for each quarter of 1980&mdash;the
            more recent values that have increased over time should potentially be closer to what will be observed in
            1980. For now, this is simply a hypothesis, so let's implement this baseline and test it to see how it
            performs. </p>
        <p>Our data is already split into test and train sets (done in section 2.2.1), so we can go
            ahead and calculate the mean of the last year in the train set, which corresponds to the last four data
            points in 1979:</p>
        <p>last_year_mean = np.mean(train.data[-4:]) </p>
        <p><b>Compute the average EPS for the four </b></p>
        <p><b>quarters of 1979, which are the last </b></p>
        <p>print(last_year_mean)</p>
        <p><b>four data points of the train set. </b></p>
        <p>This gives us an average EPS of $12.96. Therefore, we will predict that Johnson &amp;
            Johnson will have an EPS of $12.96 for the four quarters of 1980. Using the same procedure that we used for
            the previous baseline, we'll create a new pred_last_yr_mean column to hold the mean of last year as our
            predictions:</p>
        <p>test.loc[:, 'pred__last_yr_mean'] = last_year_mean</p>
        <p>Then, using the mape function that we defined earlier, we can evaluate the performance of
            our new baseline. Remember that the first parameter is the observed values, </p>
        <p><a id="calibre_link-340"></a><img src="images/000042.jpg" alt="Image 17" class="calibre2" />
        </p>
        <p><b>24</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>which are held in the test set. Then we pass in the predicted values, which are in the
            pred_last_yr_mean column:</p>
        <p>mape_last_year_mean = mape(test['data'], test['pred__last_yr_mean'])</p>
        <p>print(mape_last_year_mean)</p>
        <p>This gives us a MAPE of 15.60%. We can visualize our forecasts in figure 2.8. </p>
        <p>Can you recreate figure 2.8? </p>
        <p>As an exercise, try to recreate figure 2.8 to visualize the forecasts using the mean of the
            quarters of 1979. The code should be identical to listing 2.1, only this time the predictions are in a
            different column. </p>
        <p>Figure 2.8</p>
        <p>Predicting the mean of the last year in the training set (1979) as a baseline </p>
        <p>model. You can see that the prediction is closer to the actual values of the test set when
            compared to the previous baseline that we built in figure 2.6. </p>
        <p>This new baseline is a clear improvement over the previous one, even though its
            implementation is just as simple, as we decreased the MAPE from 70% to 15.6%. This means that our forecasts
            deviate from the observed values by 15.6% on average. Using the last year's mean is a good step in the right
            direction. We want to get a MAPE as close to 0% as possible, since that would translate into predictions
            that are closer to the actual values in our forecast horizon. </p>
        <p>We can learn from this baseline that future values likely depend on past values that are not
            too far back in history. This is a sign of <i>autocorrelation</i>, and we will dive deep
        </p>
        <p><a id="calibre_link-41"></a> <i><b>2.4</b></i></p>
        <p> <i><b>Predicting using the last known value</b></i></p>
        <p><b>25</b></p>
        <p>into this subject in chapter 5. For now, let's look at another baseline that we could
            develop for this situation. </p>
        <p> <i><b>2.4</b></i></p>
        <p> <i><b>Predicting using the last known value</b></i></p>
        <p>Previously we used the mean over different periods to develop a baseline model. So far, the
            best baseline has been the mean of the last recorded year in our training set, since it yielded the lowest
            MAPE. We learned from that baseline that future values depend on past values, but not those too far back in
            time. Indeed, predicting the mean EPS from 1960 to 1979 yielded worse forecasts than predicting the mean EPS
        </p>
        <p>over 1979. </p>
        <p>Therefore, we could suppose that using the last known value of the training set as a
            baseline model will give us even better forecasts, which would translate to a MAPE</p>
        <p>closer to 0%. Let's test that hypothesis. </p>
        <p>The first step is to extract the last known value of our train set, which corresponds to the
            EPS recorded for the last quarter of 1979:</p>
        <p>last = train.data.iloc[-1]</p>
        <p>print(last)</p>
        <p>When we retrieve the EPS recorded for the last quarter of 1979, we get a value of $9.99. We
            will thus predict that Johnson &amp; Johnson will have an EPS of $9.99 for the four quarters of 1980. </p>
        <p>Again, we'll append a new column called pred_last to hold the predictions. </p>
        <p>test.loc[:, 'pred_last'] = last</p>
        <p>Then, using the same MAPE function that we defined earlier, we can evaluate the performance
            of this new baseline model. Again, we pass to the function the actual values from the test set and our
            prediction from the pred_last column of test:</p>
        <p>mape_last = mape(test['data'], test['pred_last'])</p>
        <p>print(mape_last)</p>
        <p>This gives us a MAPE of 30.45%. We can visualize the forecasts in figure 2.9. </p>
        <p>Can you recreate figure 2.9? </p>
        <p>Try to produce figure 2.9 on your own! As data scientists, it is important for us to con-vey
            our results in a way that is accessible to people who do not work in our domain. </p>
        <p>Thus, producing plots showing our forecasts is an important skill to develop. </p>
        <p>It seems that our new hypothesis did not improve upon the last baseline that we built, since
            we have a MAPE of 30.45%, whereas we achieved a MAPE of 15.60% using the</p>
        <p><a id="calibre_link-42"></a><img src="images/000134.jpg" alt="Image 18" class="calibre2" />
        </p>
        <p><img src="images/000117.jpg" alt="Image 19" class="calibre2" /></p>
        <p><b>26</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>Figure 2.9</p>
        <p>Predicting the last known value of the train set as a baseline model. We </p>
        <p>can see that this baseline, with a MAPE of 30.45%, is better than our first baseline, but
            less performant than our second one. </p>
        <p>mean EPS over 1979. Therefore, these new forecasts are farther from the observed values in
            1980. </p>
        <p>This can be explained by the fact that the EPS displays a cyclical behavior, where it is
            high during the first three quarters and then falls at the last quarter. Using the last known value does not
            take the seasonality into account, so we need to use another naive forecasting technique to see if we can
            produce a better baseline. </p>
        <p> <i><b>2.5</b></i></p>
        <p> <i><b>Implementing the naive seasonal forecast</b></i>
        </p>
        <p>We considered the trend component for the first two baselines in this chapter, but we have
            not studied another important component from our dataset, which is the seasonal component shown in figure
            2.10. There are clear cyclical patterns in our data, and that is a piece of information that we could use to
            construct one last baseline: the naive seasonal forecast. </p>
        <p>Figure 2.10</p>
        <p>Seasonal component of our time series. We can see periodic fluctuations </p>
        <p>here, which indicate the presence of seasonality. </p>
        <p><a id="calibre_link-341"></a><img src="images/000172.jpg" alt="Image 20" class="calibre2" />
        </p>
        <p> <i><b>2.5</b></i></p>
        <p> <i><b>Implementing the naive seasonal forecast</b></i>
        </p>
        <p><b>27</b></p>
        <p>The naive seasonal forecast takes the last observed cycle and repeats it into the future.
        </p>
        <p>In our case, a full cycle occurs in four quarters, so we will take the EPS from the first
            quarter of 1979 and predict that value for the first quarter of 1980. Then we'll take the EPS from the
            second quarter of 1979 and predict that value for the second quarter of 1980. This process will be repeated
            for the third and fourth quarters. </p>
        <p>In Python, we can implement this baseline by simply taking the last four values of the train
            set, which correspond to the four quarters of 1979, and assigning them to the corresponding quarters in
            1980. The following code appends the pred_last_season column to hold our predictions from the naive seasonal
            forecast method:</p>
        <p>test.loc[:, 'pred_last_season'] = train['data'][-4:].values </p>
        <p><b>Our predictions are the last four values of our train</b></p>
        <p><b>set, which correspond to the quarters of 1979. </b></p>
        <p>Then we calculate the MAPE the same way we did in the previous sections:</p>
        <p>mape_naive_seasonal = mape(test['data'], test['pred_last_season'])</p>
        <p>print(mape_naive_seasonal)</p>
        <p>This gives us a MAPE of 11.56%, which is the lowest MAPE from all the baselines in this
            chapter. Figure 2.11 illustrates our forecast compared to the observed data in the test set. As an exercise,
            I strongly suggest that you try to recreate it on your own. </p>
        <p>Figure 2.11</p>
        <p>Result of the naive seasonal forecast on the test set. This forecast is more </p>
        <p>similar to the data observed in the test set, and it resulted in the lowest MAPE. Clearly,
            the seasonality of this dataset has an impact on future values, and it must be considered when forecasting.
        </p>
        <p><a id="calibre_link-43"></a><img src="images/000136.jpg" alt="Image 21" class="calibre2" />
        </p>
        <p><b>28</b></p>
        <p>CHAPTER 2</p>
        <p> <i><b>A naive prediction of the future</b></i></p>
        <p>As you can see, our naive seasonal forecast resulted in the lowest MAPE of all the baselines
            we built in this chapter. This means that seasonality has a significant impact on future values, since
            repeating the last season into the future yields fairly accurate forecasts. Intuitively, this makes sense,
            because we can clearly observe a cyclical pattern being repeated every year in figure 2.11. Seasonal effects
            will have to be considered when we develop a more complex forecasting model for this problem. I will explain
            in detail how to account for them in chapter 8. </p>
        <p> <i><b>2.6</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we developed four different baselines for our forecasting project. </p>
        <p>We used the arithmetic mean of the entire training set, the mean of the last year in the
            train set, the last known value of the train set, and a naive seasonal forecast. </p>
        <p>Each baseline was then evaluated on a test set using the MAPE metric. Figure 2.12</p>
        <p>summarizes the MAPE of each baseline we developed in this chapter. As you can see, the
            baseline using the naive seasonal forecast has the lowest MAPE, and therefore the best performance. </p>
        <p>Figure 2.12</p>
        <p>The MAPE of the four baselines developed in this chapter. The lower the </p>
        <p>MAPE, the better the baseline; therefore, we'll choose the naive seasonal baseline as our
            benchmark and compare it to our more complex models. </p>
        <p>Keep in mind that a baseline model serves as a basis for comparison. We will develop more
            complex models by applying statistical learning or deep learning techniques, and when we evaluate our more
            complex solutions against the test set and record our error metrics, we can compare them to those of the
            baseline. In our case, we'll compare the MAPE from a complex model against the MAPE of our naive seasonal
        </p>
        <p><a id="calibre_link-261"></a> <i><b>Summary</b></i></p>
        <p><b>29</b></p>
        <p>forecast. If the MAPE of a complex model is lower than 11.56%, then we'll know that we have
            a better-performing model. </p>
        <p>There will be special situations in which a time series can only be forecast using naive
            methods. These are special cases where the process moves at random and cannot be predicted using statistical
            learning methods. This means that we are in the presence of a random walk&mdash;we'll examine this in the
            next chapter. </p>
        <p> <i><b>Summary</b></i></p>
        <p> Time series forecasting starts with a baseline model that serves as a benchmark for
            comparison with more complex models. </p>
        <p> A baseline model is a trivial solution to our forecasting problem because it only uses
            heuristics, or simple statistics, such as the mean. </p>
        <p> MAPE stands for <i>mean absolute percentage error</i>, and it is an
            intuitive measure of how much a predicted value deviates from the actual value. </p>
        <p> There are many ways to develop a baseline. In this chapter, you saw how to use the mean,
            the last known value, or the last season. </p>

        <h2>CHAPTER 3 Going on a random walk</h2>
        <p> <i><b>This chapter covers</b></i></p>
        <ul>
            <li>Identifying a random walk process</li>
            <li>Understanding the ACF function</li>
            <li>Classifying differencing, stationarity, and white noise</li>
            <li>Using the ACF plot and differencing to identify a random walk</li>
            <li>Forecasting a random walk </li>
        </ul>

        <p>In the previous chapter, we compared different naive forecasting methods and learned that they often serve as
            benchmarks for more sophisticated models. However, there are instances where the simplest methods will yield
            the best forecasts. This is the case when we face a random walk process. </p>
        <p>In this chapter, you will learn what a random walk process is, how to recognize it, and how to make forecasts
            using random walk models. Along the way, we will look at the concepts of differencing, stationarity, and
            white noise, which will come back in later chapters as we develop more advanced statistical learning models.
        </p>
        <p>For this chapter's examples, suppose that you want to buy shares of Alphabet Inc. (GOOGL). Ideally, you would
            want to buy if the closing price of the stock is expected to go up in the future; otherwise, your investment
            will not be profitable. Hence, you decide to collect data on the daily closing price of GOOGL over 1 year
            and use time series forecasting to determine the future closing price of the stock. The closing price of
            GOOGL from April 27, 2020, to April 27, 2021, is shown in figure 3.1. At the time of writing, data beyond
            April 27, 2021, was not available yet. </p>
        <p>For this chapter's examples, suppose that you want to buy shares of Alphabet</p>
        <img src="images/000027.jpg" alt="Image 22" class="calibre2" />
        <p>Figure 3.1 Daily closing price of GOOGL from April 27, 2020, to April 27, 2021</p>
        <p>In figure 3.1 you can clearly see a long-term trend, since the closing price increased between April 27,
            2020, and April 27, 2021. However, there are also abrupt changes in the trend, with periods where it sharply
            decreases before suddenly increasing again. </p>
        <p>It turns out that the daily closing price of GOOGL can be modeled using the random walk model. To do so, we
            will first determine whether our process is <i>stationary</i> or not. If it is a non-stationary process, we
            will have to apply transformations, such as <i>differencing</i>, in order to make it stationary. Then we
            will be able to use the <i>autocorrelation</i> <i>function</i> plot to conclude that the daily closing price
            of GOOGL can be approximated by the random walk model. Both differencing and the autocorrelation plot will
            be covered in this chapter. Finally, we'll wrap up the chapter with forecasting methods that attempt to
            predict the future closing price of GOOGL. </p>
        <p>By the end of this chapter, you will have mastered the concepts of stationarity, differencing, and
            autocorrelation, which will return in later chapters as we further develop our forecasting skills. For now,
            let's focus on defining the random walk process.</p>
        <h3><i><b>3.1 The random walk process</b></i></h3>
        <p>A <i>random walk</i> is a process in which there is an equal chance of going up or down by a random number.
            This is usually observed in financial and economic data, like the daily closing price of GOOGL. Random walks
            often expose long periods where a positive or negative trend can be observed. They are also often
            accompanied by sudden changes in direction. </p>
        <p>In a random walk process, we say that the present value <i>y<sub>t</sub></i> is a function of the value at
            the previous
            timestep <i>y<sub>t-1</sub></i>, a constant C, and a random number <i>ϵ<sub>t</sub></i>, also termed
            <i>white
                noise</i>. Here, <i>ϵ<sub>t</sub></i> is the realization of the standard normal distribution, which has
            avariance of 1 and a mean of 0. </p>
        <p>Therefore, we can mathematically express a random walk with the following equation, where <i>yt</i> is the
            value at the present time <i>t</i>, C is a constant, <i>y<sub>t-1</sub></i> is the value at the previous
            timestep <i>t-1</i>, and ϵ<i><sub>t</sub></i> is a random number.
        </p>
        <p> <i> yt</i> = C + <i>yt</i>&ndash;1 + ϵ <i>t</i></p>
        <p>Equation 3.1</p>
        <p>Note that if the constant C is nonzero, we designate this process as a random walk with
            drift. </p>
        <p> <i><b>3.1.1</b></i></p>
        <p> <i><b>Simulating a random walk process</b></i></p>
        <p>To help you understand the random walk process, let's simulate one with Python&mdash;</p>
        <p>that way you can understand how a random walk behaves, and we can study its properties in a
            purely theoretical scenario. Then we'll transpose our knowledge onto our real-life example, where we'll
            model and forecast the closing price of GOOGL. </p>
        <p>From equation 3.1, we know that a random walk depends on its previous value <i>yt
            </i>&ndash;1</p>
        <p>plus white noise ϵt and some constant C. To simplify our simulation, let's assume that the
            constant C is 0. That way, our simulated random walk can be expressed as</p>
        <p> <i> yt</i> = <i>y<sub>t-1</sub></i> + ϵ <i>t</i></p>
        <p>Equation 3.2</p>
        <p>Now we must choose the first value of our simulated sequence. Again, for simplification, we
            will initialize our sequence at 0. This will be the value of <i>y</i> 0. </p>
        <p>We can now start building our sequence using equation 3.2. We'll start off with our initial
            value of 0 at time <i>t</i> = 0. Then, from equation 3.2, the value at <i class="calibre3">t</i> = 1,
            represented by <i>y</i> 1 will be equal to the previous
            value <i>y</i> 0 plus white noise. </p>
        <p> <i>y</i> 0 = 0</p>
        <p> <i>y</i> 1 = <i>y</i> 0 + ϵ1 = 0 + ϵ1 = ϵ1</p>
        <p>Equation 3.3</p>
        <p>The value at <i>t</i> = 2, denoted as <i>y</i> 2, will be
            equal to the value at the previous step, which is <i>y</i> 1, plus some white noise.
        </p>
        <p> <i>y</i> 1 = ϵ1</p>
        <p> <i>y</i> 2 = <i>y</i> 1 + ϵ2 = ϵ1 + ϵ2</p>
        <p>Equation 3.4</p>
        <p><a id="calibre_link-235"></a> <i><b>3.1</b></i></p>
        <p> <i><b>The random walk process</b></i></p>
        <p><b>33</b></p>
        <p>Then the value at <i>t</i> = 3, denoted as <i>y</i> 2,
            will be equal to the value at the previous step, which is <i>y</i> 2, plus some white
            noise. </p>
        <p> <i>y</i> 2 = ϵ1 + ϵ2</p>
        <p> <i>y</i> 3 = <i>y</i> 2 + ϵ3 = ϵ1 + ϵ2 + ϵ3</p>
        <p>Equation 3.5</p>
        <p>Looking at equation 3.5, you should start seeing a pattern. By initializing our random walk
            process at 0 and setting the constant C to 0, we determine that the value at time <i>t</i>
            is simply the sum of white noise from <i>t</i> = 1 to time <i>t</i>.
            Thus,
            our simulated random walk will respect the equation 3.6, where <i>yt</i> is the value
            of
            the random walk process at time <i>t</i>, and ϵ <i>t</i> is a random
            number at time <i>t</i>. </p>
        <p></p>
        <p>Equation 3.6</p>
        <p>Equation 3.6 establishes that at any point in time <i>t</i>, the value of
            our simulated time series will be the cumulative sum of a series of random numbers. We can visualize how
            our
            simulated random walk takes shape in figure 3.2. </p>
        <p>Simulated random walk</p>
        <p> <i>T</i></p>
        <p>0</p>
        <p>0 + 1</p>
        <p>0 + </p>
        <p>+ </p>
        <p></p>
        <p></p>
        <p>1</p>
        <p>2</p>
        <p>0 +</p>
        <p>+</p>
        <p>1</p>
        <p>2 + 3</p>
        <p>∑ <i>t</i></p>
        <p> <i>t = </i> 1</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= 2</p>
        <p> <i>t </i>= 3</p>
        <p> <i>... </i></p>
        <p> <i>t </i>= <i>T</i></p>
        <p>Figure 3.2</p>
        <p>Visualizing the construction of our simulated random walk. As you can see, our </p>
        <p>initial value is 0. Then, since the constant was also set to 0, the value of our random walk
            at any point in time is simply the cumulative sum of random numbers, or white noise. </p>
        <p>We are now ready to simulate our random process using Python. In order for this exercise to
            be reproducible, we will need to set a <i>seed</i>, which is an integer that we pass to
            the
            random.seed method. That way, no matter how many times we run the code, </p>
        <p>the same random numbers will be generated. This ensures that you will obtain the same
            results and plot as outlined in this chapter. </p>
        <p>NOTE</p>
        <p>At any time, you can refer to the source code for this chapter here:<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03"> https://</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03.
            </a></p>
        <p>Then we must decide on the length of our simulated process. For this exercise, we will
            generate 1,000 samples. The numpy library allows us to generate numbers from a normal distribution by
            using
            the standard_normal method. This ensures that the num-</p>
        <p>bers come from a distribution with mean of 0, as per the definition of white noise; I've
            also given it a variance of 1 (a normal distribution). Then we can set the very first value of our
            series to
            0. Finally, the cumsum method will calculate the cumulative</p>
        <p><a id="calibre_link-342"></a><img src="images/000028.jpg" alt="Image 23" class="calibre2" />
        </p>
        <p><b>34</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>sum of white noise for each timestep in our series, and we will have simulated our random
            walk:</p>
        <p><b>Set the random seed. This is </b></p>
        <p>import numpy as np</p>
        <p><b>done by passing an integer, </b></p>
        <p><b>Generate 1,000 random </b></p>
        <p><b>in this case 42. </b></p>
        <p><b>numbers from a normal </b></p>
        <p>np.random.seed(42) </p>
        <p><b>distribution with a mean </b></p>
        <p><b>of 0 and variance of 1. </b></p>
        <p>steps = np.random.standard_normal(1000) </p>
        <p>steps[0]=0 </p>
        <p><b>Initialize the </b></p>
        <p>random_walk = np.cumsum(steps) </p>
        <p><b>first value of </b></p>
        <p><b>the series to 0. </b></p>
        <p><b>Calculate the cumulative sum of errors for</b></p>
        <p><b>each timestep in the simulated process. </b></p>
        <p>We can plot our simulated random walk and see what it looks like. Since our <i>x</i>-axis
            and <i>y</i>-axis do not have a real-life meaning, we will
            simply label them as “timesteps” and</p>
        <p>“value,” respectively. The following code block generates figure 3.3:</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(random_walk)</p>
        <p>ax.set_xlabel('Timesteps')</p>
        <p>ax.set_ylabel('Value')</p>
        <p>plt.tight_layout()</p>
        <p>You can see the defining characteristics of a random walk in figure 3.3. You'll notice a
            positive trend over the first 400 timesteps, followed by a negative trend, and a sharp Figure 3.3</p>
        <p>A simulated random walk. Notice how we have a positive trend during the </p>
        <p>first 400 timesteps, followed by a negative trend, and a sharp increase toward the end. </p>
        <p>These are good hints that we have a random walk process. </p>
        <p><a id="calibre_link-45"></a> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>35</b></p>
        <p>increase toward the end. Therefore, we have both sudden changes and long periods where a
            trend is observed. </p>
        <p>We know this is a random walk because we simulated it. However, when dealing</p>
        <p>with real-life data, we need to find a way to identify whether our time series is a random
            walk or not. Let's see how we can achieve this. </p>
        <p> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p>To determine if our time series can be approximated as a random walk or not, we must first
            define a random walk. In the context of time series, a <i>random walk</i> is defined as
            a
            series whose first difference is stationary and uncorrelated. </p>
        <p>Random walk</p>
        <p>A random walk is a series whose first difference is stationary and uncorrelated. </p>
        <p>This means that the process moves completely at random. </p>
        <p>I've just introduced a lot of new concepts in a single sentence, so let's break down the
            steps to identify a random walk into a process. The steps are outlined in figure 3.4. </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Figure 3.4</p>
        <p>Steps to follow to identify </p>
        <p>whether time series data can be </p>
        <p>approximated as a random walk or not. </p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>The first step is naturally to gather the </p>
        <p>autocorrelation? </p>
        <p>data. Then we test for stationarity. If it is </p>
        <p>not stationary, we apply transformations </p>
        <p>until stationarity is achieved. Then we can </p>
        <p>Yes</p>
        <p>plot the autocorrelation function (ACF). If </p>
        <p>there is no autocorrelation, we have a </p>
        <p>Not a random walk</p>
        <p>random walk. </p>
        <p><a id="calibre_link-46"></a><b>36</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>In the following subsections, we will cover the concepts of stationarity and autocorrelation
            in detail. </p>
        <p> <i><b>3.2.1</b></i></p>
        <p> <i><b>Stationarity</b></i></p>
        <p>A <i>stationary time series</i> is one whose statistical properties do not
            change over time. In other words, it has a constant mean, variance, and autocorrelation, and these
            properties are independent of time. </p>
        <p>Many forecasting models assume stationarity. The moving average model (chap-</p>
        <p>ter 4), autoregressive model (chapter 5), and autoregressive moving average model (chapter
            6) all assume stationarity. These models can only be used if we verify that the data is indeed
            stationary.
            Otherwise, the models will not be valid, and the forecasts will not be reliable. Intuitively, this makes
            sense, because if the data is non-stationary, its properties are going to change over time, which would
            mean
            that our model parameters must also change through time. This means that we cannot possibly derive a
            function of future values as a function of past values, since the coefficients change at each point in
            time,
            making forecasting unreliable. </p>
        <p>We can view stationarity as an assumption that can make our lives easier when forecasting.
            Of course, we will rarely see a stationary time series in its original state because we are often
            interested
            in forecasting processes with a trend or with seasonal cycles. This is when models like ARIMA (chapter
            7)
            and SARIMA (chapter 8) come</p>
        <p>into play. </p>
        <p>Stationarity</p>
        <p>A stationary process is one whose statistical properties do not change over time. </p>
        <p>A times series is said to be stationary if its mean, variance, and autocorrelation do not
            change over time. </p>
        <p>For now, since we are still in the early stages of time series forecasting, we'll focus on
            stationary time series, which means that we will need to find ways to <i>transform</i>
            our
            time series to make them stationary. A transformation is simply a mathematical manipulation of the data
            that
            stabilizes its mean and variance, thus making it stationary. </p>
        <p>The simplest transformation one can apply is differencing. This transformation helps
            stabilize the mean, which in turn removes or reduces the trend and seasonality effects. Differencing
            involves calculating the series of changes from one timestep to another. To accomplish that, we simply
            subtract the value of the previous timestep <i>yt</i>&ndash;1</p>
        <p>from the value in the present <i>yt</i> to obtain the differenced value <i class="calibre3">y't</i>. </p>
        <p> <i> </i></p>
        <p>Equation 3.7</p>
        <p><a id="calibre_link-243"></a> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>37</b></p>
        <p>Transformation in time series forecasting</p>
        <p>A transformation is a mathematical operation applied to a time series in order to make it
            stationary. </p>
        <p>Differencing is a transformation that calculates the change from one timestep to another.
            This transformation is useful for stabilizing the mean. </p>
        <p>Applying a log function to the series can stabilize its variance. </p>
        <p>Figure 3.5 illustrates the process of differencing. Notice that taking the difference makes
            us lose one data point, because at the initial point in time, we cannot take the difference with its
            previous step, since <i>t</i> = &ndash;1 does not exist. </p>
        <p>Original</p>
        <p> <i>y</i> 0</p>
        <p> <i>y</i> 1</p>
        <p> <i>y</i> 2</p>
        <p> <i>y</i> 3</p>
        <p> <i>t </i>=0</p>
        <p> <i>t </i>=1</p>
        <p> <i>t </i>=2</p>
        <p> <i>t </i>=3</p>
        <p>First-order</p>
        <p> <i>y</i> 1 &ndash; <i>y</i> 0</p>
        <p> <i>y</i> 2 &ndash; <i>y</i> 1</p>
        <p> <i>y</i> 3 &ndash; <i>y</i> 2</p>
        <p>differencing</p>
        <p>Result</p>
        <p> <i>&ndash;</i></p>
        <p> <i>y' </i> 1</p>
        <p> <i>y' </i> 2</p>
        <p> <i>y' </i> 3</p>
        <p> <i>t </i>=0</p>
        <p> <i>t </i>=1</p>
        <p> <i>t </i>=2</p>
        <p> <i>t </i>=3</p>
        <p>Figure 3.5</p>
        <p>Visualizing the differencing transformation. Here, a first-order differencing </p>
        <p>is applied. Notice how we lose one data point after this transformation because the initial
            point in time cannot be differenced with previous values since they do not exist. </p>
        <p>It is possible to difference a time series many times. Taking the difference once is
            applying a <i>first-order</i> differencing. Taking it a second time would be a <i
                class="calibre3">second-order</i> differencing. It is often not necessary to difference more than
            twice
            to obtain a stationary series. </p>
        <p>While differencing is used to obtain a constant mean through time, we must also</p>
        <p>make sure we have a constant variance in order for our process to be stationary. Logarithms
            are used to help stabilize the variance. </p>
        <p><a id="calibre_link-47"></a><b>38</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Keep in mind that when we model a time series that has been transformed, we must <i
                class="calibre3">untransform</i> it to return the results of the model to the original units of
            measure-ment. The formal term for undoing a transformation is <i>inverse transform</i>.
            Therefore, if you apply a log transformation to your data, make sure you raise your forecast values to
            the
            power of 10 in order to bring the values back to their original magnitude. That way, your predictions
            will
            make sense in their original context. </p>
        <p>Now that we know what type of transformations we need to apply on a time series</p>
        <p>to make it stationary, we need to find a way to test whether a series is stationary or not.
        </p>
        <p> <i><b>3.2.2</b></i></p>
        <p> <i><b>Testing for stationarity</b></i></p>
        <p>Once a transformation is applied to a time series, we need to test for stationarity to
            determine if we need to apply another transformation to make the time series stationary, or if we need
            to
            transform it at all. A common test is the augmented Dickey-Fuller (ADF) test. </p>
        <p>The ADF test verifies the following null hypothesis: there is a unit root present in a time
            series. The alternative hypothesis is that there is no unit root, and therefore the time series is
            stationary. The result of this test is the ADF statistic, which is a negative number. The more negative
            it
            is, the stronger the rejection of the null hypothesis. In its implementation in Python, the p-value is
            also
            returned. If its value is less than 0.05, we can also reject the null hypothesis and say the series is
            stationary. </p>
        <p>Augmented Dickey-Fuller (ADF) test</p>
        <p>The augmented Dickey-Fuller (ADF) test helps us determine if a time series is stationary by
            testing for the presence of a unit root. If a unit root is present, the time series is not stationary.
        </p>
        <p>The null hypothesis states that a unit root is present, meaning that our time series is not
            stationary. </p>
        <p>Let's consider a very simple time series where the present value <i>yt</i>
            only depends on its past value <i>y<sub>t-1</sub></i> subject to a coefficient α1, a
            constant
            C, and white noise ϵ <i>t</i>. We can write the following general expression:</p>
        <p> <i> yt</i> = C + α1 <i>y<sub>t-1</sub></i> + ϵ <i>t</i>
        </p>
        <p>Equation 3.8</p>
        <p>In equation 3.8, ϵ <i>t</i> represents some error that we cannot predict,
            and C is a constant. </p>
        <p>Here, α1 is the root of the time series. This time series will be stationary only if the
            root lies within the unit circle. Therefore, its value must be between &ndash;1 and 1. Otherwise the
            series
            is non-stationary. </p>
        <p>Let's verify this by simulating two different series. One will be stationary and the other
            will have a unit root, meaning that it will not be stationary. The stationary process follows equation
            3.9,
            and the non-stationary process follows equation 3.10. </p>
        <p><a id="calibre_link-343"></a><img src="images/000017.jpg" alt="Image 24" class="calibre2" />
        </p>
        <p> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>39</b></p>
        <p> <i>yt</i> = 0.5 <i>y<sub>t-1</sub></i> + ϵ <i>t</i></p>
        <p>Equation 3.9</p>
        <p> <i> yt</i> = <i>y<sub>t-1</sub></i> + ϵ <i>t</i></p>
        <p>Equation 3.10</p>
        <p>In equation 3.9, the root of the series is 0.5. Since it is between &ndash;1 and 1, this
            series is stationary. On the other hand, in equation 3.10, the root of the series is 1, meaning that it
            is a
            unit root. Therefore, we expect this series to be non-stationary. </p>
        <p>By looking at both series in figure 3.6, we can gain some intuition about how stationary and
            non-stationary series evolve through time. We can see that the non-stationary process has long periods
            of
            positive and negative trends. However, the stationary process does not seem to increase or decrease over
            the
            long term. This high-level qualitative analysis can help us intuitively determine if a series is
            stationary
            or not. </p>
        <p>Figure 3.6</p>
        <p>Simulated stationary and non-stationary time series over 400 timesteps. You </p>
        <p>can see that the stationary series does not increase or decrease over the long term. </p>
        <p>However, the non-stationary process has long periods of positive and negative trends. </p>
        <p>A stationary series has constant properties over time, meaning that the mean and variance
            are not a function of time, so let's plot the mean of each series over time. The mean of a stationary
            process should be flat over time, whereas the mean of a non-stationary process should vary. </p>
        <p>As you can see in figure 3.7, the mean of the stationary process becomes constant after the
            first few timesteps. This is the expected behavior of a stationary process. The fact that the mean does
            not
            change as a function of time means that it is independent of time, as per the definition of a stationary
            process. However, the mean of the non-stationary process is clearly a function of time, as we can see it
            decreasing and increasing</p>
        <p><a id="calibre_link-344"></a><img src="images/000102.jpg" alt="Image 25" class="calibre2" />
        </p>
        <p><b>40</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Figure 3.7</p>
        <p>Mean of stationary and non-stationary processes over time. You can see </p>
        <p>how the mean of the stationary process becomes constant after the first few timesteps. </p>
        <p>On the other hand, the mean of the non-stationary process is a clear function of time, as it
            is constantly changing. </p>
        <p>again over time. Thus, the presence of a unit root makes the mean of the series dependent on
            time, so the series is not stationary. </p>
        <p>Let's further prove to ourselves that a unit root is a sign of non-stationarity by plotting
            the variance of each series over time. Again, a stationary series will have a constant variance over
            time,
            meaning that it is time independent. On the other hand, the non-stationary process will have a variance
            that
            changes over time. </p>
        <p>In figure 3.8 we can see that after the first few timesteps the variance of the stationary
            process is constant over time, which follows equation 3.9. Again, this corresponds to the definition of
            a
            stationary process, since variance does not depend on time. On the other hand, the process with a unit
            root
            has a variance that depends on time, since it greatly varies over the 400 timesteps. Therefore, this
            series
            is not stationary. </p>
        <p>By now, you should be convinced that a series with a unit root is not a stationary series.
            In both figures 3.7 and 3.8, the mean and variance were dependent on time, as their values kept
            changing.
            Meanwhile, the series with a root of 0.5 displayed a constant mean and variance over time, demonstrating
            that this series is indeed stationary. </p>
        <p>All these steps were performed to justify the use of the augmented Dickey-Fuller (ADF) test.
            We know that the ADF test verifies the presence of a unit root in the series. </p>
        <p>The null hypothesis, stating that a unit root is present, means that the series is not
            stationary. If the test returns a p-value less than a certain significance level, typically 0.05</p>
        <p>or 0.01, then we can reject the null hypothesis, meaning that there are no unit roots, and
            so the series is stationary. </p>
        <p><a id="calibre_link-48"></a><img src="images/000170.jpg" alt="Image 26" class="calibre2" />
        </p>
        <p> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>41</b></p>
        <p>Figure 3.8</p>
        <p>Variance of the simulated stationary and non-stationary series over time. </p>
        <p>The variance of the stationary process is independent of time, as it is constant after the
            first few timesteps. For the non-stationary process, the variance changes over time, meaning that it is
            not
            independent. </p>
        <p>Once we have a stationary series, we must determine whether there is autocorrelation or not.
            Remember that a random walk is a series whose first difference is stationary and uncorrelated. The ADF
            test
            takes care of the stationarity portion, but we'll need to use the autocorrelation function to determine
            if
            the series is correlated or not. </p>
        <p> <i><b>3.2.3</b></i></p>
        <p> <i><b>The autocorrelation function</b></i></p>
        <p>Once a process is stationary, plotting the autocorrelation function (ACF) is a great way to
            understand what type of process you are analyzing. In this case, we will use it to determine if we are
            studying a random walk or not. </p>
        <p>We know that correlation measures the extent of a linear relationship between two variables.
            Autocorrelation therefore measures the linear relationship between lagged values of a time series. Thus,
            the
            ACF reveals how the correlation between any two values changes as the lag increases. Here, the lag is
            simply
            the number of timesteps separating two values. </p>
        <p>Autocorrelation function</p>
        <p>The autocorrelation function (ACF) measures the linear relationship between lagged values of
            a time series. </p>
        <p>In other words, it measures the correlation of the time series with itself. </p>
        <p><a id="calibre_link-49"></a><b>42</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>For example, we can calculate the autocorrelation coefficient between <i>yt</i> and <i
                class="calibre3">yt</i>&ndash;1. In this case, the lag is equal to 1, and
            the coefficient would be denoted as <i>r</i> 1. Similarly, we can calculate the
            autocorrelation between <i>yt</i> and <i>yt</i>&ndash;2. Then the lag
            would be 2, and the coefficient would be denoted as <i>r</i> 2. When we plot the ACF
            function, the coefficient is the dependent variable, while the lag is the independent variable. Note
            that
            the autocorrelation coefficient at lag 0 will always be equal to 1. This makes sense intuitively,
            because
            the linear relationship between a variable and itself at the same timestep should be perfect, and
            therefore
            equal to 1. </p>
        <p>In the presence of a trend, a plot of the ACF will show that the coefficients are high for
            short lags, and they will decrease linearly as the lag increases. If the data is seasonal, the ACF plot
            will
            also display cyclical patterns. Therefore, plotting the ACF</p>
        <p>function of a non-stationary process will not give us more information than is available by
            looking at the evolution of our process through time. However, plotting the ACF</p>
        <p>for a stationary process can help us identify the presence of a random walk. </p>
        <p> <i><b>3.2.4</b></i></p>
        <p> <i><b>Putting it all together</b></i></p>
        <p>Now that you understand what stationarity is, how to transform a time series to make it
            stationary, what statistical test can be used to assess stationarity, and how plotting the ACF function
            can
            help you identify the presence of a random walk, we can put all these concepts together and apply them
            in
            Python. In this section, we will work with our simulated data (from section 3.1.1) and cover the
            necessary
            steps to identify a random walk. </p>
        <p>The first step is to determine whether our random walk is stationary or not. We</p>
        <p>know that since there are visible trends in our sequence, it is not stationary.
            Nevertheless, let's apply the ADF test to make sure. We will use the statsmodels library, which is a
            Python
            library that implements many statistical models and tests. To run the ADF</p>
        <p>test, we simply pass it our array of simulated data. The result is a list of different
            values, but we are mainly interested in the first two: the ADF statistic and the p-value. </p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p><b>Pass the simulated random </b></p>
        <p><b>walk to the adfuller function. </b></p>
        <p>ADF_result = adfuller(random_walk) </p>
        <p><b>Retrieve the ADF statistic, which is </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}') </p>
        <p><b>the first value in the list of results. </b></p>
        <p>print(f'p-value: {ADF_result[1]}') </p>
        <p><b>Retrieve the p-value, which is the </b></p>
        <p><b>second value in the list of results. </b></p>
        <p>This prints an ADF statistic of &ndash;0.97 and a p-value of 0.77. The ADF statistic is not
            a large negative number, and with a p-value greater than 0.05, we cannot reject the null hypothesis
            stating
            that our time series is not stationary. We can further support our conclusion by plotting the ACF
            function.
        </p>
        <p>The statsmodels library conveniently has a function to quickly plot the ACF. </p>
        <p>Again, we can simply pass it our array of data. We can optionally specify the number of</p>
        <p><a id="calibre_link-233"></a><img src="images/000014.jpg" alt="Image 27" class="calibre2" />
        </p>
        <p> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>43</b></p>
        <p>lags, which will determine the range on the <i>x</i>-axis. In this case, we
            will plot the first 20</p>
        <p>lags, but feel free to plot as many lags as you wish. </p>
        <p>from statsmodels.graphics.tsaplots import plot_acf</p>
        <p>plot_acf(random_walk, lags=20); </p>
        <p>The output is shown in figure 3.9. </p>
        <p>Figure 3.9</p>
        <p>Plot of the ACF of our simulated random walk. Notice how the </p>
        <p>autocorrelation coefficients slowly decrease. Even at lag 20, the value is still
            autocorrelated, which means that our random walk is not stationary at the moment. </p>
        <p>In figure 3.9 you'll notice how the autocorrelation coefficients slowly decrease as the lag
            increases, which is a clear indicator that our random walk is not a stationary process. Note that the
            shaded
            area represents a confidence interval. If a point is within the shaded area, then it is not
            significantly
            different from 0. Otherwise, the autocorrelation coefficient is significant. </p>
        <p>Because our random walk is not stationary, we need to apply a transformation to</p>
        <p>make it stationary in order to retrieve useful information from the ACF plot. Since our
            sequence mostly displays changes in the trend without seasonal patterns, we will apply a first-order
            differencing. Remember that we'll lose the first data point every time we difference. </p>
        <p>To difference, we will use the numpy method diff. This will difference a given array of
            data. The n parameter controls how many times the array must be differenced. To apply a first-order
            differencing, the n parameter must be set to 1:</p>
        <p>diff_random_walk = np.diff(random_walk, n=1)</p>
        <p><a id="calibre_link-345"></a><img src="images/000088.jpg" alt="Image 28" class="calibre2" />
        </p>
        <p><b>44</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>We can visualize the differenced simulated random walk in figure 3.10. </p>
        <p>Figure 3.10</p>
        <p>Evolution of our differenced random walk. It seems that we successfully </p>
        <p>removed the trend and that the variance is stable. </p>
        <p>As you can see in figure 3.10, we have removed the trend from our series. Furthermore, the
            variance looks quite stable. Let's test for stationarity again, using the ADF test: ADF_result =
            adfuller(diff_random_walk) </p>
        <p><b>Here we pass in </b></p>
        <p><b>our differenced </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p><b>random walk. </b></p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p>This prints out an ADF statistic of &ndash;31.79 with a p-value of 0. This time the ADF
            statistic is a large negative number, and the p-value is less than 0.05. Therefore, we reject the null
            hypothesis, and we can say that this process has no unit root and is thus stationary. </p>
        <p>We can now plot the ACF function of our newly stationary series: </p>
        <p>plot_acf(diff_random_walk, lags=20); </p>
        <p>Looking at figure 3.11, you'll notice that there are no significant autocorrelation
            coefficients after lag 0. This means that the stationary process is completely random and can therefore
            be
            described as <i>white noise</i>. Each value is simply a random step away from the
            previous
            one, with no relation between them. </p>
        <p>We have demonstrated that our simulated data is indeed a random walk: the series is
            stationary and uncorrelated after a first-order differencing, which corresponds to the definition of a
            random walk. </p>
        <p><a id="calibre_link-50"></a><img src="images/000057.jpg" alt="Image 29" class="calibre2" />
        </p>
        <p> <i><b>3.2</b></i></p>
        <p> <i><b>Identifying a random walk</b></i></p>
        <p><b>45</b></p>
        <p>Figure 3.11</p>
        <p>An ACF plot of our differenced random walk. Notice how there are no </p>
        <p>significant coefficients after lag 0. This is a clear indicator that we are dealing with a
            random walk. </p>
        <p> <i><b>3.2.5</b></i></p>
        <p> <i><b>Is GOOGL a random walk? </b></i></p>
        <p>We've applied the necessary steps to identify a random walk on our simulated data, so this
            is a great time to test our knowledge and new skills on a real-life dataset. Taking the closing price of
            GOOGL from April 27, 2020, to April 27, 2021, from <a href="finance.yahoo.com">finance.yahoo.com, </a>
        </p>
        <p>let's determine whether the process can be approximated as a random walk or not. </p>
        <p>You can load the data in a DataFrame using the read_csv method from pandas:</p>
        <p>df = pd.read_csv('data/GOOGL.csv')</p>
        <p>Hopefully, your conclusion is that the closing price of GOOGL is indeed a random walk
            process. Let's see how we arrive at this conclusion. For visualization purposes, let's quickly plot our
            data, which results in figure 3.12:</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['Date'], df['Close'])</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Closing price (USD)')</p>
        <p>plt.xticks(</p>
        <p>[4, 24, 46, 68, 89, 110, 132, 152, 174, 193, 212, 235], </p>
        <p>['May', 'June', 'July', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 2021, 'Feb', </p>
        <p>➥ 'Mar', 'April'] </p>
        <p><b>Nicely label the </b></p>
        <p>fig.autofmt_xdate()</p>
        <p><b>ticks on the x-axis. </b></p>
        <p>plt.tight_layout()</p>
        <p><a id="calibre_link-346"></a><img src="images/000077.jpg" alt="Image 30" class="calibre2" />
        </p>
        <p><b>46</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Figure 3.12</p>
        <p>Closing price of GOOGL from April 27, 2020, to April 27, 2021</p>
        <p>Looking at figure 3.12, we can see a trend in the data as the closing price is increasing
            over time; therefore, we do not have a stationary process. This is further supported by the ADF test:
        </p>
        <p>GOOGL_ADF_result = adfuller(df['Close'])</p>
        <p>print(f'ADF Statistic: {GOOGL_ADF_result[0]}')</p>
        <p>print(f'p-value: {GOOGL_ADF_result[1]}')</p>
        <p>This returns an ADF statistic of 0.16 and a p-value greater than 0.05, so we know that our
            data is not stationary. Hence, we will difference our data to see if that makes it stationary:</p>
        <p>diff_close = np.diff(df['Close'], n=1)</p>
        <p>Next, we can run the ADF test on the differenced data:</p>
        <p>GOOGL_diff_ADF_result = adfuller(diff_close)</p>
        <p>print(f'ADF Statistic: {GOOGL_diff_ADF_result[0]}')</p>
        <p>print(f'p-value: {GOOGL_diff_ADF_result[1]}')</p>
        <p>This gives an ADF statistic of &ndash;5.3 and a p-value smaller than 0.05, meaning that we
            have a stationary process. </p>
        <p>Now we can plot the ACF function and see if there is autocorrelation:</p>
        <p>plot_acf(diff_close, lags=20); </p>
        <p><a id="calibre_link-51"></a><img src="images/000131.jpg" alt="Image 31" class="calibre2" />
        </p>
        <p> <i><b>3.3</b></i></p>
        <p> <i><b>Forecasting a random walk</b></i></p>
        <p><b>47</b></p>
        <p>Figure 3.13 might make you scratch your head and wonder if there is autocorrelation or not.
            We do not see any significant coefficients, except at lags 5 and 18. This situation can arise sometimes,
            and
            it is due to chance only. In such a situation, we can safely assume that the coefficients at lags 5 and
            18
            are not significant, because we do not have consecutive significant coefficients. It just happened by
            chance
            that the differenced values are slightly correlated with the ones at lags 5 and 18. </p>
        <p>Figure 3.13</p>
        <p>We can see that there are no significant coefficients in the ACF plot. </p>
        <p>You might notice that at lags 5 and 18 the coefficients are significant, while the others
            are not. This happens by chance with some data, and these points can be </p>
        <p>assumed to be non-significant, because we do not have consecutive significant </p>
        <p>coefficients between lags 0 and 5 or lags 0 and 18. </p>
        <p>Therefore, we can conclude that the closing price of GOOGL can be approximated by a random
            walk process. Taking the first difference makes the series stationary, and its ACF plot shows no
            autocorrelation, meaning that it is purely random. </p>
        <p> <i><b>3.3</b></i></p>
        <p> <i><b>Forecasting a random walk</b></i></p>
        <p>Now that we know what a random walk is and how to identify one, we can start forecasting.
            This might sound surprising, since we established that a random walk takes random steps as time
            progresses.
        </p>
        <p>Predicting a random change is impossible, unless we predict a random value our-</p>
        <p>selves, which is not ideal. In this case, we can only use naive forecasting methods, or
            baselines, which we covered in chapter 2. Since the values change randomly, no statistical learning
            model
            can be applied. Instead, we can only reasonably predict the historical mean, or the last value. </p>
        <p><a id="calibre_link-52"></a><img src="images/000133.jpg" alt="Image 32" class="calibre2" />
        </p>
        <p><b>48</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Depending on the use case, your forecasting horizon will vary. Ideally, when dealing with a
            random walk, you will only forecast the next timestep. However, you may be required to forecast many
            timesteps into the future. Let's look at how to tackle each of these situations. </p>
        <p> <i><b>3.3.1</b></i></p>
        <p> <i><b>Forecasting on a long horizon</b></i></p>
        <p>In this section, we'll forecast a random walk on a long horizon. This is not an ideal
            case&mdash;a random walk can unexpectedly increase or decrease because past observations are not
            predictive
            of changes in the future. Here we'll continue working with our simulated random walk from section 3.1.1.
        </p>
        <p>To make things easier, we will assign the random walk to a DataFrame and split the dataset
            into train and test sets. The train set will contain the first 800 timesteps, which corresponds to 80%
            of
            the simulated data. The test set will thus contain the last 200 values: <b>Assign the
                simulated random walk </b></p>
        <p>import pandas as pd</p>
        <p><b>to a DataFrame. It will contain a </b></p>
        <p><b>single column called value. </b></p>
        <p>df = pd.DataFrame({'value': random_walk}) </p>
        <p>train = df[:800] </p>
        <p><b>The first 80% of the data is assigned to the train set. </b></p>
        <p>test = df[800:] </p>
        <p><b>Since we have 1,000 timesteps, 80% of our simulated </b></p>
        <p><b>data corresponds to the values up to index 800. </b></p>
        <p><b>Assign the last 20% of the</b></p>
        <p><b>simulated random walk</b></p>
        <p><b>to the test set. </b></p>
        <p>Figure 3.14 illustrates our split. Using the train set, we must now predict the next 200</p>
        <p>timesteps in the test set. </p>
        <p>Figure 3.14</p>
        <p>The train/test split of our generated random walk. The first 800 timesteps </p>
        <p>are part of the train set, and the remaining values are part of the test set. Our goal is to
            forecast the values in the shaded area. </p>
        <p><a id="calibre_link-347"></a> <i><b>3.3</b></i></p>
        <p> <i><b>Forecasting a random walk</b></i></p>
        <p><b>49</b></p>
        <p>As mentioned, we can only use naive forecasting methods for this situation, since we are
            dealing with a random walk. In this case, we will use the historical mean, the last known value, and the
            drift method. </p>
        <p>Forecasting the mean is fairly straightforward. We'll simply calculate the mean of the train
            set and say that the next 200 timesteps will be equal to that value. Here, we'll create a new column
            pred_mean that will hold the historical mean as a prediction: <b>Calculate the mean
            </b>
        </p>
        <p>mean = np.mean(train.value) </p>
        <p><b>of the train set. </b></p>
        <p>test.loc[:, 'pred_mean'] = mean </p>
        <p><b>Predict the historical </b></p>
        <p><b>mean for the next 200 </b></p>
        <p><b>Show the first five </b></p>
        <p>test.head() </p>
        <p><b>timesteps. </b></p>
        <p><b>rows of test. </b></p>
        <p>You will get a historical mean of &ndash;3.68. This means that we'll forecast that the next
            200</p>
        <p>timesteps of our simulated random walk will have a value of &ndash;3.68. </p>
        <p>Another possible baseline is to predict the last known value of the train set. Here, we'll
            simply extract the last value of the train set and assign its value as our prediction for the next 200
            timesteps:</p>
        <p><b>Retrieve the last </b></p>
        <p>last_value = train.iloc[-1].value </p>
        <p><b>value of the train set. </b></p>
        <p>test.loc[:, 'pred_last'] = last_value </p>
        <p><b>Assign the last value as a prediction </b></p>
        <p><b>for the next 200 timesteps under </b></p>
        <p>test.head()</p>
        <p><b>the pred_last column. </b></p>
        <p>This method yields forecasts with a constant value of &ndash;6.81. </p>
        <p>Finally, we'll apply the drift method, which we have not covered yet. The <i>drift</i>
            method is a modification of predicting the last known value. In this case, we
            allow the values to increase or decrease over time. The rate at which values will change in the future
            is
            equal to that seen in the train set. Therefore, it is equivalent to calculating the slope between the
            first
            and last value of the train set and simply extrapolating this straight line into the future. </p>
        <p>Remember that we can calculate the slope of a straight line by dividing the change in the <i
                class="calibre3">y</i>-axis by the change in the <i>x</i>-axis. In our case, the
            change
            in the <i>y</i>-axis is the difference between the last value of our random walk <i class="calibre3">yf</i>
            and its initial value <i>yi</i>. Then, the change in the <i class="calibre3">x</i>-axis is equivalent to the
            number of timesteps minus 1, as shown in equation
            3.11.
        </p>
        <p>Equation 3.11</p>
        <p>We calculated the last value of the train set when we implemented the last known value
            baseline, and we know that the initial value of our simulated random walk is 0; therefore, we can plug
            the
            numbers into equation 3.11 and calculate the drift in equation 3.12. </p>
        <p>Equation 3.12</p>
        <p><a id="calibre_link-348"></a><b>50</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Let's implement this in Python now. We will calculate the change in the <i>x</i>-axis
            and
            the <i>y</i>-axis, and simply divide them to obtain the
            drift:</p>
        <p><b>Calculate the change in the x-axis, which is the difference </b></p>
        <p><b>between the last index (799) and first index (0). It is </b></p>
        <p>deltaX = 800 &ndash; 1 </p>
        <p><b>equivalent to the number of timesteps minus 1. </b></p>
        <p>deltaY = last_value &ndash; 0 </p>
        <p><b>Calculate the difference between the last and </b></p>
        <p><b>initial values of the simulated random walk in </b></p>
        <p>drift = deltaY / deltaX </p>
        <p><b>the train set. Recall that the last value of the </b></p>
        <p><b>train set is in the last_value variable from the </b></p>
        <p>print(drift)</p>
        <p><b>Calculate the</b></p>
        <p><b>previous baseline we implemented. </b></p>
        <p><b>drift according to</b></p>
        <p><b>equation 3.11. </b></p>
        <p>As expected, this gives us a drift of &ndash;0.0085, which means that the values of our
            forecasts will slowly decrease over time. The drift method simply states that the value of our forecast
            is
            linearly dependent on the timestep, the value of the drift, and the initial value of our random walk, as
            expressed in equation 3.13. Keep in mind that our random walk starts at 0, so we can remove that from
            equation 3.13. </p>
        <p>forecast = drift × timestep + <i>yi</i></p>
        <p>forecast = drift × timestep</p>
        <p>Equation 3.13</p>
        <p>Since we want to forecast the next 200 timesteps following the train set, we'll first create
            an array containing the range of timesteps starting at 800 and ending at 1000 with a step of 1. Then we
            simply multiply each timestep by the drift to get our forecast values. Finally, we assign them to the
            pred_drift column of test:</p>
        <p><b>Create a list containing the range of timesteps </b></p>
        <p>x_vals = np.arange(800, 1001, 1) </p>
        <p><b>starting at 800 and ending at 1000 with a step of 1. </b></p>
        <p>pred_drift = drift * x_vals </p>
        <p><b>Multiply each timestep by the </b></p>
        <p><b>drift to get the forecast value </b></p>
        <p>test.loc[:, 'pred_drift'] = pred_drift </p>
        <p><b>at each timestep. </b></p>
        <p>test.head()</p>
        <p><b>Assign our forecast values to</b></p>
        <p><b>the pred_drift column. </b></p>
        <p>With all three methods, we can now visualize what our forecasts look like against the actual
            values of the test set:</p>
        <p><b>Plot the values in the train set. </b></p>
        <p><b>Plot the </b></p>
        <p><b>Plot the forecast from the </b></p>
        <p><b>observed </b></p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>historical mean. It will be a </b></p>
        <p><b>values in </b></p>
        <p><b>red dotted and dashed line. </b></p>
        <p><b>the test set. </b></p>
        <p>ax.plot(train.value, 'b-') </p>
        <p>ax.plot(test['value'], 'b-') </p>
        <p><b>Plot the forecast </b></p>
        <p>ax.plot(test['pred_mean'], 'r-.', label='Mean') </p>
        <p><b>from the last value </b></p>
        <p>ax.plot(test['pred_last'], 'g--', label='Last value') </p>
        <p><b>of train set. It will </b></p>
        <p>ax.plot(test['pred_drift'], 'k:', label='Drift') </p>
        <p><b>be a green dashed </b></p>
        <p><b>line. </b></p>
        <p><b>Plot the forecast using the drift method. It will be a black dotted
                line. </b></p>
        <p><a id="calibre_link-265"></a><img src="images/000048.jpg" alt="Image 33" class="calibre2" />
        </p>
        <p> <i><b>3.3</b></i></p>
        <p> <i><b>Forecasting a random walk</b></i></p>
        <p><b>51</b></p>
        <p>ax.axvspan(800, 1000, color='#808080', alpha=0.2) </p>
        <p><b>Shade the </b></p>
        <p>ax.legend(loc=2) </p>
        <p><b>forecast </b></p>
        <p><b>Place the legend </b></p>
        <p><b>horizon. </b></p>
        <p>ax.set_xlabel('Timesteps')</p>
        <p><b>in the upper-left </b></p>
        <p>ax.set_ylabel('Value')</p>
        <p><b>corner. </b></p>
        <p>plt.tight_layout()</p>
        <p>As you can see in figure 3.15, our forecasts are faulty. They all fail to predict the sudden
            increase observed in the test set, which makes sense, because the future change in a random walk is
            completely random, and therefore unpredictable. </p>
        <p>Figure 3.15</p>
        <p>Forecasting our random walk using the mean, last known value, and </p>
        <p>drift methods. As you can see, all predictions are fairly poor and fail to predict the
            sudden increase observed in the test set. </p>
        <p>We can further demonstrate that by calculating the mean squared error (MSE) of our
            forecasts. We cannot use the MAPE, as in chapter 2, because our random walk can take the value
            0&mdash;it is
            impossible to calculate the percentage difference from an observed value of 0 because that implies a
            division by 0, which is not allowed in mathematics. </p>
        <p>Therefore, we opt for the MSE, as it can measure the quality of the fit of a model, even if
            the observed value is 0. The sklearn library has a mean_squared_error function that simply needs the
            observed and predicted values. It will then return the MSE. </p>
        <p>from sklearn.metrics import mean_squared_error</p>
        <p>mse_mean = mean_squared_error(test['value'], test['pred_mean'])</p>
        <p>mse_last = mean_squared_error(test['value'], test['pred_last'])</p>
        <p>mse_drift = mean_squared_error(test['value'], test['pred_drift'])</p>
        <p>print(mse_mean, mse_last, mse_drift)</p>
        <p><a id="calibre_link-53"></a><img src="images/000047.jpg" alt="Image 34" class="calibre2" />
        </p>
        <p><b>52</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>You will obtain an MSE of 327, 425, and 466 for the historical mean, last value, and drift
            methods, respectively. We can compare the MSEs for these three baselines in figure 3.16. </p>
        <p>Figure 3.16</p>
        <p>MSEs of our forecasts. Clearly, the future of a random walk is </p>
        <p>unpredictable, with MSEs exceeding 300. </p>
        <p>As you can see in figure 3.16, the best forecast was obtained by predicting the historical
            mean, and yet the MSE exceeds 300. This is an extremely high value considering that our simulated random
            walk does not exceed the value of 30. </p>
        <p>By now, you should be convinced that forecasting a random walk on a long hori-</p>
        <p>zon does not make sense. Since the future value is dependent on the past value plus a random
            number, the randomness portion is magnified in a long horizon where many</p>
        <p>random numbers are added over the course of many timesteps. </p>
        <p> <i><b>3.3.2</b></i></p>
        <p> <i><b>Forecasting the next timestep</b></i></p>
        <p>Forecasting the next timestep of a random walk is the only reasonable situation we can
            tackle, although we will still use naive forecasting methods. Specifically, we will predict the last
            known
            value. However, we will make this forecast only for the next timestep. That way, our forecast should
            only be
            off by a random number, since the future value of a random walk is always the past value plus white
            noise.
        </p>
        <p>Implementing this method is straightforward: we take our initial observed value and use it
            to predict the next timestep. Once we record a new value, it will be used as a forecast for the
            following
            timestep. This process is then repeated into the future. </p>
        <p><a id="calibre_link-278"></a> <i><b>3.3</b></i></p>
        <p> <i><b>Forecasting a random walk</b></i></p>
        <p><b>53</b></p>
        <p>Figure 3.17 illustrates this process. Here, the observed value at 8:00 a.m. is used to
            forecast the value for 9:00 a.m., the actual value observed at 9:00 a.m. is used to forecast the value
            at
            10:00 a.m., and so on. </p>
        <p>Observed</p>
        <p>10</p>
        <p>13</p>
        <p>8</p>
        <p>9</p>
        <p>8:00 a.m. </p>
        <p>9:00 a.m. </p>
        <p>10:00 a.m. </p>
        <p>11:00 a.m. </p>
        <p>Forecasts</p>
        <p>&ndash;</p>
        <p>10</p>
        <p>13</p>
        <p>8</p>
        <p>8:00 a.m. </p>
        <p>9:00 a.m. </p>
        <p>10:00 a.m. </p>
        <p>11:00 a.m. </p>
        <p>Figure 3.17</p>
        <p>Forecasting the following timestep of a random walk. Here, the </p>
        <p>observed value at a point in time will be used as a forecast for the next point in time.
        </p>
        <p>Let's apply this method to our random walk process. For the sake of illustrating this
            method, we will apply it over the entire random walk. This naive forecast can look deceptively amazing,
            when
            we are actually only predicting the last known value at each timestep. </p>
        <p>A good way to simulate this process is by shifting our data, and the pandas library has a
            shift method that does exactly what we want. We simply pass in the number of periods, which in our case
            is
            1, since we are forecasting the next timestep:</p>
        <p>df_shift = df.shift(periods=1) </p>
        <p><b>df_shift is now our forecast over the </b></p>
        <p><b>entire random walk, and it corresponds </b></p>
        <p>df_shift.head()</p>
        <p><b>to the last known value at each timestep. </b></p>
        <p>You will notice that at step 1, the value is 0, which corresponds to the observed value at
            step 0 in the simulated random walk. Therefore, we are effectively using the present observed value as a
            forecast for the next timestep. Plotting our forecast yields figure 3.18. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df, 'b-', label='actual')</p>
        <p>ax.plot(df_shift, 'r-.', label='forecast')</p>
        <p>ax.legend(loc=2)</p>
        <p>ax.set_xlabel('Timesteps')</p>
        <p>ax.set_ylabel('Value')</p>
        <p>plt.tight_layout()</p>
        <p><a id="calibre_link-298"></a><img src="images/000149.jpg" alt="Image 35" class="calibre2" />
        </p>
        <p><b>54</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Figure 3.18</p>
        <p>A naive forecast of the next timestep of a random walk. This plot gives </p>
        <p>the illusion of a very good model, when we are in fact only predicting the value observed at
            the previous timestep. </p>
        <p>Looking at figure 3.18, you might think that we have developed an amazing model that is
            almost a perfect fit to our data. It seems that we do not have two separate lines in the graph, since
            both
            of them almost perfectly overlap, which is a sign of a perfect fit. Now, we can calculate the MSE:</p>
        <p>mse_one_step = mean_squared_error(test['value'], df_shift[800:]) </p>
        <p>print(mse_one_step)</p>
        <p><b>Calculate the MSE on the test set. </b></p>
        <p>This yields a value of 0.93, which again might lead us to think that we have a very
            performant model, since the MSE is very close to 0. However, we know that we are simply forecasting the
            value observed at the previous timestep. This becomes more apparent if we zoom in on our graph, as shown
            in
            figure 3.19. </p>
        <p>Therefore, if a random walk process must be forecast, it is better to make many short-term
            forecasts. That way, we do not allow for many random numbers to accumulate over time, which will degrade
            the
            quality of our forecasts in the long term. </p>
        <p>Because a random process takes random steps into the future, we cannot use statistical or
            deep learning techniques to fit such a process: there is nothing to learn from randomness and it cannot
            be
            predicted. Instead, we must rely on naive forecasting methods. </p>
        <p><a id="calibre_link-54"></a><img src="images/000114.jpg" alt="Image 36" class="calibre2" />
        </p>
        <p> <i><b>3.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>55</b></p>
        <p>Figure 3.19</p>
        <p>Close-up on the last 100 timesteps of our random walk. Here we can </p>
        <p>see how our forecasts are a simple shift of the original time series. </p>
        <p> <i><b>3.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>So far you've learned how to develop baseline models, and you've discovered that in the
            presence of a random walk you can only reasonably apply baseline models to make forecasts. You cannot
            fit a
            statistical model or use deep learning techniques on data that takes random steps in the future.
            Ultimately,
            you cannot predict random movements. </p>
        <p>You learned that a random walk is a sequence where the first difference is not
            autocorrelated and is a stationary process, meaning that its mean, variance, and autocorrelation are
            constant over time. The steps required to identify a random walk are shown in figure 3.20. </p>
        <p>But what happens if your process is stationary and autocorrelated, meaning that you see
            consecutive significant coefficients on the ACF plot? For now, figure 3.20 simply states that it is not
            a
            random walk, so you have to find another model to approximate the process and forecast it. In such a
            situation, you are facing a process that can be approximated by the moving average (MA) model, an
            autoregressive (AR) model, </p>
        <p>or the combination of both processes, leading to an autoregressive moving average (ARMA)
            model. </p>
        <p>In the next chapter, we will focus solely on the moving average model. You'll learn how to
            identify such processes and how to use the moving average model to make forecasts. </p>
        <p><a id="calibre_link-55"></a><b>56</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Figure 3.20</p>
        <p>Steps to identify </p>
        <p>Not a random walk</p>
        <p>a random walk</p>
        <p> <i><b>3.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Now is a great time to apply the different skills you learned in this chapter. The following
            three exercises will test your knowledge and understanding of random walks and forecasting a random
            walk.
            The exercises are in order of difficulty and the time required to complete them. The solutions to
            exercises
            3.5.1 and 3.5.2 are on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03">https://</a></p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH03</a>.
        </p>
        <p> <i><b>3.5.1</b></i></p>
        <p> <i><b>Simulate and forecast a random walk</b></i></p>
        <p>Simulate a different random walk than the one we have worked with in this chapter. </p>
        <p>You can simply change the seed and get new values:</p>
        <p>1</p>
        <p>Generate a random walk of 500 timesteps. Feel free to choose an initial value</p>
        <p>different from 0. Also, make sure you change the seed by passing a different integer to
            np.random.seed(). </p>
        <p>2</p>
        <p>Plot your simulated random walk. </p>
        <p>3</p>
        <p>Test for stationarity. </p>
        <p>4</p>
        <p>Apply a first-order difference. </p>
        <p><a id="calibre_link-56"></a> <i><b>Summary</b></i></p>
        <p><b>57</b></p>
        <p>5</p>
        <p>Test for stationarity. </p>
        <p>6</p>
        <p>Split your simulated random walk into a train set containing the first 400 time-</p>
        <p>steps. The remaining 100 timesteps will be your test set. </p>
        <p>7</p>
        <p>Apply different naive forecasting methods and measure the MSE. Which</p>
        <p>method yields the lowest MSE? </p>
        <p>8</p>
        <p>Plot your forecasts. </p>
        <p>9</p>
        <p>Forecast the next timestep over the test set and measure the MSE. Did it decrease? </p>
        <p>10</p>
        <p>Plot your forecasts. </p>
        <p> <i><b>3.5.2</b></i></p>
        <p> <i><b>Forecast the daily closing price of GOOGL</b></i>
        </p>
        <p>Using the GOOGL dataset that we worked with in this chapter, apply the forecasting
            techniques we've discussed and measure their performance:</p>
        <p>1</p>
        <p>Keep the last 5 days of data as a test set. The rest will be the train set. </p>
        <p>2</p>
        <p>Forecast the last 5 days of the closing price using naive forecasting methods and measure
            the MSE. Which method is the best? </p>
        <p>3</p>
        <p>Plot your forecasts. </p>
        <p>4</p>
        <p>Forecast the next timestep over the test set and measure the MSE. Did it decrease? </p>
        <p>5</p>
        <p>Plot your forecasts. </p>
        <p> <i><b>3.5.3</b></i></p>
        <p> <i><b>Forecast the daily closing price of a stock of your
                    choice</b></i></p>
        <p>The historical daily closing price of many stocks is available for free on <a
                href="http://finance.yahoo.com">finance.yahoo</a></p>
        <p><a href="http://finance.yahoo.com">.com</a>. Select a stock ticker of your choice, and
            download its historical daily closing price for 1 year:</p>
        <p>1</p>
        <p>Plot the daily closing price of your chosen stock. </p>
        <p>2</p>
        <p>Determine if it is a random walk or not. </p>
        <p>3</p>
        <p>If it is not a random walk, explain why. </p>
        <p>4</p>
        <p>Keep the last 5 days of data as a test set. The rest will be the train set. </p>
        <p>5</p>
        <p>Forecast the last 5 days using naive forecasting methods, and measure the MSE. </p>
        <p>Which method is the best? </p>
        <p>6</p>
        <p>Plot your forecasts. </p>
        <p>7</p>
        <p>Forecast the next timestep over the test set, and measure the MSE. Did it decrease? </p>
        <p>8</p>
        <p>Plot your forecasts. </p>
        <p> <i><b>Summary</b></i></p>
        <p> A random walk is a process where the first difference is stationary and not
            autocorrelated. </p>
        <p> We cannot use statistical or deep learning techniques on a random walk, since it moves at
            random in the future. Therefore, we must use naive forecasts. </p>
        <p><a id="calibre_link-297"></a><b>58</b></p>
        <p>CHAPTER 3</p>
        <p> <i><b>Going on a random walk</b></i></p>
        <p> A stationary time series is one whose statistical properties (mean, variance,
            autocorrelation) do not change over time. </p>
        <p> The augmented Dickey-Fuller (ADF) test is used to assess stationarity by testing for unit
            roots. </p>
        <p> The null hypothesis of the ADF test is that there is a unit root in the series. If the ADF
            statistic is a large negative value and the p-value is less than 0.05, the null hypothesis is rejected,
            and
            the series is stationary. </p>
        <p> Transformations are used to make a series stationary. Differencing can stabilize the trend
            and seasonality, while logarithms stabilize the variance. </p>
        <p> Autocorrelation measures the correlation between a variable and itself at a previous
            timestep (lag). The autocorrelation function (ACF) shows how the auto-</p>
        <p>correlation changes as a function of the lag. </p>
        <p> Ideally, we will forecast a random walk in the short term or the next timestep. </p>
        <p>That way, we do not allow for random numbers to accumulate, which will degrade the quality
            of our forecasts in the long term. </p>
        <p><a id="calibre_link-5"></a> <i>Part 2</i></p>
        <p> <i>Forecasting with</i></p>
        <p> <i>statistical models</i></p>
        <p>In this part of the book, we'll explore statistical models for time series fore-</p>
        <p>casting. When performing statistical modeling, we need to perform hypothesis</p>
        <p>testing, study our data carefully to extract its properties, and find the best model for our
            data. </p>
        <p>By the end of this part, you will have a robust framework for modeling any type</p>
        <p>of time series using statistical models. You will develop MA( <i>q</i>)
            models, AR( <i>p</i>) models, ARMA( <i>p</i>, <i>q</i>)
            models, ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) models
            for
            non-stationary time series, SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>, <i
                class="calibre3">Q</i>) <i>m</i> for seasonal time series, and SARIMAX models to
            include external variables in your forecast. We'll also cover the VAR( <i>p</i>) model
            for
            predicting many time series at once. We'll conclude this part of the book with a capstone project, so
            that
            you'll get to apply what you've learned on your own. </p>
        <p>There are, of course, many other statistical models for time series forecast-</p>
        <p>ing. For example, exponential smoothing basically takes a weighted average of</p>
        <p>past values to predict future values. The general idea behind exponential smoothing is that
            past values are less important than more recent values when predict-</p>
        <p>ing the future, so they are assigned a smaller weight. This model can then be extended to
            include trend and seasonal components. There are also statistical approaches to modeling time series
            with
            different seasonal periods, such as the</p>
        <p>BATS and TBATS models. </p>
        <p>To keep this section manageable, we won't address those models, but they are implemented in
            the statsmodels library, which we will use extensively. </p>
        <p><a id="calibre_link-349"></a> </p>
        <p><a id="calibre_link-6"></a> <i>Modeling a moving</i></p>
        <p> <i>average process</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Defining a moving average process</p>
        <p> Using the ACF to identify the order of a moving </p>
        <p>average process</p>
        <p> Forecasting a time series using the moving </p>
        <p>average model</p>
        <p>In the previous chapter, you learned how to identify and forecast a random walk process. We
            defined a random walk process as a series whose first difference is stationary with no autocorrelation.
            This
            means that plotting its ACF will show no significant coefficients after lag 0. However, it is possible
            that
            a stationary process may still exhibit autocorrelation. In this case, we have a time series that can be
            approximated by a moving average model MA( <i>q</i>), an autoregressive model AR( <i
                class="calibre3">p</i>), or an autoregressive moving average model ARMA( <i>p</i>,
            <i>q</i>). In this chapter, we will focus on identifying and modeling using the moving
            average model. </p>
        <p>Suppose that you want to forecast the volume of widget sales from the XYZ Wid-</p>
        <p>get Company. By predicting futures sales, the company will be able to better manage its
            production of widgets and avoid producing too many or too few. If not enough widgets are produced, the
            company will not be able to meet their clients'</p>
        <p><b>61</b></p>
        <p><a id="calibre_link-190"></a><img src="images/000085.jpg" alt="Image 37" class="calibre2" />
        </p>
        <p><b>62</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>demands, leaving customers unhappy. On the other hand, producing too many wid-</p>
        <p>gets will increase inventory. The widgets might become obsolete or lose their value, which
            will increase the business's liabilities, ultimately making shareholders unhappy. </p>
        <p>In this example, we will study the sales of widgets over 500 days starting in 2019. </p>
        <p>The recorded sales over time are shown in figure 4.1. Note that the volume of sales is
            expressed in thousands of US dollars. </p>
        <p>Figure 4.1</p>
        <p>Volume of widget sales for the XYZ Widget Company over 500 days, </p>
        <p>starting on January 1, 2019. This is fictional data, but it will be useful for learning how
            to identify and model a moving average process. </p>
        <p>Figure 4.1 shows a long-term trend with peaks and troughs along the way. We can intuitively
            say that this time series is not a stationary process, since we can observe a trend over time.
            Furthermore,
            there is no apparent cyclical pattern in the data, so we can rule out any seasonal effects for now. </p>
        <p>In order to forecast the volume of widget sales, we need to identify the underlying process.
            To do so, we will apply the same steps that we covered in chapter 3 when working with a random walk
            process,
            shown again in figure 4.2. </p>
        <p>Once the data is gathered, we will test for stationarity. If it is not stationary, we will
            apply a transformation to make it stationary. Then, once the series is a stationary process, we will
            plot
            the autocorrelation function (ACF). In our example of forecasting widget sales, our process will show
            significant coefficients in the ACF plot, meaning that it cannot be approximated by the random walk
            model.
        </p>
        <p>In this chapter, we will discover that the volume of widget sales from the XYZ Widget
            Company can be approximated as a moving average process, and we will look at</p>
        <p><a id="calibre_link-57"></a> <i><b>4.1</b></i></p>
        <p> <i><b>Defining a moving average process</b></i></p>
        <p><b>63</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Figure 4.2</p>
        <p>Steps for identifying </p>
        <p>Not a random walk</p>
        <p>a random walk</p>
        <p>the definition of the moving average model. Then you'll learn how to identify the order of
            the moving average process using the ACF plot. The order of this process determines the number of
            parameters
            for the model. Finally, we will apply the moving average model to forecast the next 50 days of widget
            sales.
        </p>
        <p> <i><b>4.1</b></i></p>
        <p> <i><b>Defining a moving average process</b></i></p>
        <p>A <i>moving average process</i>, or the moving average (MA) model, states
            that the current value is linearly dependent on the current and past error terms. The error terms are
            assumed to be mutually independent and normally distributed, just like white noise. </p>
        <p>A moving average model is denoted as MA( <i>q</i>), where <i>q</i> is
            the
            order. The model expresses the present value as a linear combination of
            the mean of the series µ, the present error term ϵ <i>t</i>, and past error terms ϵ <i
                class="calibre3">t&ndash;q</i>. The magnitude of the impact of past errors on the present value is
            quantified using a coefficient denoted as θ <i>q</i>. Mathematically, we express a
            general
            moving average process of order <i>q</i> as in equation 4.1. </p>
        <p> <i>y</i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = µ + ϵ <i>t</i> + θ1 <i>t
            </i>&ndash;1 + θ2 <i>t </i>&ndash;2 +⋅⋅⋅+ θ <i>q t&ndash;q</i></p>
        <p>Equation 4.1</p>
        <p><a id="calibre_link-58"></a><b>64</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>Moving average process</p>
        <p>In a moving average (MA) process, the current value depends linearly on the mean of the
            series, the current error term, and past error terms. </p>
        <p>The moving average model is denoted as MA( <i>q</i>), where <i>q</i> is
            the
            order. The general expression of an MA( <i>q</i>) model
            is</p>
        <p> <i>y</i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = µ + ϵ <i>t</i> + θ1 <i>t-1</i> + θ2
            <i>t</i>&ndash;2 +⋅⋅⋅+ θ <i>q
                t&ndash;q</i></p>
        <p>The order <i>q</i> of the moving average model determines the number of
            past error terms that affect the present value. For example, if it is of order 1, meaning that we have
            an
            MA(1) process, the model is expressed as in equation 4.2. Here we can see that the present value <i
                class="calibre3">yt</i> is dependent on the mean µ, the present error term ϵ <i>t</i>,
            and the error term at the previous timestep θ ϵ</p>
        <p>1 <i>t </i>&ndash;1. </p>
        <p> <i>y</i></p>
        <p>ϵ</p>
        <p> <i>t</i> = µ + ϵ <i>t</i> + θ1 <i>t
            </i>&ndash;1</p>
        <p>Equation 4.2</p>
        <p>If we have a moving average process of order 2, or MA(2), then <i>yt</i> is
            dependent on the mean of the series µ, the present error term ϵ <i>t</i>, the error
            term at
            the previous timestep θ ϵ</p>
        <p>ϵ</p>
        <p>1 <i>t </i>&ndash;1, and the error term two timesteps prior θ2 <i>t
            </i>&ndash;2, resulting in equation 4.3. </p>
        <p> <i>y</i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = µ + ϵ <i>t</i> + θ1 <i>t
            </i>&ndash;1 + θ2 <i>t </i>&ndash;2</p>
        <p>Equation 4.3</p>
        <p>Hence, we can see how the order <i>q</i> of the MA( <i>q</i>) process
            affects the number of past error terms that must be included in the
            model. The larger <i>q</i> is, the more past error terms affect the present value.
            Therefore, it is important to determine the order of the moving average process in order to fit the
            appropriate model&mdash;if we have a second-order moving average process, then a second-order moving
            average
            model will be used for forecasting. </p>
        <p> <i><b>4.1.1</b></i></p>
        <p> <i><b>Identifying the order of a moving average
                    process</b></i></p>
        <p>To identify the order of a moving average process, we can extend the steps needed to
            identify a random walk, as shown in figure 4.3. </p>
        <p>As usual, the first step is to gather the data. Then we test for stationarity. If our series
            is not stationary, we apply transformations, such as differencing, until the series is stationary. Then
            we
            plot the ACF and look for significant autocorrelation coefficients. In the case of a random walk, we
            will
            not see significant coefficients after lag 0. </p>
        <p>On the other hand, if we see significant coefficients, we must check whether they become
            abruptly non-significant after some lag <i>q</i>. If that is the case, then we know
            that we
            have a moving average process of order <i>q</i>. Otherwise, we must follow a different
            set
            of steps to discover the underlying process of our time series. </p>
        <p>Let's put this in action using our data for the volume of widget sales for the XYZ Widget
            Company. The dataset contains 500 days of sales volume data starting on January 1, </p>
        <p><a id="calibre_link-350"></a> <i><b>4.1</b></i></p>
        <p> <i><b>Defining a moving average process</b></i></p>
        <p><b>65</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( <i>q</i>) process. </p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Not a moving average</p>
        <p>Figure 4.3</p>
        <p>Steps to identify the order </p>
        <p>process</p>
        <p>of a moving average process</p>
        <p>2019. We will follow the steps outlined in figure 4.3 and determine the order of the
            underlying moving average process. </p>
        <p>The first step is to gather the data. This step has already been done for you, so this is a
            great time to load the data into a DataFrame using pandas and display the first five rows of data. At
            any
            point, you can refer to the source code for this chapter on GitHub:</p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH04">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH04</a>.
        </p>
        <p>import pandas as pd</p>
        <p><b>Read the CSV file </b></p>
        <p><b>into a DataFrame. </b></p>
        <p>df = pd.read_csv('../data/widget_sales.csv') </p>
        <p>df.head() </p>
        <p><b>Display the first </b></p>
        <p><b>five rows of data. </b></p>
        <p>You'll see that the volume of sales is in the widget_sales column. Note that the volume of
            sales is in units of thousands of US dollars. </p>
        <p><a id="calibre_link-351"></a><img src="images/000024.jpg" alt="Image 38" class="calibre2" />
        </p>
        <p><b>66</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>We can plot our data using matplotlib. Our values of interest are in the widget_</p>
        <p>sales columns, so that is what we pass into ax.plot(). Then we give the <i>x</i>-axis
            the
            label of “Time” and <i>y</i>-axis the label of “Widget
            sales (k$).” Next, we specify that the labels for the ticks on the <i>x</i>-axis should
            display the month of the year. Finally, we tilt the <i>x</i>-axis tick labels and
            remove
            extra whitespace around the figure using plt.tight_layout(). The result is figure 4.4. </p>
        <p>import matplotlib.pyplot as plt</p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>Plot the volume </b></p>
        <p><b>of widget sales. </b></p>
        <p>ax.plot(df['widget_sales']) </p>
        <p>ax.set_xlabel('Time') </p>
        <p><b>Label the x-axis. </b></p>
        <p>ax.set_ylabel('Widget sales (k$)') </p>
        <p><b>Label the y-axis. </b></p>
        <p>plt.xticks(</p>
        <p>[0, 30, 57, 87, 116, 145, 175, 204, 234, 264, 293, 323, 352, 382, 409, </p>
        <p>➥ 439, 468, 498], </p>
        <p>['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', </p>
        <p>➥ 'Nov', 'Dec', '2020', 'Feb', 'Mar', 'Apr', 'May', 'Jun']) </p>
        <p><b>Label the </b></p>
        <p><b>ticks on </b></p>
        <p><b>Tilt the labels on the x-axis ticks </b></p>
        <p>fig.autofmt_xdate() </p>
        <p><b>the x-axis. </b></p>
        <p><b>so that they display nicely. </b></p>
        <p>plt.tight_layout() </p>
        <p><b>Remove extra whitespace </b></p>
        <p><b>around the figure. </b></p>
        <p>Figure 4.4</p>
        <p>Volume of widget sales for the XYZ Widget Company over 500 days, starting </p>
        <p>on January 1, 2019</p>
        <p><a id="calibre_link-204"></a> <i><b>4.1</b></i></p>
        <p> <i><b>Defining a moving average process</b></i></p>
        <p><b>67</b></p>
        <p>The next step is to test for stationarity. We intuitively know that the series is not
            stationary, since there is an observable trend in figure 4.4. Still, we will use the ADF test to make
            sure.
            Again, we'll use the adfuller function from the statsmodels library and extract the ADF statistic and
            p-value. If the ADF statistic is a large negative number and the p-value is smaller than 0.05, our
            series is
            stationary. Otherwise, we must apply transformations. </p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p><b>Run the ADF test on the volume </b></p>
        <p><b>of widget sales, which is stored </b></p>
        <p><b>in the widget_sales column. </b></p>
        <p>ADF_result = adfuller(df['widget_sales']) </p>
        <p><b>Print the ADF statistic. </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}') </p>
        <p>print(f'p-value: {ADF_result[1]}') </p>
        <p><b>Print the p-value. </b></p>
        <p>This results in an ADF statistic of &ndash;1.51 and a p-value of 0.53. Here, the ADF
            statistic is not a large negative number, and the p-value is greater than 0.05. Therefore, our time
            series
            is not stationary, and we must apply transformations to make it stationary. </p>
        <p>In order to make our series stationary, we will try to stabilize the trend by applying a
            first-order differencing. We can do so by using the diff method from the numpy library. Remember that
            this
            method takes in a parameter n that specifies the order of differencing. In this case, because it is a
            first-order differencing, n will be equal to 1. </p>
        <p>import numpy as np</p>
        <p><b>Apply first-order differencing </b></p>
        <p><b>on our data and store the </b></p>
        <p><b>result in widget_sales_diff. </b></p>
        <p>widget_sales_diff = np.diff(df['widget_sales'], n=1) </p>
        <p>We can optionally plot the differenced series to see if we have stabilized the trend. Figure
            4.5 shows the differenced series. We can see that we successfully removed the long-term trend component
            of
            our series, as values are hovering around 0 over the entire period. </p>
        <p>Can you recreate figure 4.5? </p>
        <p>While optional, it is a good idea to plot your series as you apply transformations. This
            will give you a better intuition as to whether a series is stationary or not after a particular
            transformation. Try recreating figure 4.5 on your own. </p>
        <p>Now that a transformation has been applied to our series, we can test for stationarity again
            using the ADF test. This time, make sure to run the test on the differenced data stored in the
            widget_sales_diff variable. </p>
        <p>ADF_result = adfuller(widget_sales_diff) </p>
        <p><b>Run the ADF test </b></p>
        <p><b>on the differenced </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p><b>time series. </b></p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p><a id="calibre_link-248"></a><img src="images/000086.jpg" alt="Image 39" class="calibre2" />
        </p>
        <p><b>68</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>Figure 4.5</p>
        <p>Differenced volume of widget sales. The trend component has been </p>
        <p>stabilized, since values are hovering around 0 over our entire sample. </p>
        <p>This gives an ADF statistic of &ndash;10.6 and a p-value of 7 × 10&ndash;19. Therefore, with
            a large negative ADF statistic and a p-value much smaller than 0.05, we can say that our series is
            stationary. </p>
        <p>Our next step is to plot the autocorrelation function. The statsmodels library conveniently
            includes the plot_acf function. We simply pass in our differenced series and specify the number of lags
            in
            the lags parameter. Remember that the number of lags determines the range of values on the <i
                class="calibre3">x</i>-axis. </p>
        <p>from statsmodels.graphics.tsaplots import plot_acf</p>
        <p>plot_acf(widget_sales_diff, lags=30); </p>
        <p><b>Plot the ACF of the </b></p>
        <p><b>differenced series. </b></p>
        <p>plt.tight_layout()</p>
        <p>The resulting ACF plot is shown in figure 4.6. You'll notice that there are significant
            coefficients up until lag 2. Then they abruptly become non-significant, as they remain in the shaded
            area of
            the plot. This means that we have a stationary moving average process of order 2. We can use a
            second-order
            moving average model, or MA(2) model, to forecast our stationary time series. </p>
        <p>You can see how the ACF plot helps us determine the order of a moving average</p>
        <p>process. The ACF plot will show significant autocorrelation coefficients up until lag <i
                class="calibre3">q</i>, after which all coefficients will be non-significant. We can then conclude
            that
            we have a moving average process of order <i>q</i>, or an MA( <i>q</i>)
            process. </p>
        <p><a id="calibre_link-59"></a><img src="images/000066.jpg" alt="Image 40" class="calibre2" />
        </p>
        <p> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p><b>69</b></p>
        <p>Figure 4.6</p>
        <p>ACF plot of the differenced series. Notice how the coefficients are </p>
        <p>significant up until lag 2, and then they fall abruptly into the non-significance zone
            (shaded area) of the plot. There are some significant coefficients around lag 20, but this is likely due
            to
            chance, since they are non-significant between lags 3 and 20 and after lag 20. </p>
        <p> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p>Once the order <i>q </i> of the moving average process is identified, we
            can fit the model to our training data and start forecasting. In our case, we discovered that the
            differenced volume of widget sales is a moving average process of order 2, or an MA(2) process. </p>
        <p>The moving average model assumes stationarity, meaning that our forecasts must</p>
        <p>be done on a stationary time series. Therefore, we will train and test our model on the
            differenced volume of widget sales. We will try two naive forecasting techniques and fit a second-order
            moving average model. The naive forecasts will serve as baselines to evaluate the performance of the
            moving
            average model, which we expect to be better than the baselines, since we previously identified our
            process
            to be a moving average process of order 2. Once we obtain our forecasts for the stationary process, we
            will
            have to inverse-transform the forecasts, meaning that we must undo the process of differencing to bring
            the
            forecasts back to their original scale. </p>
        <p>In this scenario, we will allocate 90% of the data to the train set and reserve the other
            10% for the test set, meaning that we must forecast 50 timesteps into the future. </p>
        <p>We will assign our differenced data to a DataFrame and then split the data. </p>
        <p><b>The first 90% of the data goes in the training set. </b></p>
        <p><b>Place the differenced data in a DataFrame. </b></p>
        <p>df_diff = pd.DataFrame({'widget_sales_diff': widget_sales_diff}) </p>
        <p>train = df_diff[:int(0.9*len(df_diff))] </p>
        <p><b>The last 10% of the data goes </b></p>
        <p>test = df_diff[int(0.9*len(df_diff)):] </p>
        <p><b>in the test set for prediction. </b></p>
        <p><a id="calibre_link-352"></a><img src="images/000081.jpg" alt="Image 41" class="calibre2" />
        </p>
        <p><b>70</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>print(len(train))</p>
        <p>print(len(test))</p>
        <p>We've printed out the size of the train and test sets to remind you of the data point that
            we lose when we difference. The original dataset contained 500 data points, while the differenced series
            contains a total of 499 data points, since we differenced once. </p>
        <p>Now we can visualize the forecasting period for the differenced and original series. </p>
        <p>Here we will make two subplots in the same figure. The result is shown in figure 4.7. </p>
        <p>fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True) </p>
        <p>ax1.plot(df['widget_sales'])</p>
        <p><b>Make two</b></p>
        <p>ax1.set_xlabel('Time')</p>
        <p><b>subplots inside</b></p>
        <p>ax1.set_ylabel('Widget sales (k$)')</p>
        <p><b>the same figure. </b></p>
        <p>ax1.axvspan(450, 500, color='#808080', alpha=0.2)</p>
        <p>ax2.plot(df_diff['widget_sales_diff'])</p>
        <p>ax2.set_xlabel('Time')</p>
        <p>ax2.set_ylabel('Widget sales - diff (k$)')</p>
        <p>ax2.axvspan(449, 498, color='#808080', alpha=0.2)</p>
        <p>plt.xticks(</p>
        <p>[0, 30, 57, 87, 116, 145, 175, 204, 234, 264, 293, 323, 352, 382, 409, </p>
        <p>439, 468, 498], </p>
        <p>['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', </p>
        <p>'Nov', 'Dec', '2020', 'Feb', 'Mar', 'Apr', 'May', 'Jun'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 4.7</p>
        <p>Forecasting period for the original and differenced series. Remember that </p>
        <p>our differenced series has one less data point than in its original state. </p>
        <p><a id="calibre_link-217"></a> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p><b>71</b></p>
        <p>For the forecast horizon, the moving average model brings in a particularity. The MA( <i
                class="calibre3">q</i>) model does not allow us to forecast 50 steps into the future in one shot.
        </p>
        <p>Remember that the moving average model is linearly dependent on past error terms, and those
            terms are not observed in the dataset&mdash;they must therefore be recursively estimated. This means
            that
            for an MA( <i>q</i>) model, we can only forecast <i>q</i> steps into
            the
            future. Any prediction made beyond that point will not have past error terms, and the model will only
            predict the mean. Therefore, there is no added value in forecasting beyond <i>q</i>
            steps
            into the future, because the predictions will fall flat, as only the mean is returned, which is
            equivalent
            to a baseline model. </p>
        <p>To avoid simply predicting the mean beyond two timesteps into the future, we need to develop
            a function that will predict two timesteps or less at a time, until 50</p>
        <p>predictions are made, so that we can compare our predictions against the observed values of
            the test set. This method is called <i>rolling forecasts</i>. On the first pass, we
            will
            train on the first 449 timesteps and predict timesteps 450 and 451. Then, on the second pass, we will
            train
            on the first 451 timesteps, and predict timesteps 452 and 453. </p>
        <p>This is repeated until we finally predict the values at timesteps 498 and 499. </p>
        <p>Forecasting using the MA( <i><b>q</b></i>) model</p>
        <p>When using an MA( <i>q</i>) model, forecasting beyond <i>q</i> steps
            into
            the future will simply return the mean, because there are no error
            terms to estimate beyond <i>q</i> steps. We can use rolling forecasts to predict up to
            <i>q</i> steps at a time in order avoid predicting only the mean of the series. </p>
        <p>We will compare our fitted MA(2) model to two baselines: the historical mean and the last
            value. That way, we can make sure that an MA(2) model will yield better predictions than naive
            forecasts,
            which should be the case, since we know the stationary process is an MA(2) process. </p>
        <p>NOTE</p>
        <p>You do not have to forecast two steps ahead when you perform rolling</p>
        <p>forecasts with an MA(2) model. You can forecast either one or two steps ahead repeatedly in
            order to avoid predicting only the mean. Similarly, with</p>
        <p>an MA(3) model, you could perform rolling forecasts with one-, two-, or three-step-ahead
            rolling forecasts. </p>
        <p>To create these forecasts, we need a function that will repeatedly fit a model and generate
            forecasts over a certain window of time, until forecasts for the entire test set are obtained. This
            function
            is shown in listing 4.1. </p>
        <p>First, we import the SARIMAX function from the statsmodels library. This function will allow
            us to fit an MA(2) model to our differenced series. Note that SARIMAX is a complex model that allows us
            to
            consider seasonal effects, autoregressive processes, non-stationary time series, moving average
            processes,
            and exogenous variables all in a single model. For now, we will disregard all factors except the moving
            average portion. </p>
        <p><a id="calibre_link-250"></a><b>72</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>We will gradually build on the moving average model and eventually reach the SARIMAX</p>
        <p>model in later chapters:</p>
        <p> Next, we define our rolling_forecast function. It will take in a DataFrame, the length of
            the training set, the forecast horizon, a window size, and a method. </p>
        <p>The DataFrame contains the entire time series. </p>
        <p> The train_len parameter initializes the number of data points that can be used to fit a
            model. As predictions are done, we can update this to simulate the observation of new values and then
            use
            them to make the next sequence of forecasts. </p>
        <p> The horizon parameter is equal to the length of the test set and represents how many
            values must be predicted. </p>
        <p> The window parameter specifies how many timesteps are predicted at a time. In</p>
        <p>our case, because we have an MA(2) process, the window will be equal to 2. </p>
        <p> The method parameter specifies what model to use. The same function allows us</p>
        <p>to generate forecasts from the naive methods and the MA(2) model. </p>
        <p>Note the use of type hinting in the function declaration. This will help us avoid passing
            parameters of an unexpected type, which might cause our function to fail. </p>
        <p>Then, each forecasting method is run in a loop. The loop starts at the end of the training
            set and continues until total_len, exclusive, with steps of window (total_len is the sum of train_len
            and
            horizon). This loop generates a list of 25 values, </p>
        <p>[450,451,452,…,497], but each pass generates two forecasts, thus returning a list of 50</p>
        <p>forecasts for the entire test set. </p>
        <p>Listing 4.1</p>
        <p>A function for rolling forecasts on a horizon </p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, </p>
        <p>➥ window: int, method: str) -&gt; list: </p>
        <p><b>The function takes in a </b></p>
        <p></p>
        <p><b>DataFrame containing the </b></p>
        <p>total_len = train_len + horizon</p>
        <p><b>full simulated moving </b></p>
        <p></p>
        <p><b>average process. We also </b></p>
        <p>if method == 'mean':</p>
        <p><b>pass in the length of the </b></p>
        <p>pred_mean = []</p>
        <p><b>training set (800 in this </b></p>
        <p></p>
        <p><b>case) and the horizon of </b></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p><b>the forecast (200). The </b></p>
        <p>mean = np.mean(df[:i].values)</p>
        <p><b>next parameter specifies </b></p>
        <p>pred_mean.extend(mean for _ in range(window))</p>
        <p><b>how many steps at a time </b></p>
        <p><b>we wish to forecast (2). </b></p>
        <p>return pred_mean</p>
        <p><b>Finally, we specify the </b></p>
        <p><b>method to use to make </b></p>
        <p>elif method == 'last':</p>
        <p><b>forecasts. </b></p>
        <p>pred_last_value = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>last_value = df[:i].iloc[-1].values[0]</p>
        <p>pred_last_value.extend(last_value for _ in range(window))</p>
        <p><a id="calibre_link-353"></a> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p><b>73</b></p>
        <p>return pred_last_value</p>
        <p></p>
        <p>elif method == 'MA':</p>
        <p>pred_MA = []</p>
        <p><b>The MA(q) model is </b></p>
        <p><b>The predicte </b> </p>
        <p><b>d_mean</b></p>
        <p><b>part of the more </b></p>
        <p><b>method allows us to</b></p>
        <p><b>complex SARIMAX </b></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p><b>retrieve the actual</b></p>
        <p><b>model. </b></p>
        <p>model = SARIMAX(df[:i], order=(0,0,2)) </p>
        <p><b>value of the </b> </p>
        <p><b>forecast</b></p>
        <p>res = model.fit(disp=False)</p>
        <p><b>as defined by the</b></p>
        <p>predictions = res.get_prediction(0, i + window - 1)</p>
        <p><b>statsmodels library. </b></p>
        <p>oos_pred = predictions.predicted_mean.iloc[-window:] </p>
        <p>pred_MA.extend(oos_pred)</p>
        <p></p>
        <p>return pred_MA</p>
        <p>Once it's defined, we can use our function and forecast using three methods: the historical
            mean, the last value, and the fitted MA(2) model. </p>
        <p>First, we'll first create a DataFrame to hold our predictions and name it pred_df. </p>
        <p>We can copy the test set, to include the actual values in pred_df, making it easier to
            evaluate the performance of our models. </p>
        <p>Then, we'll specify some constants. In Python, it is a good practice to name constants in
            capital letters. TRAIN_LEN is simply the length of our training set, HORIZON is the length of the test
            set,
            which is 50 days, and WINDOW can be 1 or 2 because we are using an MA(2) model. In this case we will use
            a
            value of 2. </p>
        <p>Next, we'll use our rolling_forecast function to generate a list of predictions for each
            method. Each list of predictions is then stored in its own column in pred_df. </p>
        <p>pred_df = test.copy()</p>
        <p>TRAIN_LEN = len(train)</p>
        <p>HORIZON = len(test)</p>
        <p>WINDOW = 2</p>
        <p>pred_mean = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'mean')</p>
        <p>pred_last_value = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'last')</p>
        <p>pred_MA = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'MA')</p>
        <p>pred_df['pred_mean'] = pred_mean</p>
        <p>pred_df['pred_last_value'] = pred_last_value</p>
        <p>pred_df['pred_MA'] = pred_MA</p>
        <p>pred_df.head()</p>
        <p>Now we can visualize our predictions against the observed values in the test set. Keep in
            mind that we are still working with the differenced dataset, so our predictions are also differenced
            values.
        </p>
        <p>For this figure, we will plot part of the training data to see the transition between the
            train and test sets. Our observed values will be a solid line, and we will label this curve as “actual.”
            Then we'll plot the forecasts from the historical mean, those from</p>
        <p><a id="calibre_link-266"></a><img src="images/000168.jpg" alt="Image 42" class="calibre2" />
        </p>
        <p><b>74</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>the last observed value, and those from the MA(2) model. They will respectively be a dotted
            line, a dotted and dashed line, and a dashed line, with labels of “mean,” “last,” </p>
        <p>and “MA(2).” The result is shown in figure 4.8. </p>
        <p>Figure 4.8</p>
        <p>Forecasts of the differenced volume of widget sales. In a professional </p>
        <p>setting, it does not make sense to report differenced predictions. Therefore, we will undo
            the transformation later on. </p>
        <p>In figure 4.8 you'll notice that the prediction coming from the historical mean, shown as a
            dotted line, is almost a straight line. This is expected; the process is stationary, so the historical
            mean
            should be stable over time. </p>
        <p>The next step is to measure the performance of our models. To do so, we will calculate the
            mean squared error (MSE). Here we will use the mean_squared_error function from the sklearn package. We
            simply need to pass the observed values and the predicted values into the function. </p>
        <p>from sklearn.metrics import mean_squared_error</p>
        <p>mse_mean = mean_squared_error(pred_df['widget_sales_diff'], </p>
        <p>➥ pred_df['pred_mean'])</p>
        <p>mse_last = mean_squared_error(pred_df['widget_sales_diff'], </p>
        <p>➥ pred_df['pred_last_value'])</p>
        <p>mse_MA = mean_squared_error(pred_df['widget_sales_diff'], </p>
        <p>➥ pred_df['pred_MA'])</p>
        <p>print(mse_mean, mse_last, mse_MA)</p>
        <p>This prints out an MSE of 2.56 for the historical mean method, 3.25 for the last value
            method, and 1.95 for the MA(2) model. Here our MA(2) model is the best-performing</p>
        <p><a id="calibre_link-354"></a><img src="images/000000.jpg" alt="Image 43" class="calibre2" />
        </p>
        <p> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p><b>75</b></p>
        <p>forecasting method, since its MSE is the lowest of the three methods. This is expected,
            because we previously identified a second-order moving average process for the differenced volume of
            widget
            sales, thus resulting in a smaller MSE compared to the naive forecasting methods. We can visualize the
            MSE
            for all forecasting techniques in figure 4.9. </p>
        <p>Figure 4.9</p>
        <p>MSE for each forecasting method on the differenced volume of widget </p>
        <p>sales. Here the MA(2) model is the champion, since its MSE is the lowest. </p>
        <p>Now that we have our champion model on the stationary series, we need to inverse-transform
            our predictions to bring them back to the original scale of the untransformed dataset. Recall that
            differencing is the result of the difference between a value at time <i>t</i> and its
            previous value, as shown in figure 4.10. </p>
        <p>In order to reverse our first-order difference, we need to add an initial value <i>y</i>
            0
            to the first differenced value <i>y' </i> 1. That way, we can
            recover <i>y</i> 1 in its original scale. This is what is demonstrated in equation 4.4:
        </p>
        <p> <i> y</i> 1 = <i>y</i> 0 + <i>y' </i> 1
            = <i>y</i> 0 + <i>y</i> 1 &ndash; <i>y</i> 0 = <i class="calibre3">y</i> 1</p>
        <p>Equation 4.4</p>
        <p>Then <i>y</i> 2 can be obtained using a cumulative sum of the differenced
            values, as shown in equation 4.5. </p>
        <p> <i> y</i> 2 = <i>y</i> 0 + <i>y' </i> 1
            + <i>y' </i> 2 = <i>y</i> 0 + <i>y</i> 1 &ndash; <i class="calibre3">y</i> 0 + <i>y</i> 2 &ndash; <i>y</i> 1
            = ( <i class="calibre3">y</i> 0 &ndash; <i>y</i> 0) + ( <i>y</i> 1
            &ndash; <i>y</i> 1) + <i>y</i> 2 = <i>y</i> 2
            Equation 4.5</p>
        <p>Applying the cumulative sum once will undo a first-order differencing. In the case where the
            series was differenced twice to become stationary, we would need to repeat this process. </p>
        <p><a id="calibre_link-355"></a><b>76</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>Original</p>
        <p> <i>y</i> 0</p>
        <p> <i>y</i> 1</p>
        <p> <i>y</i> 2</p>
        <p> <i>y</i> 3</p>
        <p> <i>t </i>=0</p>
        <p> <i>t </i>=1</p>
        <p> <i>t </i>=2</p>
        <p> <i>t </i>=3</p>
        <p>First-order</p>
        <p> <i>y</i> 1 &ndash; <i>y</i> 0</p>
        <p> <i>y</i> 2 &ndash; <i>y</i> 1</p>
        <p> <i>y</i> 3 &ndash; <i>y</i> 2</p>
        <p>differencing</p>
        <p>Result</p>
        <p> <i>&ndash;</i></p>
        <p> <i>y' </i> 1</p>
        <p> <i>y' </i> 2</p>
        <p> <i>y' </i> 3</p>
        <p> <i>t </i>=0</p>
        <p> <i>t </i>=1</p>
        <p> <i>t </i>=2</p>
        <p> <i>t </i>=3</p>
        <p>Figure 4.10</p>
        <p>Visualizing a first-order difference</p>
        <p>Thus, to obtain our predictions in the original scale of our dataset, we need to use the
            first value of the test as our initial value. Then we can perform a cumulative sum to obtain a series of
            50
            predictions in the original scale of the dataset. We will assign these predictions to the
            pred_widget_sales
            column. </p>
        <p><b>Initialize an empty column </b></p>
        <p><b>to hold our predictions. </b></p>
        <p>df['pred_widget_sales'] = pd.Series() </p>
        <p>df['pred_widget_sales'][450:] = df['widget_sales'].iloc[450] +</p>
        <p>➥ pred_df['pred_MA'].cumsum() </p>
        <p><b>Inverse-transform the predictions </b></p>
        <p><b>to bring them back to the original </b></p>
        <p><b>scale of the dataset. </b></p>
        <p>Let's visualize our untransformed predictions against the recorded data. Remember that we
            are now using the original dataset stored in df. </p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>Plot the actual </b></p>
        <p><b>values. </b></p>
        <p>ax.plot(df['widget_sales'], 'b-', label='actual') </p>
        <p>ax.plot(df['pred_widget_sales'], 'k--', label='MA(2)') </p>
        <p><b>Plot the inverse-</b></p>
        <p><b>transformed </b></p>
        <p>ax.legend(loc=2)</p>
        <p><b>predictions. </b></p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Widget sales (K$)')</p>
        <p>ax.axvspan(450, 500, color='#808080', alpha=0.2)</p>
        <p><a id="calibre_link-258"></a><img src="images/000089.jpg" alt="Image 44" class="calibre2" />
        </p>
        <p> <i><b>4.2</b></i></p>
        <p> <i><b>Forecasting a moving average process</b></i></p>
        <p><b>77</b></p>
        <p>ax.set_xlim(400, 500)</p>
        <p>plt.xticks(</p>
        <p>[409, 439, 468, 498], </p>
        <p>['Mar', 'Apr', 'May', 'Jun'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>You can see in figure 4.11 that our forecast curve, shown with a dashed line, follows the
            general trend of the observed values, although it does not predict bigger troughs and peaks. </p>
        <p>Figure 4.11</p>
        <p>Inverse-transformed MA(2) forecasts</p>
        <p>The final step is to report the MSE on the original dataset. In a professional setting, we
            would not report the differenced predictions, because they do not make sense from a business
            perspective; we
            must report values and errors in the original scale of the data. </p>
        <p>We can measure the mean absolute error (MAE) using the mean_absolute_error</p>
        <p>function from sklearn. We'll use this metric because it is easy to interpret, as it returns
            the average of the absolute difference between the predicted and actual values, instead of a squared
            difference like the MSE. </p>
        <p>from sklearn.metrics import mean_absolute_error</p>
        <p>mae_MA_undiff = mean_absolute_error(df['widget_sales'].iloc[450:], </p>
        <p>➥ df['pred_widget_sales'].iloc[450:])</p>
        <p>print(mae_MA_undiff)</p>
        <p><a id="calibre_link-60"></a><b>78</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p>This prints out an MAE of 2.32. Therefore, our predictions are, on average, off by $2,320,
            either above or below the actual value. Remember that our data has units of thousands of dollars, so we
            multiply the MAE by 1,000 to express the average absolute difference. </p>
        <p> <i><b>4.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the moving average process and how it can be modeled by an MA(
            <i>q</i>) model, where <i>q</i> is the order. You learned that to
            identify
            a moving average process, you must study the ACF plot once it is stationary. The ACF plot will show
            significant peaks all the way to lag <i>q</i>, and the rest will not be significantly
            different from 0. </p>
        <p>However, it is possible that when studying the ACF plot of a stationary process, you'll see
            a sinusoidal pattern, with negative coefficients and significant autocorrelation at large lags. For now
            you
            can simply accept that this is not a moving average process (see figure 4.12). </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( <i>q</i>) process. </p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Figure 4.12</p>
        <p>Steps to identify the </p>
        <p>Not a moving average</p>
        <p>underlying process of a stationary </p>
        <p>process</p>
        <p>time series</p>
        <p><a id="calibre_link-61"></a> <i><b>4.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p><b>79</b></p>
        <p>When we see a sinusoidal pattern in the ACF plot of a stationary process, this is a hint
            that an autoregressive process is at play, and we must use an AR( <i>p</i>) model to
            produce our forecast. Just like the MA( <i>q</i>) model, the AR( <i>p</i>)
            model will require us to identify its order. This time we will have to plot the <i
                class="calibre3">partial</i> autocorrelation function and see at which lag the coefficients suddenly
            become non-significant. The next chapter will focus entirely on the autoregressive process, how to
            identify
            its order, and how to forecast such a process. </p>
        <p> <i><b>4.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Take some time to test your knowledge and mastery of the MA( <i>q</i>)
            model with these exercises. The full solutions are available on GitHub:<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH04">
                https://github.com/marcopeix/</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH04">TimeSeriesForecastingInPython/tree/master/CH04.
            </a></p>
        <p> <i><b>4.4.1</b></i></p>
        <p> <i><b>Simulate an MA(2) process and make
                    forecasts</b></i></p>
        <p>Simulate a stationary MA(2) process. To do so, use the ArmaProcess function from the
            statsmodels library and simulate the following process:</p>
        <p> <i>yt</i> = 0.9θ <i>t </i>&ndash;1 + 0.3θ <i>t
            </i>&ndash;2</p>
        <p>1</p>
        <p>For this exercise, generate 1,000 samples. </p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p>import numpy as np</p>
        <p><b>Set the seed for reproducibility. </b></p>
        <p>np.random.seed(42) </p>
        <p><b>Change the seed if you want to </b></p>
        <p><b>experiment with different values. </b></p>
        <p>ma2 = np.array([1, 0.9, 0.3])</p>
        <p>ar2 = np.array([1, 0, 0])</p>
        <p>MA2_process = ArmaProcess(ar2, ma2).generate_sample(nsample=1000)</p>
        <p>2</p>
        <p>Plot your simulated moving average. </p>
        <p>3</p>
        <p>Run the ADF test, and check if the process is stationary. </p>
        <p>4</p>
        <p>Plot the ACF, and see if there are significant coefficients after lag 2. </p>
        <p>5</p>
        <p>Separate your simulated series into train and test sets. Take the first 800 timesteps for
            the train set, and assign the rest to the test set. </p>
        <p>6</p>
        <p>Make forecasts over the test set. Use the mean, last value, and an MA(2) model. </p>
        <p>Make sure you repeatedly forecast 2 timesteps at a time using the recursive_</p>
        <p>forecast function we defined. </p>
        <p>7</p>
        <p>Plot your forecasts. </p>
        <p>8</p>
        <p>Measure the MSE, and identify your champion model. </p>
        <p>9</p>
        <p>Plot your MSEs in a bar plot. </p>
        <p><a id="calibre_link-62"></a><b>80</b></p>
        <p>CHAPTER 4</p>
        <p> <i><b>Modeling a moving average process</b></i></p>
        <p> <i><b>4.4.2</b></i></p>
        <p> <i><b>Simulate an MA(q) process and make
                    forecasts</b></i></p>
        <p>Recreate the previous exercise, but simulate a moving average process of your choice. </p>
        <p>Try simulating a third-order or fourth-order moving average process. I recommend generating
            10,000 samples. Be especially attentive to the ACF, and see if your coefficients become non-significant
            after lag <i>q</i>. </p>
        <p> <i><b>Summary</b></i></p>
        <p> A moving average process states that the present value is linearly dependent on the mean,
            present error term, and past error terms. The error terms are normally distributed. </p>
        <p> You can identify the order <i>q</i> of a stationary moving average
            process by studying the ACF plot. The coefficients are significant up until lag <i>q</i>
            only. </p>
        <p> You can predict up to <i>q</i> steps into the future because the error
            terms are not observed in the data and must be recursively estimated. </p>
        <p> Predicting beyond <i>q</i> steps into the future will simply return the
            mean of the series. To avoid that, you can apply rolling forecasts. </p>
        <p> If you apply a transformation to the data, you must undo it to bring your predictions back
            to the original scale of the data. </p>
        <p> The moving average model assumes the data is stationary. Therefore, you can</p>
        <p>only use this model on stationary data. </p>
        <p><a id="calibre_link-7"></a> <i>Modeling an</i></p>
        <p> <i>autoregressive process</i></p>
        <p> <i><b>This chapter covers </b></i></p>
        <p> Illustrating an autoregressive process</p>
        <p> Defining the partial autocorrelation function (PACF)</p>
        <p> Using the PACF plot to determine the order of an </p>
        <p>autoregressive process</p>
        <p> Forecasting a time series using the </p>
        <p>autoregressive model</p>
        <p>In the previous chapter, we covered the moving average process, also denoted as</p>
        <p>MA( <i>q</i>), where <i>q</i> is the order. You learned
            that in a moving average process, the present value is linearly dependent on current and past error
            terms.
            Therefore, if you predict more than <i>q</i> steps ahead, the prediction will fall flat
            and
            will return only the mean of the series, because the error terms are not observed in the data and must
            be
            recursively estimated. Finally, you saw that you can determine the order of a stationary MA( <i
                class="calibre3">q</i>) process by studying the ACF plot; the autocorrelation coefficients will be
            significant up until lag <i>q</i>. In the case where the autocorrelation coefficients
            slowly decay or exhibit a sinusoidal pattern, then you are possibly in the presence of an autoregressive
            process. </p>
        <p>In this chapter, we will first define the autoregressive process. Then, we will define the
            partial autocorrelation function and use it to find the order of the <b>81</b></p>
        <p><a id="calibre_link-63"></a><img src="images/000005.jpg" alt="Image 45" class="calibre2" />
        </p>
        <p><b>82</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>underlying autoregressive process of a dataset. Finally, we will use the AR( <i>p</i>)
            model
            to produce forecasts. </p>
        <p> <i><b>5.1</b></i></p>
        <p> <i><b>Predicting the average weekly foot traffic in a
                    retail store</b></i></p>
        <p>Suppose that you want to forecast the average weekly foot traffic in a retail store so that
            the store manager can better manage the staff's schedule. If many people are expected to come to the
            store,
            more employees should be present to provide assistance. If fewer people are expected to visit the store,
            the
            manager can schedule fewer employees to work. That way the store can optimize its spending on salaries
            and
            ensure that employees are not overwhelmed or underwhelmed by store visitors. </p>
        <p>For this example, we have 1,000 data points, each representing the average weekly foot
            traffic at a retail store starting in the year 2000. You can see the evolution of our data through time
            in
            figure 5.1. </p>
        <p>Figure 5.1</p>
        <p>Average weekly foot traffic in a retail store. The dataset contains </p>
        <p>1,000 data points, starting in the first week of 2000. Note that this is fictional data.
        </p>
        <p>In figure 5.1 we can see a long-term trend with peaks and troughs along the way. We can
            intuitively say that this time series is not a stationary process, since we observe a trend over time.
            Furthermore, there is no apparent cyclical pattern in the data, so we can rule out any seasonal effects
            for
            now. </p>
        <p>Again, in order to forecast the average weekly foot traffic, we need to identify the
            underlying process. Thus, we must apply the same steps that we covered in chapter 4. </p>
        <p>That way, we can verify whether we have a random walk or a moving average process at play.
            The steps are shown in figure 5.2. </p>
        <p><a id="calibre_link-191"></a> <i><b>5.1</b></i></p>
        <p> <i><b>Predicting the average weekly foot traffic in a
                    retail store</b></i></p>
        <p><b>83</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( <i>q</i>) process. </p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Figure 5.2</p>
        <p>Steps to identify the underlying </p>
        <p>process of a stationary time series. So far </p>
        <p>Not a moving average</p>
        <p>we can identify a random walk or a moving </p>
        <p>process</p>
        <p>average process. </p>
        <p>In this example, the data is already collected, so we can move on to testing for
            stationarity. As mentioned previously, the presence of a trend over time means that our series is likely
            not
            stationary, so we will have to apply a transformation in order to make it stationary. Then we will plot
            the
            ACF. As we work through the chapter, you will see that not only is there autocorrelation, but the ACF
            plot
            will have a slowly decaying trend. </p>
        <p>This is indicative of an autoregressive process of order <i>p</i>, also
            denoted as AR( <i>p</i>). In this case, we must plot the <i>partial
                autocorrelation function</i> (PACF) to find the order <i>p</i>. </p>
        <p>Just like the coefficients on an ACF plot for an MA( <i>q</i>) process, the
            coefficients on the PACF plot will become abruptly non-significant after lag <i>p</i>,
            hence determining the order of the autoregressive process. </p>
        <p>Again, the order of the autoregressive process determines how many parameters</p>
        <p>must be included in the AR( <i>p</i>) model. Then we will be ready to make
            forecasts. In this example, we wish to forecast next week's average foot traffic. </p>
        <p><a id="calibre_link-64"></a><b>84</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p> <i><b>5.2</b></i></p>
        <p> <i><b>Defining the autoregressive process</b></i></p>
        <p>An autoregressive process establishes that the output variable depends linearly on its own
            previous values. In other words, it is a regression of the variable against itself. </p>
        <p>An <i>autoregressive process</i> is denoted as an AR( <i>p</i>)
            process,
            where <i>p</i> is the order. In such a process, the
            present value <i>yt</i> is a linear combination of a constant C, the present error term
            ϵ
            <i>t</i>, which is also white noise, and the past values of the series <i class="calibre3">yt&ndash;p</i>.
            The magnitude of the influence of the past values on the present
            value
            is denoted as φ <i>p</i>, which represents the coefficients of the AR( <i class="calibre3">p</i>) model.
            Mathematically, we express a general AR( <i>p</i>)
            model
            with equation 5.1. </p>
        <p> <i> yt</i> = C + φ1 <i>y<sub>t-1</sub></i> + φ2 <i class="calibre3">yt</i>&ndash;2 +⋅⋅⋅ φ <i>pyt&ndash;p</i>
            +
            ϵt Equation 5.1</p>
        <p>Autoregressive process</p>
        <p>An autoregressive process is a regression of a variable against itself. In a time series,
            this means that the present value is linearly dependent on its past values. </p>
        <p>The autoregressive process is denoted as AR( <i>p</i>), where <i>p</i>
            is
            the order. The general expression of an AR( <i>p</i>) model
            is</p>
        <p> <i>yt</i> = C + φ1 <i>yt</i>&ndash;1 + φ2 <i class="calibre3">yt</i>&ndash;2 +⋅⋅⋅+ φ <i>pyt&ndash;p</i> + ϵ
            <i class="calibre3">t</i></p>
        <p>Similar to the moving average process, the order <i>p</i> of an
            autoregressive process determines the number of past values that affect the present value. If we have a
            first-order autoregressive process, also denoted as AR(1), then the present value <i>yt</i>
            is only dependent on a constant C, the value at the previous timestep φ1 <i>yt
            </i>&ndash;1, and some white noise ϵ <i>t</i>, as shown in equation 5.2. </p>
        <p> <i> yt</i> = C + φ1 <i>y<sub>t-1</sub></i> + ϵ <i>t</i>
        </p>
        <p>Equation 5.2</p>
        <p>Looking at equation 5.2, you might notice that it is very similar to a random walk process,
            which we covered in chapter 3. In fact, if φ1 = 1, then equation 5.2 becomes <i>yt</i>
            = C
            + <i>y<sub>t-1</sub></i> + ϵ <i>t</i></p>
        <p>which is our random walk model. Therefore, we can say that the random walk is a special case
            of an autoregressive process, where the order <i>p</i> is 1 and φ1 is equal to 1. </p>
        <p>Notice also that if C is not equal to 0, then we have a random walk with drift. </p>
        <p>In the case of a second-order autoregressive process, or AR(2), the present value <i>yt</i>
            is linearly dependent on a constant C, the value at the previous timestep φ1 <i>yt
            </i>&ndash;1, the value two timesteps prior φ2 <i>yt </i>&ndash;2,
            and the present error term ϵ <i>t</i>, as shown in equation 5.3. </p>
        <p> <i>yt</i> = C + φ1 <i>y<sub>t-1</sub></i> + φ2 <i>yt
            </i>&ndash;2 + ϵ <i>t</i></p>
        <p>Equation 5.3</p>
        <p>We see how the order <i>p</i> influences the number of parameters that must
            be included in our model. As with a moving average process, we must find the right order of an
            autoregressive process in order to build the appropriate model. This means that if</p>
        <p><a id="calibre_link-65"></a> <i><b>5.3</b></i></p>
        <p> <i><b>Finding the order of a stationary autoregressive
                    process</b></i></p>
        <p><b>85</b></p>
        <p>we identify an AR(3) process, we will use a third-order autoregressive model to make
            forecasts. </p>
        <p> <i><b>5.3</b></i></p>
        <p> <i><b>Finding the order of a stationary autoregressive
                    process</b></i></p>
        <p>Just like with the moving average process, there is a way to determine the order <i>p</i> of
            a stationary autoregressive process. We can extend the steps needed to
            identify the order of a moving average, as shown in figure 5.3. </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( ) process. </p>
        <p> <i>q</i></p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Plot PACF</p>
        <p>Do coefficients become</p>
        <p>abruptly non-significant</p>
        <p>Yes</p>
        <p>It is an AR( <i>p</i>) process. </p>
        <p>after lag <i>p</i>? </p>
        <p>No</p>
        <p>Figure 5.3</p>
        <p>Steps to identify the order </p>
        <p>Not an AR( <i>p</i>) process</p>
        <p>of an autoregressive process</p>
        <p><a id="calibre_link-356"></a><b>86</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>The natural first step is to collect the data. Here we will work with the average weekly
            foot traffic dataset that you saw at the beginning of the chapter. We will read the data using pandas
            and
            store it as a DataFrame. </p>
        <p>NOTE</p>
        <p>Feel free to consult the source code for this chapter on GitHub at any </p>
        <p>time: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH05">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/
            </a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH05">master/CH05</a>.
        </p>
        <p>import pandas as pd</p>
        <p><b>Read the CSV file into </b></p>
        <p><b>a DataFrame. </b></p>
        <p>df = pd.read_csv('../data/foot_traffic.csv') </p>
        <p><b>Display the first five </b></p>
        <p>df.head() </p>
        <p><b>rows of data. </b></p>
        <p>You'll see that our data contains a single foot_traffic column in which the average weekly
            foot traffic at the retail store is recorded. </p>
        <p>As always, we will plot our data to see if there are any observable patterns, such as a
            trend or seasonality. By now, you should be comfortable with plotting time series, so we will not dive
            deeply into the code that generates the graph. The result is the plot shown in figure 5.4. </p>
        <p>import matplotlib.pyplot as plt</p>
        <p><b>Plot the average weekly foot </b></p>
        <p><b>traffic at a retail store. </b></p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>Label the x-axis. </b></p>
        <p>ax.plot(df['foot_traffic']) </p>
        <p>ax.set_xlabel('Time') </p>
        <p><b>Label the y-axis. </b></p>
        <p>ax.set_ylabel('Average weekly foot traffic') </p>
        <p><b>Label the ticks </b></p>
        <p>plt.xticks(np.arange(0, 1000, 104), np.arange(2000, 2020, 2)) </p>
        <p><b>on the x-axis. </b></p>
        <p><b>Tilt the labels on the x-axis ticks </b></p>
        <p>fig.autofmt_xdate() </p>
        <p><b>so that they display nicely. </b></p>
        <p>plt.tight_layout() </p>
        <p><b>Remove extra whitespace around the figure. </b></p>
        <p>Looking at figure 5.4, you'll notice that there is no cyclical pattern, so we can rule out
            the presence of seasonality. As for the trend, it is sometimes positive and sometimes negative
            throughout
            the years, with the most recent trend being positive, since 2016. </p>
        <p>The next step is to check for stationarity. As mentioned before, the presence of a trend
            means that our series is likely non-stationary. Let's verify that using the ADF</p>
        <p>test. Again, you should be comfortable running this without a detailed explanation of the
            code. </p>
        <p><b>Run the ADF test on the average </b></p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p><b>weekly foot traffic, which is stored </b></p>
        <p><b>in the foot_traffic column. </b></p>
        <p>ADF_result = adfuller(df['foot_traffic']) </p>
        <p><b>Print the</b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}') </p>
        <p><b>Print the ADF statistic. </b></p>
        <p><b> p-value. </b></p>
        <p>print(f'p-value: {ADF_result[1]}') </p>
        <p><a id="calibre_link-357"></a><img src="images/000018.jpg" alt="Image 46" class="calibre2" />
        </p>
        <p> <i><b>5.3</b></i></p>
        <p> <i><b>Finding the order of a stationary autoregressive
                    process</b></i></p>
        <p><b>87</b></p>
        <p>Figure 5.4</p>
        <p>Average weekly foot traffic in a retail store. The dataset contains 1,000 </p>
        <p>data points, starting in the first week of 2000. </p>
        <p>This prints out an ADF statistic of &ndash;1.18 along with a p-value of 0.68. Since the ADF
            statistic is not a large negative number, and it has a p-value greater than 0.05, we cannot reject the
            null
            hypothesis and our series is therefore non-stationary. </p>
        <p>Hence, we must apply a transformation to make it stationary. To remove the effect of the
            trend and stabilize the mean of the series, we will use differencing. </p>
        <p><b>Apply a first-order </b></p>
        <p>import numpy as np</p>
        <p><b>differencing on the data </b></p>
        <p>foot_traffic_diff = np.diff(df['foot_traffic'], n=1) </p>
        <p><b>and store the result in </b></p>
        <p><b>foot_traffic_diff. </b></p>
        <p>Optionally, we could plot our differenced series foot_traffic_diff to see if we successfully
            removed the effect of the trend. The differenced series is shown in figure 5.5. </p>
        <p>We can see that we indeed removed the long-term trend, since the series starts and finishes
            roughly at the same value. </p>
        <p>Can you recreate figure 5.5? </p>
        <p>While optional, it is a good idea to plot your series as you apply transformations. This
            will give you a better intuition as to whether the series is stationary or not after a particular
            transformation. Try recreating figure 5.5 on your own. </p>
        <p></p>
        <p><a id="calibre_link-280"></a><img src="images/000163.jpg" alt="Image 47" class="calibre2" />
        </p>
        <p><b>88</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>Figure 5.5</p>
        <p>Differenced average weekly foot traffic at a retail store. Notice that the trend effect has
            been removed, since the series starts and ends at roughly the same value. </p>
        <p>With a transformation applied to the series, we can verify whether the series is stationary
            by running the ADF test on the differenced series. </p>
        <p>ADF_result = adfuller(foot_traffic_diff) </p>
        <p><b>Run the ADF test on the </b></p>
        <p><b>differenced time series. </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p>This prints out an ADF statistic of &ndash;5.27 and a p-value of 6.36×10&ndash;6. With a
            p-value smaller than 0.05, we can reject the null hypothesis, meaning that we now have a stationary
            series.
        </p>
        <p>The next step is to plot the ACF and see if there is autocorrelation and if the coefficients
            become abruptly non-significant after a certain lag. As we did in the two previous chapters, we will use
            the
            plot_acf function from statsmodels. The result is shown in figure 5.6. </p>
        <p>from statsmodels.graphics.tsaplots import plot_acf</p>
        <p>plot_acf(foot_traffic_diff, lags=20); </p>
        <p><b>Plot the ACF of the </b></p>
        <p><b>differenced series. </b></p>
        <p>plt.tight_layout()</p>
        <p>Looking at figure 5.6, you'll notice that we have significant autocorrelation coefficients
            beyond lag 0. Therefore, we know that our process is not a random walk. Furthermore, you'll notice that
            the
            coefficients are decaying exponentially as the lag</p>
        <p><a id="calibre_link-66"></a><img src="images/000129.jpg" alt="Image 48" class="calibre2" />
        </p>
        <p> <i><b>5.3</b></i></p>
        <p> <i><b>Finding the order of a stationary autoregressive
                    process</b></i></p>
        <p><b>89</b></p>
        <p>Figure 5.6</p>
        <p>ACF plot of the differenced average weekly foot traffic at a retail store. </p>
        <p>Notice how the plot is slowly decaying. This is a behavior that we have not observed before,
            and it is indicative of an autoregressive process. </p>
        <p>increases. Therefore, there is no lag at which the coefficients abruptly become
            non-significant. This means that we do not have a moving average process and that we are likely studying
            an
            autoregressive process. </p>
        <p>When the ACF plot of a stationary process exhibits a pattern of exponential decay, we
            probably have an autoregressive process in play, and we must find another way to identify the order <i
                class="calibre3">p</i> of the AR( <i>p</i>) process. Specifically, we must turn our
            attention to the <i>partial autocorrelation function </i>(PACF) plot. </p>
        <p> <i><b>5.3.1</b></i></p>
        <p> <i><b>The partial autocorrelation function (PACF)</b></i>
        </p>
        <p>In an attempt to identify the order of a stationary autoregressive process, we used the ACF
            plot just as we did for a moving average process. Unfortunately, the ACF</p>
        <p>plot cannot give us this information, and we must turn to the <i>partial
                autocorrelation</i> <i>function</i> (PACF). </p>
        <p>Remember that the autocorrelation measures the linear relationship between lagged values of
            a time series. Consequently, the autocorrelation function measures how the correlation changes between
            two
            values as the lag is increased. </p>
        <p>To understand the partial autocorrelation function, let's consider the following scenario.
            Suppose we have the following AR(2) process:</p>
        <p> <i> yt</i> = 0.33 <i>y<sub>t-1</sub></i> + 0.50 <i>yt
            </i>&ndash;2</p>
        <p>Equation 5.4</p>
        <p>We wish to measure how <i>yt</i> relates to <i>yt
            </i>&ndash;2; in other words, we want to measure their correlation. This is done with the
            autocorrelation
            function (ACF). However, from the</p>
        <p><a id="calibre_link-199"></a><b>90</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>equation, we can see that <i>y<sub>t-1</sub></i> also has an influence on <i class="calibre3">yt</i>. Even
            more
            important, it also has an impact on the value of <i class="calibre3">yt </i>&ndash;2, since in an AR(2)
            process, each value depends on the previous two
            values. Therefore, when we measure the autocorrelation between <i>yt</i> and <i class="calibre3">yt
            </i>&ndash;2 using the ACF, we are not taking into account the fact that <i class="calibre3">yt </i>&ndash;1
            has an influence on both <i>yt</i> and <i class="calibre3">yt </i>&ndash;2. This means that we are not
            measuring the <i>true
            </i>
            impact of <i>yt </i>&ndash;2</p>
        <p>on <i>yt</i>. To do so, we must remove the effect of <i>yt
            </i>&ndash;1. Thus, we are measuring the partial autocorrelation between <i>yt</i> and
            <i>yt </i>&ndash;2. </p>
        <p>In more formal terms, the partial autocorrelation measures the correlation between lagged
            values in a time series when we remove the influence of correlated lagged values in between. Those are
            known
            as <i>confounding variables</i>. The partial autocorrelation function will reveal how
            the
            partial autocorrelation varies when the lag increases. </p>
        <p>Partial autocorrelation</p>
        <p>Partial autocorrelation measures the correlation between lagged values in a time series when
            we remove the influence of correlated lagged values in between. We can plot the partial autocorrelation
            function to determine the order of a stationary AR( <i>p</i>) process. The coefficients
            will be non-significant after lag <i>p</i>. </p>
        <p>Let's verify whether plotting the PACF will reveal the order of the process shown in
            equation 5.4. We know from equation 5.4 that we have a second-order autoregressive process, or AR(2). We
            will simulate it using the ArmaProcess function from statsmodels. The function expects an array
            containing
            the coefficients of an MA( <i>q</i>) process and an array containing the coefficients
            for
            an AR( <i>p</i>) process. Since we are only interested in simulating an AR(2) process,
            we
            will set the coefficients of the MA( <i>q</i>) process to 0. Then, as specified by the
            statsmodels documentation, the coefficients of the AR(2) process must have opposite signs to those we
            wish
            to simulate. Therefore, the array will contain &ndash;0.33</p>
        <p>and &ndash;0.50. In addition, the function requires us to include the coefficient at lag 0,
            which is the number that multiplies <i>yt</i>. Here, that number is simply 1. </p>
        <p>Once the arrays of coefficients are defined, we can feed them to the ArmaProcess function,
            and we will generate 1,000 samples. Make sure you set the random seed to 42 in order to reproduce the
            results shown here. </p>
        <p><b>Set the coefficients of the </b></p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p><b>MA(q) process to 0, since </b></p>
        <p>import numpy as np</p>
        <p><b>Set the random seed to 42 in order to </b></p>
        <p><b>we are only interested in </b></p>
        <p><b>reproduce the results shown here. </b></p>
        <p><b>simulating an AR(2) process. </b></p>
        <p>np.random.seed(42) </p>
        <p><b>Note that the first coefficient </b></p>
        <p><b>is 1 for lag 0, and it must be </b></p>
        <p>ma2 = np.array([1, 0, 0]) </p>
        <p><b>provided as specified by the </b></p>
        <p>ar2 = np.array([1, -0.33, -0.50]) </p>
        <p><b>documentation. </b></p>
        <p>AR2_process = ArmaProcess(ar2, ma2).generate_sample(nsample=1000) </p>
        <p><b>Set the coefficients for the AR(2) process. Again, the coefficient at
            </b></p>
        <p><b>Simulate the AR(2)</b></p>
        <p><b>lag 0 is 1. Then, write the coefficients with opposite signs to what
            </b></p>
        <p><b>process and generate</b></p>
        <p><b>was defined in equation 5.4, as specified by the documentation. </b></p>
        <p><b>1,000 samples. </b></p>
        <p><a id="calibre_link-286"></a><img src="images/000052.jpg" alt="Image 49" class="calibre2" />
        </p>
        <p> <i><b>5.3</b></i></p>
        <p> <i><b>Finding the order of a stationary autoregressive
                    process</b></i></p>
        <p><b>91</b></p>
        <p>Now that we have a simulated AR(2) process, let's plot the PACF and see if the coefficients
            become abruptly non-significant after lag 2. If that is the case, we'll know that we can use the PACF
            plot
            to determine the order of a stationary autoregressive process, just as we can use the ACF plot to
            determine
            the order of a stationary moving average process. </p>
        <p>The statsmodels library allows us to plot the PACF rapidly. We can use the plot_pacf
            function, which simply requires our series and the number of lags to display on the plot. </p>
        <p>from statsmodels.graphics.tsaplots import plot_pacf</p>
        <p>plot_pacf(AR2_process, lags=20); </p>
        <p><b>Plot the PACF of our </b></p>
        <p><b>simulated AR(2) process. </b></p>
        <p>plt.tight_layout()</p>
        <p>The resulting plot is shown in figure 5.7, and it shows that we have an autoregressive
            process of order 2. </p>
        <p>Figure 5.7</p>
        <p>Plot of the PACF for our simulated AR(2) process. You can clearly </p>
        <p>see here that after lag 2, the partial autocorrelation coefficients are not </p>
        <p>significantly different from 0. Therefore, we can identify the order of a stationary AR( <i
                class="calibre3"><b>p</b></i>) model using the PACF plot. </p>
        <p>We now know that we can use the PACF plot to identify the order of a stationary AR( <i
                class="calibre3">p</i>) process. The coefficients in the PACF plot will be significant up until lag
            <i>p</i>. Afterward, they should not be significantly different from 0. </p>
        <p>Let's see if we can apply the same strategy to our average weekly foot traffic dataset. We
            made the series stationary and saw that the ACF plot exhibited a slowly</p>
        <p><a id="calibre_link-67"></a><img src="images/000107.jpg" alt="Image 50" class="calibre2" />
        </p>
        <p><b>92</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>decaying trend. Let's plot the PACF to see if the lags become non-significant after a
            particular lag. </p>
        <p>The process is exactly the same as what we just did, but this time we will plot the PACF of
            our differenced series stored in foot_traffic_diff. You can see the resulting plot in figure 5.8. </p>
        <p>plot_pacf(foot_traffic_diff, lags=20); </p>
        <p><b>Plot the PACF of our </b></p>
        <p><b>differenced series. </b></p>
        <p>plt.tight_layout()</p>
        <p>Figure 5.8</p>
        <p>The PACF of our differenced average weekly foot traffic in a retail </p>
        <p>store. You can see that the coefficients are non-significant after lag 3. </p>
        <p>Therefore, we can say that our stationary process is a third-order </p>
        <p>autoregressive process, or an AR(3) process. </p>
        <p>Looking at figure 5.8, you can see that there are no significant coefficients after lag 3.
        </p>
        <p>Therefore, the differenced average weekly foot traffic is an autoregressive process of order
            3, which can also be denoted as AR(3). </p>
        <p> <i><b>5.4</b></i></p>
        <p> <i><b>Forecasting an autoregressive process</b></i></p>
        <p>Once the order is determined, we can fit an autoregressive model to forecast our time
            series. In this case, the model is also termed AR( <i>p</i>), where <i class="calibre3">p</i> is still the
            order of the process. </p>
        <p>We will forecast next week's average foot traffic in a retail store using the same dataset
            we have been working with. In order to evaluate our forecasts, we will hold out the last 52 weeks of
            data
            for our test set, while the rest will be used for training. That way, we can evaluate the performance of
            our
            forecast over a period of 1 year. </p>
        <p><a id="calibre_link-268"></a> <i><b>5.4</b></i></p>
        <p> <i><b>Forecasting an autoregressive process</b></i></p>
        <p><b>93</b></p>
        <p>df_diff = pd.DataFrame({'foot_traffic_diff': foot_traffic_diff}) </p>
        <p><b>Create a DataFrame</b></p>
        <p>train = df_diff[:-52] </p>
        <p><b>The training set is all the data </b></p>
        <p><b>from the differenced</b></p>
        <p>test = df_diff[-52:] </p>
        <p><b>except the last 52 data points. </b></p>
        <p><b>foot traffic data. </b></p>
        <p>print(len(train)) </p>
        <p><b>The test set is the </b></p>
        <p>print(len(test)) </p>
        <p><b>last 52 data points. </b></p>
        <p><b>Display how many data </b></p>
        <p><b>Display how many data </b></p>
        <p><b>points are in the test set. </b></p>
        <p><b>points are in the train set. </b></p>
        <p>You can see that our training set contains 947 data points, while the test set contains 52
            data points as expected. Note that the sum of both sets gives 999, which is one less data point than our
            original series. This is normal, since we applied differencing to make the series stationary, and we
            know
            that differencing removes the first data point from the series. </p>
        <p>Next, we will visualize the testing period for our scenario, in both the original series and
            the differenced series. The plot is shown in figure 5.9. </p>
        <p>fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, </p>
        <p>➥ figsize=(10, 8)) </p>
        <p><b>Specify the figure's size using the figsize </b></p>
        <p><b>parameter. The first number is the height, and </b></p>
        <p>ax1.plot(df['foot_traffic'])</p>
        <p><b>the second number is the width, both in inches. </b></p>
        <p>ax1.set_xlabel('Time')</p>
        <p>ax1.set_ylabel('Avg. weekly foot traffic')</p>
        <p>ax1.axvspan(948, 1000, color='#808080', alpha=0.2)</p>
        <p>ax2.plot(df_diff['foot_traffic_diff'])</p>
        <p>ax2.set_xlabel('Time')</p>
        <p>ax2.set_ylabel('Diff. avg. weekly foot traffic')</p>
        <p>ax2.axvspan(947, 999, color='#808080', alpha=0.2)</p>
        <p>plt.xticks(np.arange(0, 1000, 104), np.arange(2000, 2020, 2))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Given that our objective is to forecast next week's average foot traffic at the retail
            store, we will perform rolling forecasts over our test set. Remember that our data was recorded over a
            weekly period, so predicting the next timestep means we're forecasting next week's average foot traffic.
        </p>
        <p>We will forecast using three different methods. The historical mean method and</p>
        <p>the last known value method will act as baselines, and we will use an AR(3) model, since we
            previously established that we have a stationary third-order autoregressive process. As we did in the
            previous chapter, we will use the mean squared error (MSE) to evaluate the performance of each
            forecasting
            method. </p>
        <p>Also, we will reuse the function we defined in the previous chapter to recursively forecast
            over the testing period. However, this time we must include a method to use an autoregressive model.
        </p>
        <p><a id="calibre_link-358"></a><img src="images/000146.jpg" alt="Image 51" class="calibre2" />
        </p>
        <p><b>94</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>Figure 5.9</p>
        <p>Testing period for our forecasts on the original and differenced series. Keep in mind that
            our differenced series has lost its first data point. </p>
        <p>We will again use the SARIMAX function from statsmodels, as it encompasses an AR</p>
        <p>model. As mentioned previously, SARIMAX is a complex model that allows us to consider
            seasonal effects, autoregressive processes, non-stationary time series, moving average processes, and
            exogenous variables all in one single model. For now, we will disregard all factors except the moving
            autoregressive portion. </p>
        <p>Listing 5.1</p>
        <p>A function for rolling forecasts on a horizon</p>
        <p>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, </p>
        <p>➥ window: int, method: str) -&gt; list:</p>
        <p></p>
        <p>total_len = train_len + horizon</p>
        <p>end_idx = train_len</p>
        <p></p>
        <p><a id="calibre_link-359"></a> <i><b>5.4</b></i></p>
        <p> <i><b>Forecasting an autoregressive process</b></i></p>
        <p><b>95</b></p>
        <p>if method == 'mean':</p>
        <p>pred_mean = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>mean = np.mean(df[:i].values)</p>
        <p>pred_mean.extend(mean for _ in range(window))</p>
        <p></p>
        <p>return pred_mean</p>
        <p>elif method == 'last':</p>
        <p>pred_last_value = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>last_value = df[:i].iloc[-1].values[0]</p>
        <p>pred_last_value.extend(last_value for _ in range(window))</p>
        <p></p>
        <p>return pred_last_value</p>
        <p></p>
        <p>elif method == 'AR':</p>
        <p>pred_AR = []</p>
        <p></p>
        <p><b>The order specifies </b></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p><b>an AR(3) model. </b></p>
        <p>model = SARIMAX(df[:i], order=(3,0,0)) </p>
        <p>res = model.fit(disp=False)</p>
        <p>predictions = res.get_prediction(0, i + window - 1)</p>
        <p>oos_pred = predictions.predicted_mean.iloc[-window:]</p>
        <p>pred_AR.extend(oos_pred)</p>
        <p></p>
        <p>return pred_AR</p>
        <p>Once our function is defined, we can use it to generate the predictions according to each
            method. We will assign them to their own column in test. </p>
        <p><b>Since we wish to predict the </b></p>
        <p><b>Store the length of the training set. </b></p>
        <p><b>next timestep, our window is 1. </b></p>
        <p><b>Note that constants are usually in </b></p>
        <p><b>capital letters in Python. </b></p>
        <p>TRAIN_LEN = len(train) </p>
        <p>HORIZON = len(test) </p>
        <p><b>Store the length </b></p>
        <p>WINDOW = 1 </p>
        <p><b>of the test set. </b></p>
        <p>pred_mean = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'mean')</p>
        <p>pred_last_value = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'last') </p>
        <p>pred_AR = rolling_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'AR')</p>
        <p>test['pred_mean'] = pred_mean </p>
        <p><b>Store the predictions </b></p>
        <p>test['pred_last_value'] = pred_last_value </p>
        <p><b>in their respective </b></p>
        <p>test['pred_AR'] = pred_AR </p>
        <p><b>columns in test. </b></p>
        <p>test.head()</p>
        <p>We can now visualize our predictions against the observed values in the test set. Note that
            we are working with the differenced series, so our predictions are also differenced values. The result
            is
            shown in figure 5.10. </p>
        <p><a id="calibre_link-267"></a><img src="images/000118.jpg" alt="Image 52" class="calibre2" />
        </p>
        <p><b>96</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>Plot part of the training set so we can see the </b></p>
        <p><b>transition from the training set to the test set. </b></p>
        <p>ax.plot(df_diff['foot_traffic_diff']) </p>
        <p>ax.plot(test['foot_traffic_diff'], 'b-', label='actual') </p>
        <p><b>Plot the values </b></p>
        <p>ax.plot(test['pred_mean'], 'g:', label='mean') </p>
        <p><b>from the test set. </b></p>
        <p>ax.plot(test['pred_last_value'], 'r-.', label='last') </p>
        <p>ax.plot(test['pred_AR'], 'k--', label='AR(3)') </p>
        <p><b>Plot the predictions </b></p>
        <p>ax.legend(loc=2)</p>
        <p><b>from the historical </b></p>
        <p><b>mean method. </b></p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Diff. avg. weekly foot traffic')</p>
        <p><b>Plot the predictions </b></p>
        <p><b>from the last known </b></p>
        <p>ax.axvspan(947, 998, color='#808080', alpha=0.2)</p>
        <p><b>value method. </b></p>
        <p>ax.set_xlim(920, 999)</p>
        <p><b>Plot the </b></p>
        <p><b>predictions from </b></p>
        <p><b>the AR(3) model. </b></p>
        <p>plt.xticks([936, 988],[2018, 2019])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 5.10</p>
        <p>Forecasts of the differenced average weekly foot traffic in a retail store</p>
        <p>Looking at figure 5.10, you'll see that, once again, using the historical mean produces a
            straight line, which is shown in the plot as a dotted line. As for the predictions from the AR(3) model
            and
            the last known value method, the curves are almost confounding with that of the test set, so we will
            have to
            measure the MSE to assess which method is the most performant. Again, we will use the mean_squared_error
            function from the sklearn library. </p>
        <p><a id="calibre_link-360"></a> <i><b>5.4</b></i></p>
        <p> <i><b>Forecasting an autoregressive process</b></i></p>
        <p><b>97</b></p>
        <p>ffrom sklearn.metrics import mean_squared_error</p>
        <p>mse_mean = mean_squared_error(test['foot_traffic_diff'], test['pred_mean'])</p>
        <p>mse_last = mean_squared_error(test['foot_traffic_diff'], </p>
        <p>➥ test['pred_last_value'])</p>
        <p>mse_AR = mean_squared_error(test['foot_traffic_diff'], test['pred_AR'])</p>
        <p>print(mse_mean, mse_last, mse_AR)</p>
        <p>This prints out an MSE of 3.11 for the historical mean method, 1.45 for the last known value
            method, and 0.92 for the AR(3) model. Since the MSE for the AR(3) model is the lowest of the three, we
            conclude that the AR(3) model is the best-performing method for forecasting next week's average foot
            traffic. This is expected, since we established that our stationary process was a third-order
            autoregressive
            process. It makes sense that modeling using an AR(3) model will yield the best predictions. </p>
        <p>Since our forecasts are differenced values, we need to reverse the transformation in order
            to bring our forecasts back to the original scale of the data; otherwise, our predictions will not make
            sense in a business context. To do this, we can take the cumulative sum of our predictions and add it to
            the
            last value of our training set in the original series. This point occurs at index 948, since we are
            forecasting the last 52</p>
        <p>weeks in a dataset containing 1,000 points. </p>
        <p>df['pred_foot_traffic'] = pd.Series()</p>
        <p>df['pred_foot_traffic'][948:] = df['foot_traffic'].iloc[948] + </p>
        <p>➥ pred_df['pred_AR'].cumsum() </p>
        <p><b>Assign the undifferenced </b></p>
        <p><b>predictions to the pred_foot_traffic </b></p>
        <p><b>column in df. </b></p>
        <p>Now we can plot our undifferenced predictions against the observed values in the test set of
            the original series in its original scale. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['foot_traffic'])</p>
        <p><b>Plot the actual values. </b></p>
        <p>ax.plot(df['foot_traffic'], 'b-', label='actual') </p>
        <p>ax.plot(df['pred_foot_traffic'], 'k--', label='AR(3)') </p>
        <p><b>Plot the </b></p>
        <p><b>undifferenced </b></p>
        <p>ax.legend(loc=2)</p>
        <p><b>predictions. </b></p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Average weekly foot traffic')</p>
        <p>ax.axvspan(948, 1000, color='#808080', alpha=0.2)</p>
        <p>ax.set_xlim(920, 1000)</p>
        <p>ax.set_ylim(650, 770)</p>
        <p>plt.xticks([936, 988],[2018, 2019])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p><a id="calibre_link-68"></a><img src="images/000119.jpg" alt="Image 53" class="calibre2" />
        </p>
        <p><b>98</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p>In figure 5.11 you can see that our model (shown as a dashed line) follows the general trend
            of the observed values in the test set. </p>
        <p>Figure 5.11</p>
        <p>Undifferenced forecasts from the AR(3) model</p>
        <p>Now we can measure the mean absolute error (MAE) on the original dataset to get its meaning
            in a business context. We'll simply measure the MAE using the undifferenced predictions. </p>
        <p>from sklearn.metrics import mean_absolute_error</p>
        <p>mae_AR_undiff = mean_absolute_error(df['foot_traffic'][948:], </p>
        <p>➥ df['pred_foot_traffic'][948:])</p>
        <p>print(mae_AR_undiff)</p>
        <p>This prints out a mean absolute error of 3.45. This means that our predictions are off by
            3.45 people on average, either above or below the actual value for the week's foot traffic. Note that we
            report the MAE because it has a simple business meaning that is easy to understand and interpret. </p>
        <p> <i><b>5.5</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the autoregressive process and how it can be modeled by an AR(
            <i>p</i>) model, where <i>p</i> is the order, and it determines how
            many
            lagged values are included in the model. We also saw how plotting the ACF cannot help us determine the
            order
            of a stationary AR( <i>p</i>) process. Instead, we must plot the PACF, in which the
            partial
            autocorrelation coefficients will be significant up until lag <i>p</i> only. </p>
        <p><a id="calibre_link-69"></a> <i><b>5.6</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p><b>99</b></p>
        <p>However, there might be a situation where neither the ACF nor PACF gives us</p>
        <p>information. What if both the ACF and PACF plots exhibit a slow decay or a sinusoidal
            pattern? In that case, there is no order for the MA( <i>q</i>) or AR( <i class="calibre3">p</i>) process
            that can be inferred. This means that we are facing a more complex
            process that is likely a combination of both an AR( <i>p</i>) process and an MA( <i class="calibre3">q</i>)
            process. This is called an <i>autoregressive moving average
            </i>(ARMA) process, or ARMA( <i>p</i>, <i>q</i>), and it will the
            subject
            of the next chapter. </p>
        <p> <i><b>5.6</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Test your knowledge and mastery of the AR( <i>p</i>) model with these
            exercises. The solutions to all exercises are available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH05">https://github.com/marcopeix/Time</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH05">SeriesForecastingInPython/tree/master/CH05</a>.
        </p>
        <p> <i><b>5.6.1</b></i></p>
        <p> <i><b>Simulate an AR(2) process and make
                    forecasts</b></i></p>
        <p>Simulate a stationary AR(2) process. Use the ArmaProcess function from the statsmodels
            library and simulate this process:</p>
        <p> <i>yt</i> = 0.33 <i>y<sub>t-1</sub></i> + 0.50 <i>yt
            </i>&ndash;2</p>
        <p>1</p>
        <p>For this exercise, generate 1,000 samples. </p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p>import numpy as np</p>
        <p><b>Set the seed for reproducibility. </b></p>
        <p>np.random.seed(42) </p>
        <p><b>Change the seed if you want to </b></p>
        <p><b>experiment with different values. </b></p>
        <p>ma2 = np.array([1, 0, 0])</p>
        <p>ar2 = np.array([1, -0.33, -0.50])</p>
        <p>AR2_process = ArmaProcess(ar2, ma2).generate_sample(nsample=1000)</p>
        <p>2</p>
        <p>Plot your simulated autoregressive process. </p>
        <p>3</p>
        <p>Run the ADF test and check if the process is stationary. If not, apply differencing. </p>
        <p>4</p>
        <p>Plot the ACF. Is it slowly decaying? </p>
        <p>5</p>
        <p>Plot the PACF. Are there significant coefficients after lag 2? </p>
        <p>6</p>
        <p>Separate your simulated series into train and test sets. Take the first 800 timesteps for
            the train set and assign the rest to the test set. </p>
        <p>7</p>
        <p>Make forecasts over the test set. Use the historical mean method, last known value method,
            and an AR(2) model. Use the rolling_forecast function, and</p>
        <p>use a window length of 2. </p>
        <p>8</p>
        <p>Plot your forecasts. </p>
        <p>9</p>
        <p>Measure the MSE, and identify your champion model. </p>
        <p>10</p>
        <p>Plot your MSEs in a bar plot. </p>
        <p><a id="calibre_link-70"></a><b>100</b></p>
        <p>CHAPTER 5</p>
        <p> <i><b>Modeling an autoregressive process</b></i></p>
        <p> <i><b>5.6.2</b></i></p>
        <p> <i><b>Simulate an AR(p) process and make
                    forecasts</b></i></p>
        <p>Recreate the previous exercise but simulate an AR( <i>p</i>) process of
            your choice. Experiment with a third- or fourth-order autoregressive process. I would recommend
            generating
            10,000 samples. </p>
        <p>When forecasting, experiment with different values for the window parameter of</p>
        <p>your rolling_forecast function. How does it affect the model's performance? Is there a value
            that minimizes the MSE? </p>
        <p> <i><b>Summary</b></i></p>
        <p> An autoregressive process states that the present value is linearly dependent on its past
            values and an error term. </p>
        <p> If the ACF plot of a stationary process shows a slow decay, then you likely have an
            autoregressive process. </p>
        <p> The partial autocorrelation measures the correlation between two lagged val-</p>
        <p>ues of a time series when you remove the effect of the other autocorrelated lagged values.
        </p>
        <p> Plotting the PACF of a stationary autoregressive process will show the order <i>p</i>
            of
            the process. The coefficients will be significant up until lag <i>p</i> only. </p>
        <p><a id="calibre_link-8"></a> <i>Modeling</i></p>
        <p> <i>complex time series</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the autoregressive moving average </p>
        <p>model or ARMA( <i>p</i>, <i>q</i>)</p>
        <p> Experimenting with the limitations of the ACF </p>
        <p>and PACF plots</p>
        <p> Selecting the best model with the Akaike </p>
        <p>information criterion (AIC)</p>
        <p> Analyzing a time series model using residual </p>
        <p>analysis</p>
        <p> Building a general modeling procedure</p>
        <p> Forecasting using the ARMA( <i>p</i>, <i>q</i>) model
        </p>
        <p>In chapter 4 we covered the moving average process, denoted as MA( <i>q</i>), where <i class="calibre3">q</i>
            is the order. You learned that in a moving
            average process, the present value is linearly dependent on the mean, the current error term, and past
            error
            terms. The</p>
        <p>order <i>q</i> can be inferred using the ACF plot, where autocorrelation
            coefficients will be significant up until lag <i>q</i> only. In the case where the ACF
            plot
            shows a slowly decaying pattern or a sinusoidal pattern, it is possible that you are in the presence of
            an
            autoregressive process instead of a moving average process. </p>
        <p><b>101</b></p>
        <p><a id="calibre_link-214"></a><b>102</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>This led us to chapter 5, in which we covered the autoregressive process, denoted as AR( <i
                class="calibre3">p</i>), where <i>p</i> is the order. In the autoregressive
            process,
            the present value is linearly dependent on its own past value. In other words, it is a regression of the
            variable against itself. You saw that we can infer the order <i>p</i> using the PACF
            plot,
            where the partial autocorrelation coefficients will be significant up until lag <i>p</i>
            only. We are therefore at a point where we can identify, model, and predict a random walk, a pure moving
            average process, and a pure autoregressive process. </p>
        <p>The next step is learning how to treat time series where you cannot infer an order from the
            ACF plot or from the PACF plot. This means that both figures exhibit a slowly decaying pattern or a
            sinusoidal pattern. In such a case, we are in the presence of an autoregressive moving average (ARMA)
            process. This denotes the combination of both the autoregressive and moving average processes that we
            covered in the two previous chapters. </p>
        <p>In this chapter, we will examine the autoregressive moving average process, ARMA( <i>p</i>,
            <i>q</i>), where <i>p</i> denotes the order of
            the autoregressive portion and <i>q</i> denotes the order of the moving average
            portion.
            Furthermore, using the ACF and PACF plots to determine the orders <i>q</i> and <i class="calibre3">p</i>,
            respectively, becomes difficult, as both plots will show either a slowly
            decaying or sinusoidal pattern. Thus, we will define a general modeling procedure that will allow us to
            model such complex time series. This procedure involves model selection using the <i>Akaike
                information criterion</i> (AIC), which will determine the optimal combination of <i
                class="calibre3">p</i> and <i>q</i> for our series. Then we must evaluate the
            model's
            validity using residual analysis by studying the correlogram, Q-Q plot, and density plot of the model's
            residuals to assess if they closely resemble white noise. If that is the case, we can move on to
            forecasting
            our time series using the ARMA( <i>p</i>, <i>q</i>) model. </p>
        <p>This chapter will introduce foundational knowledge for forecasting complex time</p>
        <p>series. All the concepts introduced here will be reused in further chapters when we start
            modeling non-stationary time series and incorporating seasonality and exogenous variables. </p>
        <p> <i><b>6.1</b></i></p>
        <p> <i><b>Forecasting bandwidth usage for data
                    centers</b></i></p>
        <p>Suppose that you are tasked with predicting bandwidth usage for a large data center. </p>
        <p>Bandwidth is defined as the maximum rate of data that can be transferred. Its base unit is
            bits per second (bps). </p>
        <p>Forecasting bandwidth usage allows data centers to better manage their comput-</p>
        <p>ing resources. In the case where less bandwidth usage is expected, they can shut down some
            of their computing resources. This in turns reduces expenses and allows for maintenance. On the other
            hand,
            if bandwidth usage is expected to increase, they can dedicate the required resources to sustain the
            demand
            and ensure low latency, thus keeping their customers satisfied. </p>
        <p>For this situation, there are 10,000 data points representing the hourly bandwidth usage
            starting in January 1, 2019. Here the bandwidth is measured in megabits per second (Mbps), which is
            equivalent to 106 bps. We can visualize our time series in figure 6.1. </p>
        <p><a id="calibre_link-192"></a><img src="images/000061.jpg" alt="Image 54" class="calibre2" />
        </p>
        <p> <i><b>6.1</b></i></p>
        <p> <i><b>Forecasting bandwidth usage for data
                    centers</b></i></p>
        <p><b>103</b></p>
        <p>Figure 6.1</p>
        <p>Hourly bandwidth usage in a data center since January 1, 2019. The </p>
        <p>dataset contains 10,000 points. </p>
        <p>Looking at figure 6.1, you can see long-term trends over time, meaning that this series is
            likely not stationary, so we need to apply a transformation. Also, there seems to be no cyclical
            behavior,
            so we can rule out the presence of seasonality in our series. </p>
        <p>In order to forecast bandwidth usage, we need to identify the underlying process in our
            series. Thus, we'll follow the steps that we defined in chapter 5. That way, we can verify whether we
            have a
            random walk, a moving average process, or an autoregressive process. The steps are shown in figure 6.2.
        </p>
        <p>The first step is to collect the data, which is already done in this case. Then we must
            determine if our series is stationary or not. The presence of a trend in the plot hints that our series
            is
            not stationary. Nevertheless, we will apply the ADF test to check for stationarity and apply a
            transformation accordingly. </p>
        <p>Then we will plot the ACF function and find that there are significant autocorrelation
            coefficients after lag 0, which means it is not a random walk. However, we will observe that
            coefficients
            slowly decay. They do not become abruptly non-significant after a certain lag, which means that it is
            not a
            purely moving average process. </p>
        <p>We'll then move on to plotting the PACF function. This time we will notice a sinusoidal
            pattern, meaning that coefficients do not become abruptly non-significant after a certain lag. This will
            lead us to the conclusion that it is not a purely autoregressive process either. </p>
        <p>Therefore, it must be a combination of autoregressive and moving average processes,
            resulting in an autoregressive moving average process that can be modeled with the ARMA( <i
                class="calibre3">p</i>, <i>q</i>) model, where <i>p</i> is the
            order
            of the autoregressive process and <i>q</i> is</p>
        <p><a id="calibre_link-361"></a><b>104</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( <i>q</i>) process. </p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Plot PACF</p>
        <p>Do coefficients become</p>
        <p>abruptly non-significant</p>
        <p>Yes</p>
        <p>It is an AR( ) process. </p>
        <p> <i>p</i></p>
        <p>after lag <i>p</i>? </p>
        <p>No</p>
        <p>Figure 6.2</p>
        <p>Steps to identify a random </p>
        <p>walk, a moving average process, and an </p>
        <p>Not an AR( <i>p</i>) process</p>
        <p>autoregressive process</p>
        <p>the order of the moving average process. It is difficult to use the ACF and PACF plots to
            respectively find <i>p</i> and <i>q</i>, so we will fit many ARMA( <i class="calibre3">p</i>, <i>q</i>)
            models with different combinations of values for
            <i>p</i> and <i>q</i>. We will then select a model according to the
            Akaike</p>
        <p><a id="calibre_link-71"></a> <i><b>6.2</b></i></p>
        <p> <i><b>Examining the autoregressive moving average
                    process</b></i></p>
        <p><b>105</b></p>
        <p>information criterion and assess its viability by analyzing its residuals. Ideally, the
            residuals of a model will have characteristics similar to white noise. Then we will be able to use this
            model to make forecasts. For this example, we will forecast the hourly bandwidth usage over the next two
            hours. </p>
        <p> <i><b>6.2</b></i></p>
        <p> <i><b>Examining the autoregressive moving average
                    process</b></i></p>
        <p>The <i>autoregressive moving average process</i> is a combination of the
            autoregressive process and the moving average process. It states that the present value is linearly
            dependent on its own previous values and a constant, just like in an autoregressive process, as well as
            on
            the mean of the series, the current error term, and past error terms, like in a moving average process.
        </p>
        <p>The autoregressive moving average process is denoted as ARMA( <i>p</i>, <i class="calibre3">q</i>), where
            <i>p</i> is the order of the autoregressive portion,
            and
            <i>q</i> is the order of the moving average portion. Mathematically, the ARMA( <i class="calibre3">p</i>,
            <i>q</i>) process is expressed as a linear combination of a
            constant C, the past values of the series <i>yt&ndash;p</i>, the mean of the series µ,
            past
            error terms ϵ <i>t&ndash;q</i>, and the current error term ϵ <i>t</i>,
            as
            shown in equation 6.1. </p>
        <p> <i> y</i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = C + φ1 <i>yt</i>&ndash;1 + φ2 <i class="calibre3">yt</i>&ndash;2 +⋅⋅⋅+ φ <i>pyt&ndash;p</i> + ϵ
            <i class="calibre3">t</i> + θ1 <i>t-1</i> + θ2 <i>t</i>&ndash;2
            +⋅⋅⋅+ θ <i>q t&ndash;q</i> Equation 6.1</p>
        <p>Autoregressive moving average process</p>
        <p>The autoregressive moving average process is a combination of the autoregressive process and
            the moving average process. </p>
        <p>It is denoted as ARMA( <i>p</i>, <i>q</i>), where <i>p</i>
            is the order of the autoregressive process, and <i>q</i> is the
            order of the moving average process. The general equation of the ARMA( <i>p</i>, <i class="calibre3">q</i>)
            model is</p>
        <p> <i>y</i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = C + φ1 <i>yt</i>&ndash;1 + φ2 <i class="calibre3">yt</i>&ndash;2 +⋅⋅⋅+ φ <i>pyt&ndash;p</i> + µ +
            ϵ <i class="calibre3">t</i> + θ1 <i>t-1</i> + θ2 <i>t</i>&ndash;2
            +⋅⋅⋅+ θ <i>q t&ndash;q</i> An ARMA(0, <i>q</i>) process is equivalent
            to
            an MA( <i>q</i>) process, since the order <i>p</i> = 0 cancels the AR(
            <i>p</i>) portion. An ARMA( <i>p</i>,0) process is equivalent to an
            AR(
            <i>p</i>) process, since the order <i>q</i> = 0 cancels the MA( <i class="calibre3">q</i>) portion. </p>
        <p>Again, the order <i>p</i> determines the number of past values that affect
            the present value. </p>
        <p>Similarly, the order <i>q</i> determines the number of past error terms
            that affect the present value. In other words, the orders <i>p</i> and <i class="calibre3">q</i> dictate the
            number of parameters for the autoregressive and moving average
            portions, respectively. </p>
        <p>Thus, if we have an ARMA(1,1) process, we are combining an autoregressive pro-</p>
        <p>cess of order 1, or AR(1), with a moving average process of order 1, or MA(1). Recall that a
            first-order autoregressive process is a linear combination of a constant C, the value of the series at
            the
            previous timestep φ1 <i>y<sub>t-1</sub></i>, and white noise ϵ <i>t</i>,
            as
            shown in equation 6.2. </p>
        <p>AR(1) := <i>yt</i> = C + φ1 <i>y<sub>t-1</sub></i> + ϵ <i class="calibre3">t</i></p>
        <p>Equation 6.2</p>
        <p><a id="calibre_link-72"></a><b>106</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Also recall that a first-order moving average process, or MA(1), is a linear combination of
            the mean of the series µ, the current error term ϵ <i>t</i>, and the error term at the
            previous timestep θ ϵ</p>
        <p>1 <i>t </i>&ndash;1, as shown in equation 6.3. </p>
        <p>MA(1) := <i>y</i></p>
        <p>ϵ</p>
        <p> <i>t</i> = <i>µ</i> + ϵ <i>t</i> + θ1 <i class="calibre3">t </i>&ndash;1</p>
        <p>Equation 6.3</p>
        <p>We can combine the AR(1) and MA(1) processes to obtain an ARMA(1,1) process as</p>
        <p>shown in equation 6.4, which combines the effects of equations 6.2 and 6.3. </p>
        <p>ARMA(1,1) := <i>y</i></p>
        <p>ϵ</p>
        <p> <i>t</i> = C + φ1 <i>y<sub>t-1</sub></i> + ϵ <i>t</i> +
            θ1 <i>t </i>&ndash;1</p>
        <p>Equation 6.4</p>
        <p>If we have an ARMA(2,1) process, we are combining a second-order autoregressive process with
            a first-order moving average process. We know that we can express an AR(2) process as equation 6.5,
            while
            the MA(1) process from equation 6.3 remains the same. </p>
        <p></p>
        <p>Equation 6.5</p>
        <p>Thus, an ARMA(2,1) process can be expressed as the combination of the AR(2) pro-</p>
        <p>cess defined in equation 6.5 and the MA(1) process defined in equation 6.3. This is shown in
            equation 6.6. </p>
        <p></p>
        <p>Equation 6.6</p>
        <p>In the case where <i>p</i> = 0, we have an ARMA(0, <i>q</i>) process,
            which
            is equivalent to a pure MA( <i>q</i>) process as
            seen in chapter 4. Similarly, if <i>q</i> = 0, we have an ARMA( <i class="calibre3">p</i>,0) process, which
            is equivalent to a pure AR( <i>p</i>)
            process,
            as seen in chapter 5. </p>
        <p>We can see now how the order <i>p</i> only affects the autoregressive
            portion of the process by determining the number of past values to include in the equation. Similarly,
            the
            order <i>q</i> only affects the moving average portion of the process by determining
            the
            number of past error terms to include in the equation of ARMA( <i>p</i>, <i class="calibre3">q</i>). Of
            course, the higher the orders <i>p</i> and <i class="calibre3">q</i>, the more terms that are included, and
            the more complex our process becomes.
        </p>
        <p>In order to model and forecast an ARMA( <i>p</i>, <i>q</i>) process, we
            need to find the orders <i>p</i> and <i>q</i>. That way, we can use an
            ARMA( <i>p</i>, <i>q</i>) model to fit the available data and produce
            forecasts. </p>
        <p> <i><b>6.3</b></i></p>
        <p> <i><b>Identifying a stationary ARMA process</b></i></p>
        <p>Now that we've defined the autoregressive moving average process and seen how the orders <i
                class="calibre3">p</i> and <i>q</i> affect the model's equation, we need to
            determine
            how to identify such an underlying process in a given time series. </p>
        <p>We'll extend the steps that we defined in chapter 5 to include the final possibility that we
            have an ARMA( <i>p</i>, <i>q</i>) process, as shown in figure 6.3.
        </p>
        <p><a id="calibre_link-362"></a> <i><b>6.3</b></i></p>
        <p> <i><b>Identifying a stationary ARMA process</b></i></p>
        <p><b>107</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Plot ACF</p>
        <p>Is there</p>
        <p>No</p>
        <p>It is a random walk. </p>
        <p>autocorrelation? </p>
        <p>Yes</p>
        <p>Do autocorrelation</p>
        <p>coefficients become</p>
        <p>Yes</p>
        <p>It is an MA( <i>q</i>) process. </p>
        <p>abruptly non-significant</p>
        <p>after lag <i>q</i>? </p>
        <p>No</p>
        <p>Plot PACF</p>
        <p>Do coefficients become</p>
        <p>abruptly non-significant</p>
        <p>Yes</p>
        <p>It is an AR( <i>p</i>) process. </p>
        <p>after lag <i>p</i>? </p>
        <p>Figure 6.3</p>
        <p>Steps to identify a random </p>
        <p>walk, a moving average process MA( <i><b>q</b></i>), </p>
        <p>No</p>
        <p>an autoregressive process AR( <i><b>p</b></i>), and an
        </p>
        <p>autoregressive moving average process </p>
        <p>It is an ARMA( <i>p</i>, <i>q</i>) process. </p>
        <p>ARMA( <i><b>p</b></i>, <i><b>q</b></i>)
        </p>
        <p>In figure 6.3 you'll notice that if neither of the ACF and PACF plots shows a clear cutoff
            between significant and non-significant coefficients, then we have an ARMA( <i>p</i>,
            <i>q</i>) process. To verify that, let's simulate our own ARMA process. </p>
        <p><a id="calibre_link-200"></a><b>108</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>We'll simulate an ARMA(1,1) process. This is equivalent to combining an MA(1)</p>
        <p>process with an AR(1) process. Specifically, we will simulate the ARMA(1,1) process defined
            in equation 6.7. Notice that the constant C and mean µ are both equal to 0</p>
        <p>here. The coefficients 0.33 and 0.9 are subjective choices for this simulation. </p>
        <p></p>
        <p></p>
        <p></p>
        <p> <i>yt</i> = 0.33 <i>y<sub>t-1</sub></i> + 0.9ϵ <i>t
            </i>&ndash;1 + ϵ <i>t</i></p>
        <p>Equation 6.7</p>
        <p>The objective of this simulation is to demonstrate that we cannot use the ACF plot to
            identify the order <i>q</i> of an ARMA( <i>p</i>, <i class="calibre3">q</i>) process, which in this case is
            1, nor can we use the PACF plot to identify
            the
            order <i>p</i> of an ARMA( <i>p</i>, <i>q</i>)
            process,
            which in this case is 1 also. </p>
        <p>We'll use the ArmaProcess function from the statsmodels library to simulate our</p>
        <p>ARMA(1,1) process. As in previous chapters, we'll define the array of coefficients for the
            AR(1) process, as well as for the MA(1) process. From equation 6.7, we know our AR(1) process will have
            a
            coefficient of 0.33. However, keep in mind that the function expects to have the coefficient of the
            autoregressive process with its opposite sign, as this is how it is implemented in the statsmodels
            library.
            Therefore, we input it as &ndash;0.33. </p>
        <p>For the moving average portion, equation 6.7 specifies that the coefficient is 0.9. Also
            recall that when defining your arrays of coefficients, the first coefficient is always equal to 1, as
            specified by the library, which represents the coefficient at lag 0. Once our coefficients are defined,
            we
            will generate 1,000 data points. </p>
        <p>NOTE</p>
        <p>The source code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH06">https://</a></p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH06">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH06</a>.
        </p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p>import numpy as np</p>
        <p><b>Define the coefficients for the AR(1) portion. </b></p>
        <p>np.random.seed(42)</p>
        <p><b>Remember that the first coefficient is always 1, </b></p>
        <p><b>as specified by the documentation. Also, we must </b></p>
        <p>ar1 = np.array([1, -0.33]) </p>
        <p><b>write the coefficient of the AR portion with the </b></p>
        <p>ma1 = np.array([1, 0.9]) </p>
        <p><b>opposite sign of what is defined in equation 6.7. </b></p>
        <p>ARMA_1_1 = ArmaProcess(ar1, ma1).generate_sample(nsample=1000) </p>
        <p><b>Define the coefficients for the MA(1) portion. The first </b></p>
        <p><b>Generate</b></p>
        <p><b>coefficient is 1, for lag 0, as specified by the documentation. </b></p>
        <p><b>1,000 samples. </b></p>
        <p>With our simulated data ready, we can move on to the next step and verify whether our
            process is stationary or not. We can do this by running the augmented Dickey-Fuller (ADF) test. We'll
            print
            out the ADF statistic as well as the p-value. If the ADF statistic is a large negative number, and if we
            have a p-value smaller than 0.05, we can reject the null hypothesis and conclude that we have a
            stationary
            process. </p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p>ADF_result = adfuller(ARMA_1_1) </p>
        <p><b>Run the ADF test </b></p>
        <p><b>on the simulated </b></p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p><b>ARMA(1,1) data. </b></p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p><a id="calibre_link-281"></a><img src="images/000151.jpg" alt="Image 55" class="calibre2" />
        </p>
        <p> <i><b>6.3</b></i></p>
        <p> <i><b>Identifying a stationary ARMA process</b></i></p>
        <p><b>109</b></p>
        <p>This returns an ADF statistic of &ndash;6.43 and a p-value of 1.7×10&ndash;8. Since we have
            a large negative ADF statistic and a p-value that's much smaller than 0.05, we can conclude that our
            simulated ARMA(1,1) process is stationary. </p>
        <p>Following the steps outlined in figure 6.3, we'll plot the ACF and see if we can infer the
            order of the moving average portion of our simulated ARMA(1,1) process. Again, we'll use the plot_acf
            function from statsmodels to generate figure 6.4. </p>
        <p>from statsmodels.graphics.tsaplots import plot_acf</p>
        <p>plot_acf(ARMA_1_1, lags=20); </p>
        <p>plt.tight_layout()</p>
        <p>Figure 6.4</p>
        <p>ACF plot of our simulated ARMA(1,1) process. Notice the </p>
        <p>sinusoidal pattern on the plot, meaning that an AR( <i><b>p</b></i>)
            process is in play. </p>
        <p>Also, the last significant coefficient is at lag 2, which suggests that <i><b>q</b></i> = 2.
        </p>
        <p>However, we know that we simulated an ARMA(1,1) process, so <i><b>q</b></i>
            must be equal to 1! Therefore, the ACF plot cannot be used to infer the
            order <i><b>q</b></i> of an ARMA( <i><b>p</b></i>, <i><b class="calibre4">q</b></i>) process. </p>
        <p>In figure 6.4 you'll notice a sinusoidal pattern in the plot, which indicates the presence
            of an autoregressive process. This is expected, since we simulated an ARMA(1,1) process and we know of
            the
            existence of the autoregressive portion. Furthermore, you'll notice that the last significant
            coefficient is
            at lag 2. However, we know that our simulated data has an MA(1) process, so we would expect to have
            significant coefficients up to lag 1 only. We can thus conclude that the ACF plot does not reveal any
            useful
            information about the order <i>q</i> of our ARMA(1,1) process. </p>
        <p>We can now move on to the next step outlined in figure 6.3 and plot the PACF. In chapter 5
            you learned that the PACF can be used to find the order of a stationary AR( <i>p</i>)
            process. We will now verify whether we can find the order <i>p</i> of our simulated</p>
        <p><a id="calibre_link-287"></a><img src="images/000056.jpg" alt="Image 56" class="calibre2" />
        </p>
        <p><b>110</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>ARMA(1,1) process, where p = 1. We'll use the plot_pacf function to generate figure 6.5.
        </p>
        <p>from statsmodels.graphics.tsaplots import plot_pacf</p>
        <p>plot_pacf(ARMA_1_1, lags=20); </p>
        <p>plt.tight_layout()</p>
        <p>Figure 6.5</p>
        <p>PACF plot of our simulated ARMA(1,1) process. Again, we have a </p>
        <p>sinusoidal pattern with no clear cutoff between significant and non-significant </p>
        <p>coefficients. From this plot, we cannot infer that p = 1 in our simulated </p>
        <p>ARMA(1,1) process, meaning that we cannot determine the order <i><b>p</b></i> of an ARMA(
            <i><b>p</b></i>, <i class="calibre3"><b>q</b></i>) process using a PACF
            plot. </p>
        <p>In figure 6.5 we can see a clear sinusoidal pattern, meaning that we cannot infer a value
            for the order <i>p</i>. We know that we simulated an ARMA(1,1) process, but we cannot
            determine that value from the PACF plot in figure 6.5, since we have significant coefficients past lag
            1.
            Therefore, the PACF plot cannot be used to find the order <i>p</i> of an ARMA( <i class="calibre3">p</i>,
            <i>q</i>) process. </p>
        <p>According to figure 6.3, since there is no clear cutoff between significant and
            non-significant coefficients in both the ACF and PACF plots, we can conclude that we have an ARMA( <i
                class="calibre3">p</i>, <i>q</i>) process, which is indeed the case. </p>
        <p>Identifying a stationary ARMA( <i><b>p</b></i>, <i><b>q</b></i>) process
        </p>
        <p>If your process is stationary and both the ACF and PACF plots show a decaying or sinusoidal
            pattern, then it is a stationary ARMA( <i>p</i>, <i>q</i>) process.
        </p>
        <p><a id="calibre_link-73"></a> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>111</b></p>
        <p>We know that determining the order of our process is key in modeling and forecasting, since
            the order will dictate how many parameters must be included in our model. </p>
        <p>Since the ACF and PACF plots are not useful in the case of an ARMA( <i>p</i>, <i class="calibre3">q</i>)
            process, we must thus devise a general modeling
            procedure that will allow us to find the appropriate combination of ( <i>p</i>, <i class="calibre3">q</i>)
            for our model. </p>
        <p> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p>In the previous section, we covered the steps for identifying a stationary ARMA( <i>p</i>,
            <i>q</i>) process. We saw that if both the ACF and PACF plots
            display a sinusoidal or decaying pattern, our time series can be modeled by an ARMA( <i
                class="calibre3">p</i>, <i>q</i>) process. However, neither plot was useful for
            determining the orders <i>p</i> and <i>q</i>. With our simulated
            ARMA(1,1)
            process, we noticed that coefficients were significant after lag 1 in both plots. </p>
        <p>Therefore, we must devise a procedure that allows us to find the orders <i>p</i> and <i
                class="calibre3">q</i>. </p>
        <p>This procedure will have the advantage that it can also be applied in situations where our
            time series is non-stationary and has seasonal effects. Furthermore, it will also be suitable for cases
            where <i>p</i> or <i>q</i> are equal to 0, meaning that we can move
            away
            from plotting the ACF and PACF and rely entirely on a model selection criterion and residual analysis.
            The
            steps are shown in figure 6.6. </p>
        <p>In figure 6.6 you can see that this new modeling procedure completely removes the plotting
            of the ACF and PACF. It allows us to select a model based entirely on statistical tests and numerical
            criteria, instead of relying on the qualitative analysis of the ACF and PACF plots. </p>
        <p>The first few steps remain unchanged from those we gradually built up until chapter 5, as we
            must still gather the data, test for stationarity, and apply transformations accordingly. Then we list
            different possible values of <i>p</i> and <i>q</i>&mdash;note that
            they
            only take positive integers. With a list of possible values, we can fit every unique combination of
            ARMA( <i>p</i>, <i>q</i>) to our data. </p>
        <p>Once that's done, we can compute the <i>Akaike information criterion</i>
            (AIC), which is discussed at length in sections 6.4.1 and 6.4.2. This quantifies the quality of each
            model
            in relation to each other. The model with the lowest AIC is then selected. </p>
        <p>From there, we can analyze the model's residuals, which is the difference between the actual
            and predicted values of the model. Ideally, the residuals will look like white noise, which would mean
            that
            any difference between the predicted values and actual values is due to randomness. Therefore, the
            residuals
            must be uncorrelated and independently distributed. We can assess those properties by studying the <i
                class="calibre3">quantile-quantile</i> <i>plot</i> (Q-Q plot) and running the <i
                class="calibre3">Ljung-Box test</i>, which we'll explore in section 6.4.3. If the analysis leads us
            to
            conclude that the residuals are completely random, we have a model ready for forecasting. Otherwise, we
            must
            try a different set of values for <i>p</i> and <i>q</i> and start the
            process over. </p>
        <p>A lot of new concepts and techniques will be introduced as we work through our</p>
        <p>new general modeling procedure. We will dive into each step in detail in future sections</p>
        <p><a id="calibre_link-363"></a><b>112</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Ready for</p>
        <p>forecasts</p>
        <p>Figure 6.6</p>
        <p>General modeling procedure for an ARMA( <i><b>p</b></i>,
            <i><b>q</b></i>) process. The first steps are to gather the data, test
            for
            stationarity, and apply transformations accordingly. Then we define a list of possible values for <i
                class="calibre3"><b>p</b></i> and <i><b>q</b></i>. We
            then fit every combination of ARMA( <i><b>p</b></i>, <i class="calibre3"><b class="calibre4">q</b></i>) to
            our data and select the model with the lowest
            AIC.
            Then we perform the residual analysis by looking at the Q-Q plot and the residual correlogram. If they
            approach that of white noise, the model can be used for forecasts. Otherwise, we must try different
            values
            for <i><b>p</b></i> and <i><b>q</b></i>.
        </p>
        <p><a id="calibre_link-74"></a> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>113</b></p>
        <p>and work with our simulated ARMA(1,1) process. Then we will apply the same procedure to
            model bandwidth usage. </p>
        <p> <i><b>6.4.1</b></i></p>
        <p> <i><b>Understanding the Akaike information criterion
                    (AIC)</b></i></p>
        <p>Before covering the steps outlined in figure 6.6, we need to determine how we will choose
            the best model of all the models that we will fit. Here we will use the Akaike information criterion
            (AIC)
            to select the optimal model. </p>
        <p>The AIC estimates the quality of a model relative to other models. Given that there will be
            some information lost when a model is fitted to the data, the AIC quantifies the relative amount of
            information lost by the model. The less information lost, the lower the AIC value and the better the
            model.
        </p>
        <p>The AIC is a function of the number of estimated parameters <i>k</i> and
            the maximum value of the likelihood function for the model , as shown in equation 6.8. </p>
        <p>Equation 6.8</p>
        <p>Akaike information criterion (AIC)</p>
        <p>The Akaike information criterion (AIC) is a measure of the quality of a model in relation to
            other models. It is used for model selection. </p>
        <p>The AIC is a function of the number of parameters <i>k</i> in a model and
            the maximum value of the likelihood function </p>
        <p>:</p>
        <p>The lower the value of the AIC, the better the model. Selecting according to the AIC</p>
        <p>allows us to keep a balance between the complexity of a model and its goodness of fit to the
            data. </p>
        <p>The number of estimated parameters <i>k</i> is directly related to the
            order ( <i>p</i>, <i>q</i>) of an ARMA( <i>p</i>, <i class="calibre3">q</i>) model. If we fit an ARMA(2,2)
            model, then we have 2 + 2 = 4 parameters to
            estimate. If we fit an ARMA(3,4) model, then we have 3 + 4 = 7 parameters to estimate. You can see how
            fitting a more complex model can penalize the AIC score: as the order ( <i>p</i>, <i class="calibre3">q</i>)
            increases, the number of parameters <i>k</i> increases, and
            so
            the AIC increases. </p>
        <p>The likelihood function measures the goodness of fit of a model. It can be viewed as the
            opposite of the distribution function. Given a model with fixed parameters, the distribution function
            will
            measure the probability of observing a data point. The likelihood function flips the logic. Given a set
            of
            observed data, it will estimate how likely it is that different model parameters will generate the
            observed
            data. </p>
        <p>For example, consider the situation where we roll a six-sided die. The distribution function
            tells us that there is a 1/6 probability that we'll observe one of these values:</p>
        <p>[1,2,3,4,5,6]. Now let's flip this logic to explain the likelihood function. Suppose that
            you roll a die 10 times and you obtain the following values: [1,5,3,4,6,2,4,3,2,1]. The</p>
        <p><a id="calibre_link-75"></a><b>114</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>likelihood function will determine how likely it is that the die has six sides. Applying
            this logic to the context of AIC, we can think of the likelihood function as an answer to the question
            “How
            likely is it that my observed data is coming from an ARMA(1,1) model?” If it is very likely, meaning
            that <i>L</i> îs large, then the ARMA(1,1) model fits the data well. </p>
        <p>Therefore, if a model fits the data really well, the maximum value of the likelihood will be
            high. Since the AIC subtracts the natural logarithm of the maximum value of the likelihood, represented
            by
            <i>L</i> în equation 6.8, then a large value of <i>L</i> ˆ will lower
            the
            AIC. </p>
        <p>You can see how the AIC keeps a balance between underfitting and overfitting. </p>
        <p>Remember that the lower the AIC, the better the model relative to other models. Therefore,
            an overfitting model would have a very good fit, meaning that <i>L</i> îs large and AIC
        </p>
        <p>decreases. However, the number of parameters <i>k</i> would be large as
            well, which penalizes the AIC. An underfitting model would have a small number of parameters, so <i
                class="calibre3">k</i> would be small. However, the maximum value of the likelihood function would
            also
            be small due to the poor fit, meaning again that the AIC is penalized. Thus, the AIC</p>
        <p>allows us to find a balance between the number of parameters in a model and a good fit to
            the training data. </p>
        <p>Finally, we must keep in mind that the AIC quantifies the quality of a model in relation to
            other models only. It is therefore a relative measure of quality. In the event that we fit only poor
            models
            to our data, the AIC will simply help us determine the best from that group of models. </p>
        <p>Now let's use the AIC to help us select an appropriate model for our simulated ARMA(1,1)
            process. </p>
        <p> <i><b>6.4.2</b></i></p>
        <p> <i><b>Selecting a model using the AIC</b></i></p>
        <p>We'll now cover the steps of the general modeling procedure outlined in figure 6.6</p>
        <p>using our simulated ARMA(1,1) process. </p>
        <p>In section 6.3 we tested for stationarity and concluded that our simulated process is
            already stationary. Therefore, we can move on to defining a list of possible values for <i
                class="calibre3">p</i> and <i>q</i>. While we know the values of both orders from
            the
            simulation, let's consider the following steps as a demonstration that the general modeling procedure
            works.
        </p>
        <p>We will allow the values of <i>p</i> and <i>q</i> to vary
            from 0 to 3. Note that this range is arbitrary, and you may try a larger range of values if you wish. We
            will create a list of all possible combinations of ( <i>p</i>, <i>q</i>),
            using the product function from itertools. Since there four possible values for <i>p</i>
            and <i>q</i>, this will generate a list of 16 unique combinations of ( <i class="calibre3">p</i>, <i>q</i>).
        </p>
        <p><b>Create a list of possible values for p starting from </b></p>
        <p><b>0 inclusively to 4 exclusively, with steps of 1. </b></p>
        <p>from itertools import product</p>
        <p><b>Create a list of possible values for q starting from </b></p>
        <p>ps = range(0, 4, 1) </p>
        <p><b>0 inclusively to 4 exclusively, with steps of 1. </b></p>
        <p>qs = range(0, 4, 1) </p>
        <p><b>Generate a list containing all </b></p>
        <p>order_list = list(product(ps, qs)) </p>
        <p><b>unique combinations of (p,q). </b></p>
        <p><a id="calibre_link-273"></a> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>115</b></p>
        <p>With our list of possible values created, we must now fit all unique 16 ARMA( <i>p</i>,
            <i>q</i>) models to our simulated data. To do so, we'll define
            an optimize_ARMA function that takes the data and the list of unique ( <i>p</i>, <i class="calibre3">q</i>)
            combinations as input. Inside the function, we'll initialize an empty list
            to
            store each ( <i>p</i>, <i>q</i>) combination and its corresponding
            AIC.
        </p>
        <p>Then we'll iterate over each ( <i>p</i>, <i>q</i>)
            combination and fit an ARMA( <i>p</i>, <i>q</i>) model to our data.
            We'll
            compute the AIC and store the result. Then we'll create a DataFrame and sort it by AIC value in
            ascending
            order, since the lower the AIC, the better the model. </p>
        <p>Our function will finally output the ordered DataFrame so we can select the appropriate
            model. The optimize_ARMA function is shown in the following listing. </p>
        <p>Listing 6.1</p>
        <p>Function to fit all unique ARMA( <i><b>p</b></i>, <i><b>q</b></i>) models
        </p>
        <p><b>The function takes as inputs the time series </b></p>
        <p><b>data and the list of unique (p,q) combinations. </b></p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_ARMA(endog: Union[pd.Series, list], order_list: list) -&gt; </p>
        <p>➥ pd.DataFrame: </p>
        <p><b>Initialize an empty list to </b></p>
        <p></p>
        <p><b>Iterate over each unique </b></p>
        <p><b>store the order (p,q) and its </b></p>
        <p>results = [] </p>
        <p><b>(p,q) combination. The use </b></p>
        <p><b>corresponding AIC as a tuple. </b></p>
        <p><b>of tqdm_notebook will </b></p>
        <p></p>
        <p><b>display a progress bar. </b></p>
        <p>for order in tqdm_notebook(order_list): </p>
        <p>try: </p>
        <p>model = SARIMAX(endog, order=(order[0], 0, order[1]), </p>
        <p>➥ simple_differencing=False).fit(disp=False) </p>
        <p><b>Fit an ARMA(p,q) model using </b></p>
        <p>except:</p>
        <p><b>the SARIMAX function. We specify </b></p>
        <p><b>Append the </b></p>
        <p></p>
        <p><b>(p,q)</b></p>
        <p>continue</p>
        <p><b>Calculate the </b></p>
        <p><b>simple_differencing=False to </b></p>
        <p><b>combination </b> </p>
        <p><b>and</b></p>
        <p></p>
        <p><b>model's AIC. </b></p>
        <p><b>prevent differencing. Recall </b></p>
        <p><b>AIC as a tuple </b> </p>
        <p><b>to</b></p>
        <p>aic = model.aic </p>
        <p><b>that differencing is the result </b></p>
        <p><b>the results list. </b></p>
        <p>results.append([order, aic]) </p>
        <p><b>of yt &ndash; yt&ndash;1. We also specify </b></p>
        <p></p>
        <p><b>disp=False to avoid printing </b></p>
        <p><b>Label the</b></p>
        <p>result_df = pd.DataFrame(results) </p>
        <p><b>convergence messages to the </b></p>
        <p><b>columns of</b></p>
        <p>result_df.columns = ['(p,q)', 'AIC'] </p>
        <p><b>console. </b></p>
        <p><b>the DataFrame. </b></p>
        <p></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p><b>Store the (p,q) </b></p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p><b>combination and AIC </b></p>
        <p>➥ ascending=True).reset_index(drop=True) </p>
        <p><b>in a DataFrame. </b></p>
        <p></p>
        <p><b>Sort the DataFrame in</b></p>
        <p>return result_df</p>
        <p><b>ascending order of AIC</b></p>
        <p><b>values. The lower the AIC, </b></p>
        <p><b>the better the model. </b></p>
        <p>With our function defined, we can now use it and fit the different ARMA( <i>p</i>, <i class="calibre3">q</i>)
            models. </p>
        <p>The output is shown in figure 6.7. You'll see that the model with the lowest AIC corresponds
            to an ARMA(1,1) model, which is exactly the process that we simulated. </p>
        <p><b>Fit the different ARMA(p,q) </b></p>
        <p>result_df = optimize_ARMA(ARMA_1_1, order_list) </p>
        <p><b>models on the simulated </b></p>
        <p>result_df </p>
        <p><b>ARMA(1,1) data. </b></p>
        <p><b>Display the resulting DataFrame. </b></p>
        <p><a id="calibre_link-76"></a><img src="images/000135.jpg" alt="Image 57" class="calibre2" />
        </p>
        <p><b>116</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Figure 6.7</p>
        <p>Resulting <b>DataFrame</b> from fitting all </p>
        <p>ARMA( <i><b>p</b></i>, <i><b>q</b></i>)
            models to the simulated ARMA(1,1) </p>
        <p>data. We can see that the model with the lowest </p>
        <p>AIC corresponds to an ARMA(1,1) model, meaning </p>
        <p>that we successfully identified the order of our </p>
        <p>simulated data. </p>
        <p>As mentioned in the previous section, the AIC is a measure of relative quality. Here we can
            say that an ARMA(1,1) model is the best model relative to all other models that we fit to our data. Now
            we
            need an absolute measure of the model's quality. This brings us to the next step of our modeling
            procedure,
            which is residual analysis. </p>
        <p> <i><b>6.4.3</b></i></p>
        <p> <i><b>Understanding residual analysis</b></i></p>
        <p>Up to this point, we have fit different ARMA( <i>p</i>, <i>q</i>)
            models to
            our simulated ARMA(1,1) process. Using the AIC as a model selection
            criterion, we found that an ARMA(1,1) model is the best model relative to all others that were fit. Now
            we
            must measure its absolute quality by performing an analysis on the model's residuals. </p>
        <p>This brings us to the last steps before forecasting, which is residual analysis and
            answering the two questions in figure 6.8: does the Q-Q plot show a straight line, and are the residuals
            uncorrelated? If the answer to both questions is yes, then we have a model that's ready to make
            forecasts.
            Otherwise, we must try different combinations of ( <i>p</i>, <i>q</i>)
            and
            restart the process. </p>
        <p>The residuals of a model are simply the difference between the predicted values</p>
        <p>and the actual values. Consider our simulated ARMA(1,1) process expressed in equation 6.9.
        </p>
        <p> <i> </i></p>
        <p>Equation 6.9</p>
        <p><a id="calibre_link-364"></a> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>117</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Ready for</p>
        <p>Figure 6.8</p>
        <p>The general modeling </p>
        <p>forecasts</p>
        <p>procedure for an ARMA( <i><b>p</b></i>, <i><b>q</b></i>) process</p>
        <p>Now suppose that we fit an ARMA(1,1) model to our process, and we estimate the model's
            coefficients perfectly, such that the model is expressed as equation 6.10. </p>
        <p></p>
        <p>Equation 6.10</p>
        <p><a id="calibre_link-295"></a><b>118</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>The residuals will be the difference between the values coming from our model and the
            observed values from our simulated process. In other words, the residuals are the difference between
            equation 6.9 and equation 6.10. The result is shown in equation 6.11. </p>
        <p></p>
        <p>Equation 6.11</p>
        <p>As you can see in equation 6.11, in a perfect situation the residuals of a model are white
            noise. This indicates that the model has captured all predictive information, and there is only a random
            fluctuation left that cannot be modeled. Thus, the residuals must be uncorrelated and have a normal
            distribution in order for us to conclude that we have a good model for making forecasts. </p>
        <p>There are two aspects to residual analysis: a qualitative analysis and a quantitative
            analysis. The qualitative analysis focuses on studying the Q-Q plot, while the quantitative analysis
            determines whether our residuals are uncorrelated. </p>
        <p>QUALITATIVE ANALYSIS: STUDYING THE Q-Q PLOT</p>
        <p>The first step in residual analysis is the study of the <i>quantile-quantile plot</i>
            (Q-Q
            plot). </p>
        <p>The Q-Q plot is a graphical tool for verifying our hypothesis that the model's residuals are
            normally distributed. </p>
        <p>The Q-Q plot is constructed by plotting the quantiles of our residuals on the <i>y</i>-axis
            against the quantiles of a theoretical distribution, in this case the normal
            distribution, on the <i>x</i>-axis. This results in a scatterplot. We are comparing the
            distribution to a normal distribution because we want the residuals to be similar to white noise, which
            is
            normally distributed. </p>
        <p>If both distributions are similar, meaning that the distribution of the residuals is close
            to a normal distribution, the Q-Q plot will display a straight line that approximately lies on <i
                class="calibre3">y</i> = <i>x</i>. This in turn means that our model is a good fit
            for
            our data. You can see an example of a Q-Q plot where the residuals are normally distributed in figure
            6.9.
        </p>
        <p>On the other hand, a Q-Q plot of residuals that are not close to a normal distribution will
            generate a curve that departs from <i>y</i> = <i>x</i>. In figure 6.10
            you
            can see that the thick line is not straight and not lying on <i>y</i> = <i class="calibre3">x</i>. If we get
            this sort of result, we can conclude that the distribution of our
            residuals does not resemble a normal distribution, which is a sign that our model is not a good fit for
            our
            data. Therefore, we must try a different range of values for <i>p </i> and <i class="calibre3">q</i>, fit
            the models, select the one with the lowest AIC, and perform residual
            analysis on the new model. </p>
        <p></p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-365"></a><img src="images/000023.jpg" alt="Image 58" class="calibre2" />
        </p>
        <p><img src="images/000156.jpg" alt="Image 59" class="calibre2" /></p>
        <p> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>119</b></p>
        <p>Figure 6.9</p>
        <p>A Q-Q plot of randomly distributed residuals. On the <i><b>y</b></i>-axis,
            we </p>
        <p>have the quantiles coming from the residuals. On the <i><b>x</b></i>-axis,
            we have the quantiles coming from a theoretical normal
            distribution. You can see a </p>
        <p>straight line approximately lying on <i><b>y</b></i> = <i class="calibre3"><b class="calibre4">x</b></i>.
            This is an indication that our residuals are very
            close
            to a normal distribution. </p>
        <p>Figure 6.10</p>
        <p>A Q-Q plot of residuals that are not close to a normal </p>
        <p>distribution. You can clearly see that the thick line is curved, and it is not </p>
        <p>lying on <i><b>y</b></i> = <i><b>x</b></i>. Therefore, the distribution of
            the residuals is very different from
            a
            normal distribution. </p>
        <p><a id="calibre_link-251"></a><b>120</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Quantile-quantile plot (Q-Q plot)</p>
        <p>A Q-Q plot is a plot of the quantiles of two distributions against each other. In time
            series forecasting, we plot the distribution of our residuals on the <i>y</i>-axis
            against
            the theoretical normal distribution on the <i>x</i>-axis. </p>
        <p>This graphical tool allows to us to assess the goodness of fit of our model. If the
            distribution of our residuals is similar to a normal distribution, we will see a straight line lying on
            <i>y</i> = <i>x</i>. This means that our model is a good fit, because
            the
            residuals are similar to white noise. </p>
        <p>On the other hand, if the distribution of our residuals is different from a normal
            distribution, we will see a curved line. We can then conclude that our model is not a good fit, since
            the
            residuals' distribution is not close to a normal distribution, and therefore the residuals are not
            similar
            to white noise. </p>
        <p>You can see how the Q-Q plot can help us. We know that if a model is a good fit to our data,
            the residuals will be similar to white noise and therefore will have similar properties. This means that
            they should be normally distributed. Hence, if the Q-Q plot displays a straight line, we have a good
            model.
            Otherwise, our model must be discarded, and we must try to fit a better model. </p>
        <p>While the Q-Q plot is a fast method for assessing the quality of our model, this analysis
            remains subjective. Thus, we will further support our residual analysis with a quantitative method by
            applying the Ljung-Box test. </p>
        <p>QUANTITATIVE ANALYSIS: APPLYING THE LJUNG-BOX TEST</p>
        <p>Once we have analyzed the Q-Q plot and determined that our residuals are approximately
            normally distributed, we can then apply the Ljung-Box test to demonstrate that the residuals are
            uncorrelated. Remember that a good model has residuals that are similar to white noise, so the residuals
            should be normally distributed and uncorrelated. </p>
        <p>The Ljung-Box test is a statistical test that tests if the autocorrelation of a group of
            data is significantly different from 0. In our case, we will apply the Ljung-Box test to the model's
            residuals to assess whether they are correlated or not. The null hypothesis states that the data is
            independently distributed, meaning that there is no autocorrelation. </p>
        <p>Ljung-Box test</p>
        <p>The Ljung-Box test is a statistical test that determines whether the autocorrelation of a
            group of data is significantly different from 0. </p>
        <p>In time series forecasting, we apply the Ljung-Box test on the model's residuals to test
            whether they are similar to white noise. The null hypothesis states that the data is independently
            distributed, meaning that there is no autocorrelation. If the p-value is larger than 0.05, we cannot
            reject
            the null hypothesis, meaning that the residuals are independently distributed. Therefore, there is no
            autocorrelation, the residuals are similar to white noise, and the model can be used for forecasting.
        </p>
        <p><a id="calibre_link-77"></a> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>121</b></p>
        <p>If the p-value is less than 0.05, we reject the null hypothesis, meaning that our residuals
            are not independently distributed and are correlated. The model cannot be used for forecasting. </p>
        <p>The test will return the Ljung-Box statistic and a p-value. If the p-value is less than
            0.05, we reject the null hypothesis, meaning that the residuals are not independently distributed, which
            in
            turn means that there is autocorrelation. In such a situation, the residuals do not approximate the
            properties of white noise, and the model must be discarded. </p>
        <p>If the p-value is larger than 0.05, we cannot reject the null hypothesis, meaning that our
            residuals are independently distributed. Thus, there is no autocorrelation, and the residuals are
            similar to
            white noise. This means that we can move on with our model and make forecasts. </p>
        <p>Now that you understand the concepts of residual analysis, let's apply these techniques to
            our simulated ARMA(1,1) process. </p>
        <p> <i><b>6.4.4</b></i></p>
        <p> <i><b>Performing residual analysis</b></i></p>
        <p>We will now resume the modeling procedure for our simulated ARMA(1,1) process. </p>
        <p>We have successfully selected a model with the lowest AIC, which was expectedly an ARMA(1,1)
            model. Now, as you can see in figure 6.11, we need to perform residual analysis to assess whether our
            model
            is a good fit to the data. </p>
        <p>We know that our ARMA(1,1) model must be good, since we simulated an ARMA(1,1) process, but
            this section will demonstrate that our modeling procedure works. We are not likely to be modeling and
            forecasting simulated data in a business context, so it is important to cover the entire modeling
            procedure
            on a known process first, to convince ourselves that it works, before applying it on real-life data.
        </p>
        <p>To perform residual analysis, we need to fit our model and store the residuals in a variable
            for easy access. Using statsmodels, we will first define an ARMA(1,1) model before fitting it to our
            simulated data. Then we can access the residuals with the resid property. </p>
        <p>model = SARIMAX(ARMA_1_1, order=(1,0,1), simple_differencing=False)</p>
        <p>model_fit = model.fit(disp=False)</p>
        <p>residuals = model_fit.resid </p>
        <p><b>Store the model's residuals. </b></p>
        <p>The next step is to plot the Q-Q plot, and we'll use the qqplot function from statsmodels to
            display our residuals against a normal distribution. The function simply requires the data, and it will
            by
            default compare its distribution to a normal distribution. We'll also need to display the line <i
                class="calibre3">y</i> = <i>x</i> in order to assess the similarity of both
            distributions. </p>
        <p>from statsmodels.graphics.gofplots import qqplot</p>
        <p><b>Plot the Q-Q plot of the residuals. </b></p>
        <p>qqplot(residuals, line='45'); </p>
        <p><b>Specify the display of the line y = x. </b></p>
        <p><a id="calibre_link-366"></a><b>122</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Ready for</p>
        <p>Figure 6.11</p>
        <p>General modeling procedure </p>
        <p>forecasts</p>
        <p>for an ARMA( <i><b>p</b></i>, <i><b>q</b></i>) process</p>
        <p>The result is shown in figure 6.12. You will see a thick straight line that approximately
            lies on <i>y</i> = <i>x</i>. Therefore, from a qualitative standpoint,
            the
            model's residuals seem to be normally distributed, just like white noise, which is an indication that
            our
            model fits the data well. </p>
        <p><a id="calibre_link-193"></a><img src="images/000140.jpg" alt="Image 60" class="calibre2" />
        </p>
        <p> <i><b>6.4</b></i></p>
        <p> <i><b>Devising a general modeling procedure</b></i></p>
        <p><b>123</b></p>
        <p>Figure 6.12</p>
        <p>Q-Q plot of our ARMA(1,1) residuals. You can see a thick straight </p>
        <p>line lying on <i><b>y</b></i> = <i><b>x</b></i>. This means that our
            residuals are normally distributed, just like
            white
            noise. </p>
        <p>We'll extend our qualitative analysis by using the plot_diagnostics method. This generates a
            figure containing four different plots, including a Q-Q plot. </p>
        <p>model_fit.plot_diagnostics(figsize=(10, 8)); </p>
        <p>The result is shown in figure 6.13. You can see how statsmodels makes it easy for us to
            qualitatively analyze the residuals. </p>
        <p>The top-left plot shows the residuals across the entire dataset. You can see that there is
            no trend, and the mean seems stable over time, which is indicative of stationarity, just like white
            noise.
        </p>
        <p>The top-right plot shows a histogram of the residuals. You can see the shape of a normal
            distribution on this plot, which again indicates that the residuals are close to white noise, as white
            noise
            is normally distributed as well. </p>
        <p>At the bottom left, we have the Q-Q plot, which is identical to figure 6.12, and therefore
            leads us to the same conclusion. </p>
        <p>Finally, the bottom-right plot shows the autocorrelation function of our residuals. You can
            see that there is only a significant peak at lag 0, and no significant coefficients otherwise. This
            means
            that the residuals are not correlated, which further supports the conclusion that they are similar to
            white
            noise, which is what we expect from a good model. </p>
        <p>The final step in residual analysis is applying the Ljung-Box test. This allows us to
            quantitatively assess whether our residuals are indeed uncorrelated. We will use the acorr_ljungbox
            function
            from statsmodels to perform the Ljung-Box test on the residuals. The function takes as input the
            residuals
            as well as a list of lags. Here we will compute the Ljung-Box statistic and p-value for 10 lags. </p>
        <p><a id="calibre_link-226"></a><img src="images/000142.jpg" alt="Image 61" class="calibre2" />
        </p>
        <p><b>124</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Figure 6.13</p>
        <p>Model diagnostics from <b>statsmodels</b>. The top-left plot displays the
            residuals, the histogram of the residuals is at the top right, the Q-Q plot of the residuals is at the
            bottom left, and the bottom right shows the ACF plot of the residuals. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1)) </p>
        <p><b>Display the p-value </b></p>
        <p><b>Apply the Ljung-Box test on</b></p>
        <p>print(pvalue) </p>
        <p><b>for each lag. </b></p>
        <p><b>the residuals, on 10 lags. </b></p>
        <p>The resulting list of p-values shows that each is above 0.05. Therefore, at each lag, the
            null hypothesis cannot be rejected, meaning that the residuals are independently distributed and
            uncorrelated. </p>
        <p>We can conclude from our analysis that the residuals are similar to white noise. </p>
        <p>The Q-Q plot showed a straight line, meaning that the residuals are normally distributed.
            Furthermore, the Ljung-Box test shows that the residuals are uncorrelated, just like white noise. Thus,
            the
            residuals are completely random, meaning that we have a model that fits our data well. </p>
        <p>Now let's apply the same modeling procedure to the bandwidth dataset. </p>
        <p><a id="calibre_link-78"></a> <i><b>6.5</b></i></p>
        <p> <i><b>Applying the general modeling procedure</b></i></p>
        <p><b>125</b></p>
        <p> <i><b>6.5</b></i></p>
        <p> <i><b>Applying the general modeling procedure</b></i></p>
        <p>We now have a general modeling procedure that allows us to model and forecast a general
            ARMA( <i>p</i>, <i>q</i>) model, as outlined in figure 6.14. We
            applied
            this procedure to our simulated ARMA(1,1) process and found that the best fit was an ARMA(1,1) model, as
            expected. </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 6.14</p>
        <p>General modeling procedure </p>
        <p>Ready for</p>
        <p>for an ARMA(</p>
        <p>forecasts</p>
        <p> <i><b>p</b></i>, <i><b>q</b></i>)
            process</p>
        <p><a id="calibre_link-367"></a><img src="images/000130.jpg" alt="Image 62" class="calibre2" />
        </p>
        <p><b>126</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Now we can apply the same procedure on the bandwidth dataset to obtain the best model
            possible for this situation. Recall that our objective is to forecast bandwidth usage for the next 2
            hours.
        </p>
        <p>The first step is to gather and load the data using pandas:</p>
        <p>import pandas as pd</p>
        <p>df = pd.read_csv('data/bandwidth.csv')</p>
        <p>We can then plot our time series and look for a trend or a seasonal pattern. By now, you
            should be comfortable with plotting your time series. The result is shown in figure 6.15. </p>
        <p>import matplotlib.pyplot as plt</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df.hourly_bandwidth)</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Hourly bandwith usage (MBps)')</p>
        <p>plt.xticks(</p>
        <p>np.arange(0, 10000, 730), </p>
        <p>['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', </p>
        <p>➥ 'Nov', 'Dec', '2020', 'Feb'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 6.15</p>
        <p>Hourly bandwidth usage in a data center since January 1, 2019. The </p>
        <p>dataset contains 10,000 points. </p>
        <p><a id="calibre_link-205"></a> <i><b>6.5</b></i></p>
        <p> <i><b>Applying the general modeling procedure</b></i></p>
        <p><b>127</b></p>
        <p>With the data plotted in figure 6.15, you can see that there is no periodic pattern in the
            data. However, you'll notice the presence of a long-term trend, meaning that our data is likely not
            stationary. Let's apply the ADF test to verify our hypothesis. Again, we'll use the adfuller function
            from
            statsmodels and print out the ADF statistic and the p-value. </p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p>ADF_result = adfuller(df['hourly_bandwidth'])</p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p>This prints out an ADF statistic of &ndash;0.8 and a p-value of 0.80. Therefore, we cannot
            reject the null hypothesis, meaning that our time series is not stationary. </p>
        <p>We must apply a transformation to our data in order to make it stationary. Let's apply a
            first-order differencing using numpy. </p>
        <p>import numpy as np</p>
        <p>bandwidth_diff = np.diff(df.hourly_bandwidth, n=1)</p>
        <p>With this done, we can apply the ADF test again, this time on the differenced data, in order
            to test for stationarity. </p>
        <p>ADF_result = adfuller(bandwidth_diff)</p>
        <p>print(f'ADF Statistic: {ADF_result[0]}')</p>
        <p>print(f'p-value: {ADF_result[1]}')</p>
        <p>This returns an ADF statistic of &ndash;20.69 and a p-value of 0.0. With a large, negative
            ADF</p>
        <p>statistic and a p-value that is much smaller than 0.05, we can say that our differenced
            series is stationary. </p>
        <p>We are now ready to start modeling our stationary process using an ARMA( <i>p</i>, <i class="calibre3">q</i>)
            model. We'll split our series into train and test
            sets. Here we'll keep the last 7 days of data for the test set. Since our forecasts are for the next 2
            hours, the test set thus contains 84 periods of 2 hours on which to evaluate our models' performance,
            since
            7 days of hourly data totals 168 hours. </p>
        <p>df_diff = pd.DataFrame({'bandwidth_diff': bandwidth_diff})</p>
        <p>train = df_diff[:-168]</p>
        <p>test = df_diff[-168:] </p>
        <p><b>There are 168 hours in a week, </b></p>
        <p><b>so we will assign the last 168 </b></p>
        <p>print(len(train))</p>
        <p><b>data points to the test set. </b></p>
        <p>print(len(test))</p>
        <p>We can print out the length of the train and test sets as a sanity check, and sure enough,
            the test set has 168 data points, and the train set has 9,831 data points. </p>
        <p>Now let's visualize our train set and test set for both the differenced and original series.
            The resulting plot is shown in figure 6.16. </p>
        <p><a id="calibre_link-368"></a><img src="images/000128.jpg" alt="Image 63" class="calibre2" />
        </p>
        <p><b>128</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, sharex=True, figsize=(10, </p>
        <p>➥ 8))</p>
        <p>ax1.plot(df.hourly_bandwidth)</p>
        <p>ax1.set_xlabel('Time')</p>
        <p>ax1.set_ylabel('Hourly bandwidth')</p>
        <p>ax1.axvspan(9831, 10000, color='#808080', alpha=0.2)</p>
        <p>ax2.plot(df_diff.bandwidth_diff)</p>
        <p>ax2.set_xlabel('Time')</p>
        <p>ax2.set_ylabel('Hourly bandwidth (diff)')</p>
        <p>ax2.axvspan(9830, 9999, color='#808080', alpha=0.2)</p>
        <p>plt.xticks(</p>
        <p>np.arange(0, 10000, 730), </p>
        <p>['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', </p>
        <p>➥ 'Nov', 'Dec', '2020', 'Feb'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 6.16</p>
        <p>Train and test sets for the original and differenced series</p>
        <p><a id="calibre_link-195"></a> <i><b>6.5</b></i></p>
        <p> <i><b>Applying the general modeling procedure</b></i></p>
        <p><b>129</b></p>
        <p>With our train set ready, we can now fit different ARMA( <i>p</i>, <i class="calibre3">q</i>) models using
            the optimize_</p>
        <p>ARMA function that we defined earlier. Remember that the function takes the data and the
            list of unique ( <i>p</i>, <i>q</i>) combinations as input. Inside the
            function, we initialize an empty list to store each ( <i>p</i>, <i>q</i>)
            combination and its corresponding AIC. Then we iterate over each ( <i>p</i>, <i class="calibre3">q</i>)
            combination and fit an ARMA( <i>p</i>, <i class="calibre3">q</i>) model on our data. We compute the AIC and
            store the result. Then we create a
            DataFrame and sort it by AIC value in ascending order, since the lower the AIC, the better the model.
            Our
            function finally outputs the ordered DataFrame so we can select the appropriate model. The optimize_</p>
        <p>ARMA function is shown in the following listing. </p>
        <p>Listing 6.2</p>
        <p>Function to fit all unique ARMA( <i><b>p</b></i>, <i><b>q</b></i>) models
        </p>
        <p><b>The function takes as inputs the time series </b></p>
        <p><b>data and the list of unique (p,q) combinations. </b></p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_ARMA(endog: Union[pd.Series, list], order_list: list) -&gt; </p>
        <p>➥ pd.DataFrame: </p>
        <p></p>
        <p><b>Initialize an empty list to store the order </b></p>
        <p><b>Iterate over each unique </b></p>
        <p>results = [] </p>
        <p><b>(p,q) and its corresponding AIC as a tuple. </b></p>
        <p><b>(p,q) combination. The use </b></p>
        <p></p>
        <p><b>of tqdm_notebook will </b></p>
        <p><b>display a progress bar. </b></p>
        <p>for order in tqdm_notebook(order_list): </p>
        <p>try: </p>
        <p>model = SARIMAX(endog, order=(order[0], 0, order[1]), </p>
        <p>➥ simple_differencing=False).fit(disp=False) </p>
        <p><b>Fit an ARMA(p,q) model </b></p>
        <p>except:</p>
        <p><b>using the SARIMAX </b></p>
        <p><b>Append the </b> </p>
        <p><b>(p,q)</b></p>
        <p>continue</p>
        <p><b>Calculate the </b></p>
        <p><b>function. We specify </b></p>
        <p><b>combination </b> </p>
        <p><b>and</b></p>
        <p></p>
        <p><b>model's AIC. </b></p>
        <p><b>simple_differencing=False </b></p>
        <p><b>AIC as a </b></p>
        <p></p>
        <p><b>tuple to</b></p>
        <p>aic = model.aic </p>
        <p><b>to prevent differencing. We </b></p>
        <p><b>the results </b> </p>
        <p><b>list. </b></p>
        <p>results.append([order, aic]) </p>
        <p><b>also specify disp=False to </b></p>
        <p></p>
        <p><b>avoid printing convergence </b></p>
        <p>result_df = pd.DataFrame(results) </p>
        <p><b>messages to the console. </b></p>
        <p>result_df.columns = ['(p,q)', 'AIC'] </p>
        <p><b>Label the </b> </p>
        <p><b>Store the (p,q) </b></p>
        <p><b>columns of the </b> #Sort in ascending order, lower AIC is better</p>
        <p><b>combination and AIC </b></p>
        <p><b>DataFrame. </b> result_df = result_df.sort_values(by='AIC', </p>
        <p><b>in a DataFrame. </b></p>
        <p>➥ ascending=True).reset_index(drop=True) </p>
        <p></p>
        <p><b>Sort the DataFrame in ascending </b></p>
        <p>return result_df</p>
        <p><b>order of AIC value. The lower the </b></p>
        <p><b>AIC, the better the model. </b></p>
        <p>Here we will try values for <i>p</i> and <i>q</i> ranging
            from 0 to 3 inclusively. This means that we will fit 16 unique ARMA( <i>p</i>, <i class="calibre3">q</i>)
            models to our training set and select the one with the lowest AIC. Feel free
            to
            change the range of values for <i>p</i> and <i>q</i>, but keep in mind
            that a larger range will result in more models being fit and a longer computation time. </p>
        <p>Also, you don't need to worry about overfitting&mdash;we are selecting our model using the
            AIC, which will prevent us from selecting a model that overfits. </p>
        <p><a id="calibre_link-369"></a><img src="images/000051.jpg" alt="Image 64" class="calibre2" />
        </p>
        <p><b>130</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>ps = range(0, 4, 1) </p>
        <p><b>The order p can have the values {0,1,2,3}. </b></p>
        <p>qs = range(0, 4, 1) </p>
        <p><b>The order q can have the values {0,1,2,3}. </b></p>
        <p>order_list = list(product(ps, qs)) </p>
        <p><b>Generate the unique (p,q) combinations. </b></p>
        <p>With this step done, we can pass in our training set and the list of unique ( <i>p</i>,
            <i>q</i>) combinations to the optimize_ARMA function. </p>
        <p>result_df = optimize_ARMA(train['bandwidth_diff'], order_list)</p>
        <p>result_df</p>
        <p>The resulting DataFrame is shown in figure 6.17. You'll notice that the first three models
            all have an AIC of 27,991, with only slight differences. Therefore, I would argue that the ARMA(2,2)
            model
            is the model that should be selected. Its AIC value is very close to the ARMA(3,2) and ARMA(2,3) models,
            while being less complex, since it has four parameters to be estimated instead of five. Therefore, we'll
            select the ARMA(2,2) model and move on to the next steps, which is the analysis of the model's
            residuals.
        </p>
        <p>Figure 6.17</p>
        <p>A <b>DataFrame</b> ordered by </p>
        <p>ascending value of AIC, resulting from fitting </p>
        <p>different ARMA( <i><b>p</b></i>, <i><b>q</b></i>) models on the differenced
        </p>
        <p>bandwidth dataset. Notice how the first three </p>
        <p>models all have an AIC value of 27,991. </p>
        <p>To perform the residual analysis, we'll fit the ARMA(2,2) model on our training set. </p>
        <p>Then we'll use the plot_diagnostics method to study the Q-Q plot, as well as the other
            accompanying plots. The result is shown in figure 6.18. </p>
        <p><a id="calibre_link-370"></a><img src="images/000123.jpg" alt="Image 65" class="calibre2" />
        </p>
        <p> <i><b>6.5</b></i></p>
        <p> <i><b>Applying the general modeling procedure</b></i></p>
        <p><b>131</b></p>
        <p>model = SARIMAX(train['bandwidth_diff'], order=(2,0,2), </p>
        <p>➥ simple_differencing=False)</p>
        <p>model_fit = model.fit(disp=False)model_fit = best_model.fit(disp=False)</p>
        <p>model_fit.plot_diagnostics(figsize=(10, 8)); </p>
        <p>Figure 6.18</p>
        <p>Model diagnostics from <b>statsmodels</b>. The top-left plot displays the
            residuals, the histogram of the residuals is at the top right, the Q-Q plot of the residuals is at the
            bottom left, and the bottom right shows the ACF plot of the residuals. </p>
        <p>In figure 6.18 you can see that the top-left plot shows no trend, and the mean seems
            constant over time, meaning that our residuals are likely stationary. The top right displays a density
            plot
            with a shape similar to that of a normal distribution. The Q-Q plot at the bottom left shows a thick
            straight line that is very close to <i>y</i> = <i>x</i>. Finally, the
            ACF
        </p>
        <p>plot at the bottom right shows no autocorrelation after lag 0. Thus, figure 6.18 indicates
            that our residuals clearly resemble white noise, since they are normally distributed and uncorrelated.
        </p>
        <p>Our last step is to run the Ljung-Box test on the residuals for the first 10 lags. If the
            returned p-values exceed 0.05, we cannot reject the null hypothesis, which</p>
        <p><a id="calibre_link-79"></a><b>132</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>means that our residuals are uncorrelated and independently distributed, just like white
            noise. </p>
        <p>residuals = model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p>The returned p-values all exceed 0.05. Therefore, we can conclude that our residuals are
            indeed uncorrelated. Our ARMA(2,2) model has passed all the checks on the residual analysis, and we are
            ready to use this model to forecast bandwidth usage. </p>
        <p> <i><b>6.6</b></i></p>
        <p> <i><b>Forecasting bandwidth usage</b></i></p>
        <p>In the previous section, we applied the general modeling procedure on the bandwidth dataset
            and concluded than an ARMA(2,2) model was the best model for our</p>
        <p>data. Now we will use the ARMA(2,2) model to forecast the next 2 hours of bandwidth usage
            over 7 days. </p>
        <p>We will reuse the rolling_forecast function that we defined and used in chap-</p>
        <p>ters 4 and 5, as shown in listing 6.3. Recall that this function allows us to forecast a few
            timesteps at a time, until we have forecasts for the entire horizon. This time, of course, we'll fit an
            ARMA(2,2) model to our differenced data. Also, we'll compare the model's performance to two benchmarks:
            the
            mean and the last known value. This will allow us to make sure that an ARMA(2,2) model performs better
            than
            naive forecasting methods. </p>
        <p>Listing 6.3</p>
        <p>A function to perform a rolling forecast on a horizon</p>
        <p>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, </p>
        <p>➥ window: int, method: str) -&gt; list:</p>
        <p></p>
        <p>total_len = train_len + horizon</p>
        <p>end_idx = train_len</p>
        <p></p>
        <p>if method == 'mean':</p>
        <p>pred_mean = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>mean = np.mean(df[:i].values)</p>
        <p>pred_mean.extend(mean for _ in range(window))</p>
        <p></p>
        <p>return pred_mean</p>
        <p>elif method == 'last':</p>
        <p>pred_last_value = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>last_value = df[:i].iloc[-1].values[0]</p>
        <p>pred_last_value.extend(last_value for _ in range(window))</p>
        <p><a id="calibre_link-371"></a> <i><b>6.6</b></i></p>
        <p> <i><b>Forecasting bandwidth usage</b></i></p>
        <p><b>133</b></p>
        <p>return pred_last_value</p>
        <p></p>
        <p>elif method == 'ARMA':</p>
        <p>pred_ARMA = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p><b>The order specifies </b></p>
        <p>model = SARIMAX(df[:i], order=(2,0,2)) </p>
        <p><b>an ARMA(2,2) model. </b></p>
        <p>res = model.fit(disp=False)</p>
        <p>predictions = res.get_prediction(0, i + window - 1)</p>
        <p>oos_pred = predictions.predicted_mean.iloc[-window:]</p>
        <p>pred_ARMA.extend(oos_pred)</p>
        <p></p>
        <p>return pred_ARMA</p>
        <p>With rolling_forecast defined, we can use it to evaluate the performance of the different
            forecasting methods. We'll first create a DataFrame to hold the actual values of the test set as well as
            the
            predictions from the different methods. Then we'll specify the size of the train and test sets. We will
            predict two steps at a time, because we have an ARMA(2,2) model, meaning that there is an MA(2)
            component.
            We know from chapter 4 that predicting beyond <i>q</i> steps into the future with an
            MA( <i>q</i>) model will simply return the mean, so the predictions will remain flat.
            We'll
            therefore avoid this situation by setting the window to 2. We can then forecast on the test set using
            the
            mean method, the last known value method, and the ARMA(2,2) model, and store each forecast in its
            appropriate column in test. </p>
        <p>pred_df = test.copy()</p>
        <p>TRAIN_LEN = len(train)</p>
        <p>HORIZON = len(test)</p>
        <p>WINDOW = 2</p>
        <p>pred_mean = recursive_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'mean')</p>
        <p>pred_last_value = recursive_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'last')</p>
        <p>pred_ARMA = recursive_forecast(df_diff, TRAIN_LEN, HORIZON, WINDOW, 'ARMA')</p>
        <p>test.loc[:, 'pred_mean'] = pred_mean</p>
        <p>test.loc[:, 'pred_last_value'] = pred_last_value</p>
        <p>test.loc[:, 'pred_ARMA'] = pred_ARMA</p>
        <p>pred_df.head()</p>
        <p>We can then plot and visualize the forecasts for each method. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df_diff['bandwidth_diff'])</p>
        <p>ax.plot(test['bandwidth_diff'], 'b-', label='actual')</p>
        <p>ax.plot(test['pred_mean'], 'g:', label='mean')</p>
        <p>ax.plot(test['pred_last_value'], 'r-.', label='last')</p>
        <p>ax.plot(test['pred_ARMA'], 'k--', label='ARMA(2,2)')</p>
        <p>ax.legend(loc=2)</p>
        <p><a id="calibre_link-372"></a><img src="images/000055.jpg" alt="Image 66" class="calibre2" />
        </p>
        <p><b>134</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Hourly bandwidth (diff)')</p>
        <p><b>Assign a gray </b></p>
        <p><b>background for </b></p>
        <p><b>the testing period. </b></p>
        <p>ax.axvspan(9830, 9999, color='#808080', alpha=0.2) </p>
        <p><b>Zoom in on the </b></p>
        <p>ax.set_xlim(9800, 9999) </p>
        <p><b>testing period. </b></p>
        <p>plt.xticks(</p>
        <p>[9802, 9850, 9898, 9946, 9994], </p>
        <p>['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>The results are shown in figure 6.19. I've zoomed in on the testing period for a better
            visualization. </p>
        <p>Figure 6.19</p>
        <p>Forecasts of the differenced hourly bandwidth usage using the mean, the </p>
        <p>last known value, and an ARMA(2,2) model. You can see how the ARMA(2,2) forecasts and last
            known value forecasts almost coincide with the actual values of the test set. </p>
        <p>In figure 6.19 you can see that the ARMA(2,2) forecasts, shown as a dashed line, almost
            coincide with the actual values of the test set. The same can be said of the forecasts from the last
            known
            value method, shown as a dashed and dotted line. Of course, the forecasts using the mean, shown as a
            dotted
            line, are completely flat over the testing period. </p>
        <p>We'll now measure the mean squared error (MSE) to evaluate the performance of</p>
        <p>each model. The model with the lowest MSE is the best-performing model. </p>
        <p><a id="calibre_link-373"></a> <i><b>6.6</b></i></p>
        <p> <i><b>Forecasting bandwidth usage</b></i></p>
        <p><b>135</b></p>
        <p>mse_mean = mean_squared_error(test['bandwidth_diff'], test['pred_mean'])</p>
        <p>mse_last = mean_squared_error(test['bandwidth_diff'], </p>
        <p>➥ test['pred_last_value'])</p>
        <p>mse_ARMA = mean_squared_error(test['bandwidth_diff'], test['pred_ARMA'])</p>
        <p>print(mse_mean, mse_last, mse_ARMA) </p>
        <p>This returns an MSE of 6.3 for the mean method, 2.2 for the last known value method, and 1.8
            for the ARMA(2,2) model. The ARMA(2,2) model outperforms the benchmarks, meaning that we have a
            well-performing model. </p>
        <p>The final step is to reverse the transformation of our forecast in order to bring it to the
            same scale as our original data. Remember that we differenced the original data to make it stationary.
            The
            ARMA(2,2) model was then applied on the stationary dataset and produced forecasts that are differenced.
        </p>
        <p>To reverse the differencing transformation, we can apply a cumulative sum, just as we did in
            chapters 4 and 5. </p>
        <p>df['pred_bandwidth'] = pd.Series()</p>
        <p>df['pred_bandwidth'][9832:] = df['hourly_bandwidth'].iloc[9832] + </p>
        <p>➥ pred_df['pred_ARMA'].cumsum()</p>
        <p>We can then plot the forecasts on the original scale of the data. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['hourly_bandwidth'])</p>
        <p>ax.plot(df['hourly_bandwidth'], 'b-', label='actual')</p>
        <p>ax.plot(df['pred_bandwidth'], 'k--', label='ARMA(2,2)')</p>
        <p>ax.legend(loc=2)</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Hourly bandwith usage (MBps)')</p>
        <p>ax.axvspan(9831, 10000, color='#808080', alpha=0.2)</p>
        <p>ax.set_xlim(9800, 9999)</p>
        <p>plt.xticks(</p>
        <p>[9802, 9850, 9898, 9946, 9994], </p>
        <p>['2020-02-13', '2020-02-15', '2020-02-17', '2020-02-19', '2020-02-21'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Looking at the results in figure 6.20, you can see that our forecasts, shown as a dashed
            line, closely follow the actual values of the test set, and the two lines almost coincide. </p>
        <p><a id="calibre_link-80"></a><img src="images/000171.jpg" alt="Image 67" class="calibre2" />
        </p>
        <p><b>136</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>Figure 6.20</p>
        <p>Undifferenced predictions of hourly bandwidth usage. Notice how the </p>
        <p>dashed line representing our predictions almost coincides with the solid line </p>
        <p>representing the actual values. This means that our predictions are very close to the actual
            values, indicating a performant model. </p>
        <p>We can measure the mean absolute error (MAE) of the undifferenced ARMA(2,2) predictions to
            understand how far apart the predictions are from the actual values. </p>
        <p>We'll use the MAE simply because it is easy to interpret. </p>
        <p>mae_ARMA_undiff = mean_absolute_error(df['hourly_bandwidth'][9832:], </p>
        <p>➥ df['pred_bandwidth'][9832:])</p>
        <p>print(mae_ARMA_undiff)</p>
        <p>This returns an MAE of 14, meaning that, on average, our forecasts are 14 Mbps above or
            below the actual bandwidth usage. </p>
        <p> <i><b>6.7</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the ARMA( <i>p</i>, <i>q</i>)
            model and how it effectively combines an AR( <i>p</i>) model with an MA( <i class="calibre3">q</i>) model to
            model and forecast more complex time series. </p>
        <p>This required us to define an entirely new modeling procedure that does not rely on the
            qualitative study of the ACF and PACF plots. Instead, we fit many ARMA( <i>p</i>, <i class="calibre3">q</i>)
            models with different ( <i>p</i>, <i>q</i>)
            combinations and selected the model with the lowest AIC. </p>
        <p>Then we analyzed the model's residuals to make sure that their properties were similar to
            white noise: normally distributed, stationary, and uncorrelated. This analysis is both qualitative,
            because
            we can study the Q-Q plot to evaluate whether the residuals are normally distributed, as well as
            quantitative, since we can apply the Ljung-Box test</p>
        <p><a id="calibre_link-81"></a> <i><b>6.8</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p><b>137</b></p>
        <p>to determine whether the residuals are correlated or not. If the model's residuals have the
            properties of a random variable, like white noise, the model can be used for forecasting. </p>
        <p>So far we have covered different models for stationary time series: mainly the MA( <i>q</i>)
            model, AR( <i>p</i>) model, and ARMA( <i>p</i>, <i>q</i>)
            model. Each model required us to transform our data
            to make it stationary before we could forecast. Furthermore, we had to reverse the transformation on our
            forecast to obtain predictions in the original scale of the data. </p>
        <p>However, there is a way to model non-stationary time series without having to transform them
            and reverse the transformation on the predictions. Specifically, we can model <i
                class="calibre3">integrated</i> time series using the <i>autoregressive integrated
                moving average</i> model or ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>). This will be the
            subject of the next chapter. </p>
        <p> <i><b>6.8</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>It is time to test your knowledge and apply the general modeling procedure with these
            exercises. The solutions are available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH06">https://github.com/marcopeix/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH06">TimeSeriesForecastingInPython/tree/master/CH06.
            </a></p>
        <p> <i><b>6.8.1</b></i></p>
        <p> <i><b>Make predictions on the simulated ARMA(1,1)
                    process</b></i></p>
        <p>1</p>
        <p>Reusing the simulated ARMA(1,1) process, split it into train and test sets. </p>
        <p>Assign 80% of the data to the train set and the remaining 20% to the test set. </p>
        <p>2</p>
        <p>Use the rolling_forecast function to make predictions using the ARMA(1,1)</p>
        <p>model, the mean method, and the last known value method. </p>
        <p>3</p>
        <p>Plot your forecasts. </p>
        <p>4</p>
        <p>Evaluate each method's performance using the MSE. Which method per-</p>
        <p>formed best? </p>
        <p> <i><b>6.8.2</b></i></p>
        <p> <i><b>Simulate an ARMA(2,2) process and make
                    forecasts</b></i></p>
        <p>Simulate a stationary ARMA(2,2) process. Use the ArmaProcess function from statsmodels and
            simulate this:</p>
        <p> <i>yt </i> = 0.33 <i>y<sub>t-1</sub></i> + 0.50 <i>yt
            </i>&ndash;2 + 0.9ϵ <i>t </i>&ndash;1 + 0.3ϵ <i>t
            </i>&ndash;2</p>
        <p>1</p>
        <p>Simulate 10,000 samples. </p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p>import numpy as np</p>
        <p><b>Set the seed for reproducibility. </b></p>
        <p><b>Change the seed if you want </b></p>
        <p>np.random.seed(42) </p>
        <p><b>to experiment with different </b></p>
        <p><b>values. </b></p>
        <p>ma2 = np.array([1, 0.9, 0.3])</p>
        <p>ar2 = np.array([1, -0.33, -0.5])</p>
        <p>ARMA_2_2 = ArmaProcess(ar2, ma2).generate_sample(nsample=10000)</p>
        <p><a id="calibre_link-314"></a><b>138</b></p>
        <p>CHAPTER 6</p>
        <p> <i><b>Modeling complex time series</b></i></p>
        <p>2</p>
        <p>Plot your simulated process. </p>
        <p>3</p>
        <p>Test for stationarity using the ADF test. </p>
        <p>4</p>
        <p>Split your data into train and test sets. The test set must contain the last 200</p>
        <p>timesteps. The rest is for the train set. </p>
        <p>5</p>
        <p>Define a range of values for <i>p</i> and <i>q</i>, and
            generate all unique combinations of orders ( <i>p</i>, <i>q</i>). </p>
        <p>6</p>
        <p>Use the optimize_ARMA function to fit all unique ARMA( <i>p</i>, <i>q</i>)
            models, and select the one with the lowest AIC. Is the ARMA(2,2) model the one
            with the</p>
        <p>lowest AIC? </p>
        <p>7</p>
        <p>Select the best model according to the AIC, and store the residuals in a variable called
            residuals. </p>
        <p>8</p>
        <p>Perform a qualitative analysis of the residuals with the plot_diagnostics method. Does the
            Q-Q plot show a straight line that lies on <i>y</i> = <i>x</i>? Does
            the
            correlogram show significant coefficients? </p>
        <p>9</p>
        <p>Perform a quantitative analysis of the residuals by applying the Ljung-Box test</p>
        <p>on the first 10 lags. Are all returned p-values above 0.05? Are the residuals correlated or
            not? </p>
        <p>10</p>
        <p>Use the rolling_forecast function to make predictions using the selected ARMA( <i>p</i>,
            <i>q</i>) model, the mean method, and the last known value
            method. </p>
        <p>11</p>
        <p>Plot your forecasts. </p>
        <p>12</p>
        <p>Evaluate each method's performance using the MSE. Which method per-</p>
        <p>formed best? </p>
        <p> <i><b>Summary</b></i></p>
        <p> The autoregressive moving average model, denoted as ARMA( <i>p</i>, <i class="calibre3">q</i>), is the
            combination of the autoregressive model AR( <i>p</i>)
            and the moving average model MA( <i>q</i>). </p>
        <p> An ARMA( <i>p</i>, <i>q</i>) process will display a
            decaying pattern or a sinusoidal pattern on both the ACF and PACF plots. Therefore, they cannot be used
            to
            estimate the</p>
        <p>orders <i>p</i> and <i>q</i>. </p>
        <p> The general modeling procedure does not rely on the ACF and PACF plots. </p>
        <p>Instead, we fit many ARMA( <i>p</i>, <i>q</i>) models and
            perform model selection and residual analysis. </p>
        <p> Model selection is done with the Akaike information criterion (AIC). It quantifies the
            information loss of a model, and it is related to the number of parame-</p>
        <p>ters in a model and its goodness of fit. The lower the AIC, the better the model. </p>
        <p> The AIC is relative measure of quality. It returns the best model among other</p>
        <p>models. For an absolute measure of quality, we perform residual analysis. </p>
        <p> Residuals of a good model must approximate white noise, meaning that they must be
            uncorrelated, normally distributed, and independent. </p>
        <p> The Q-Q plot is a graphical tool for comparing two distributions. We use it to compare the
            distribution of the residuals against a theoretical normal distribution. </p>
        <p><a id="calibre_link-224"></a> <i><b>Summary</b></i></p>
        <p><b>139</b></p>
        <p>If the plot shows a straight line that lies on <i>y</i> = <i>x</i>,
            then
            both distributions are similar. Otherwise, it means that the residuals are
            not normally distributed. </p>
        <p> The Ljung-Box test allows us to determine whether the residuals are correlated or not. The
            null hypothesis states that the data is independently distributed and uncorrelated. If the returned
            p-values
            are larger than 0.05, we cannot reject the null hypothesis, meaning that the residuals are uncorrelated,
            just like white noise. </p>
        <p><a id="calibre_link-9"></a> <i>Forecasting</i></p>
        <p> <i>non-stationary time series</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the autoregressive integrated moving </p>
        <p>average model, or ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
        </p>
        <p> Applying the general modeling procedure for non-</p>
        <p>stationary time series</p>
        <p> Forecasting using the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model</p>
        <p>In chapters 4, 5, and 6 we covered the moving average model, MA( <i>q</i>);
            the autoregressive model, AR( <i>p</i>); and the ARMA model, ARMA( <i class="calibre3">p</i>, <i>q</i>). We
            saw how these models can only be used for
            stationary time series, which required us to apply transformations, mainly differencing, and test for
            stationarity using the ADF test. In the examples that we covered, the forecasts from each model returned
            differenced values, which required us to reverse this transformation in order to bring the values back
            to
            the scale of the original data. </p>
        <p>Now we'll add another component to the ARMA( <i>p</i>, <i>q</i>) model
            so
            we can forecast non-stationary time series. This component is the <i>integration
                order</i>,
            which is denoted by the variable <i>d</i>. This
            leads us to the <i>autoregressive integrated moving average</i> (ARIMA) model, or
            ARIMA( <i>p</i>, <i>d</i>, <i>q</i>). Using this
            model, we can
            take into account non-stationary time series and avoid the steps of modeling on differenced data and
            having
            to inverse transform the forecasts. </p>
        <p><b>140</b></p>
        <p><a id="calibre_link-374"></a> <i><b>Forecasting
                    non-stationary time series</b></i></p>
        <p><b>141</b></p>
        <p>In this chapter, we'll define the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model and the order
            of integration <i>d</i>. </p>
        <p>Then we'll add a step to our general modeling procedure. Figure 7.1 shows the general
            modeling procedure as defined in chapter 6. We must add a step to determine the order of integration in
            order to use this procedure with the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model. </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 7.1</p>
        <p>General modeling procedure using </p>
        <p>an ARMA( <i><b>p</b></i>, <i><b>q</b></i>) model. In this chapter, we will
        </p>
        <p>Ready for</p>
        <p>add another step to this procedure in order to </p>
        <p>forecasts</p>
        <p>accommodate the ARIMA( <i><b>p</b></i>, <i><b>d</b></i>, <i><b class="calibre4">q</b></i>)
            model. </p>
        <p><a id="calibre_link-82"></a><img src="images/000073.jpg" alt="Image 68" class="calibre2" />
        </p>
        <p><b>142</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>Then we'll apply our modified procedure to forecast a non-stationary time series, meaning
            that the series has a trend, or its variance is not constant over time. Specifically, we'll revisit the
            dataset of Johnson &amp; Johnson's quarterly earnings per share (EPS) between 1960 and 1980, which we
            first
            studied in chapters 1 and 2. The series is shown in figure 7.2. We'll apply the ARIMA( <i
                class="calibre3">p</i>, <i>d</i>, <i>q</i>) model to forecast the
            quarterly EPS</p>
        <p>for 1 year. </p>
        <p>Figure 7.2</p>
        <p>Quarterly earnings per share (EPS) of Johnson &amp; Johnson from 1960 to </p>
        <p>1980. We worked with the same dataset in chapters 1 and 2. </p>
        <p> <i><b>7.1</b></i></p>
        <p> <i><b>Defining the autoregressive integrated moving
                </b></i></p>
        <p> <i><b>average model</b></i></p>
        <p>An <i>autoregressive integrated moving average process</i> is the
            combination of an autoregressive process AR( <i>p</i>), integration I( <i class="calibre3">d</i>), and the
            moving average process MA( <i>q</i>). </p>
        <p>Just like the ARMA process, the ARIMA process states that the present value is dependent on
            past values, coming from the AR( <i>p</i>) portion, and past errors, coming from the
            MA( <i>q</i>) portion. However, instead of using the original series, denoted as <i class="calibre3">yt</i>,
            the ARIMA process uses the differenced series, denoted as <i class="calibre3">y't</i>. Note that <i>y't</i>
            can represent a series that has been
            differenced more than once. </p>
        <p>Therefore, the mathematical expression of the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) process
            states that the present value of the
            differenced series <i>y't</i> is equal to the sum of a constant C, past values of the
            differenced series ϕ <i>py't&ndash;p</i>, the mean of the differenced series µ, past
            error
            terms θ ϵ</p>
        <p> <i>q t&ndash;q</i>, and a current error term ϵ <i>t</i>,
            as shown in equation 7.1. </p>
        <p> <i> y' </i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = C + ϕ1 <i>y't </i>&ndash;1 +⋅⋅⋅ ϕ <i class="calibre3">py't&ndash;p</i> + θ1 <i>'t </i>&ndash;1
            +⋅⋅⋅+ θ <i>q
                't &ndash;q</i> + ϵ <i>t</i> Equation 7.1</p>
        <p><a id="calibre_link-83"></a> <i><b>7.2</b></i></p>
        <p> <i><b>Modifying the general modeling procedure to account
                    for non-stationary series</b></i></p>
        <p><b>143</b></p>
        <p>Just like in the ARMA process, the order <i>p</i> determines how many
            lagged values of the series are included in the model, while the order <i>q</i>
            determines
            how many lagged error terms are included in the model. However, in equation 7.1 you'll notice that there
            is
            no order <i>d</i> explicitly displayed. </p>
        <p>Here, the order <i>d </i> is defined as the order of integration.
            Integration is simply the reverse of differencing. The order of integration is thus equal to the number
            of
            times a series has been differenced to become stationary. </p>
        <p>If we difference a series once and it becomes stationary, then <i>d</i> =
            1. If a series is differenced twice to become stationary, then <i>d</i> = 2. </p>
        <p>Autoregressive integrated moving average model</p>
        <p>An <i>autoregressive integrated moving average</i> (ARIMA) process is the
            combination of the AR( <i>p</i>) and MA( <i>q</i>) processes, but in
            terms
            of the differenced series. </p>
        <p>It is denoted as ARIMA( <i>p</i>, <i>d</i>, <i>q</i>),
            where <i>p</i> is the order of the AR( <i>p</i>) process, <i class="calibre3">d</i> is the order of
            integration, and <i>q</i> is the order of
            the
            MA( <i>q</i>) process. </p>
        <p>Integration is the reverse of differencing, and the order of integration <i>d</i> is
            equal
            to the number of times the series has been differenced to be rendered
            stationary. </p>
        <p>The general equation of the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) process is</p>
        <p> <i>y' </i></p>
        <p>ϵ</p>
        <p>ϵ</p>
        <p> <i>t</i> = C + ϕ1 <i>y't</i>&ndash;1 +⋅⋅⋅ ϕ <i>p</i>
            <i>y't&ndash;p</i> + θ1 <i>'t</i>&ndash;1 +⋅⋅⋅+
            θ <i>q 't&ndash;q</i> + ϵ <i>t</i> Note that <i>y't</i>
            represents the differenced series, and it may have been differenced more than once. </p>
        <p>A time series that can be rendered stationary by applying differencing is said to be an <i
                class="calibre3">integrated</i> series. In the presence of a non-stationary integrated time series,
            we
            can use the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            model to
            produce forecasts. </p>
        <p>Thus, in simple terms, the ARIMA model is simply an ARMA model that can be</p>
        <p>applied on non-stationary time series. Whereas the ARMA( <i>p</i>, <i class="calibre3">q</i>) model requires
            the series to be stationary before fitting an ARMA( <i class="calibre3">p</i>, <i>q</i>) model, the ARIMA(
            <i>p</i>, <i class="calibre3">d</i>, <i>q</i>) model can be used on non-stationary series. We
            must
            simply find the order of integration <i>d</i>, which corresponds to the minimum number
            of
            times a series must be differenced to become stationary. </p>
        <p>Therefore, we must add the step of finding the order of integration to our general modeling
            procedure before we apply it to forecast the quarterly EPS of Johnson &amp; Johnson. </p>
        <p> <i><b>7.2</b></i></p>
        <p> <i><b>Modifying the general modeling procedure to account
                </b></i></p>
        <p> <i><b>for non-stationary series</b></i></p>
        <p>In chapter 6 we built a general modeling procedure that allowed us to model more complex
            time series, meaning that the series has both an autoregressive and a moving</p>
        <p><a id="calibre_link-375"></a><b>144</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>average component. This procedure involves fitting many ARMA( <i>p</i>, <i class="calibre3">q</i>) models and
            selecting the one with the lowest AIC. Then we study the model's
            residuals to verify that they resemble white noise. If that is the case, the model can be used for
            forecasting. We can visualize the general modeling procedure in its present state in figure 7.3. </p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARMA ( <i>p</i>, <i>q</i>)</p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 7.3</p>
        <p>General modeling procedure </p>
        <p>using an ARMA( <i><b>p</b></i>, <i><b>q</b></i>) model. Now we must </p>
        <p>adapt it to apply to an ARIMA( <i><b>p</b></i>, <i><b>d</b></i>, <i><b class="calibre4">q</b></i>)
            model, Ready for</p>
        <p>allowing us to work with non-stationary </p>
        <p>forecasts</p>
        <p>time series. </p>
        <p><a id="calibre_link-84"></a> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p><b>145</b></p>
        <p>The next iteration of the general modeling procedure will include a step to determine the
            order of integration <i>d</i>. That way, we can apply the same procedure but using an
            ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) model, which
            will
            allow us to forecast non-stationary time series. </p>
        <p>From the previous section, we know that the order of integration <i>d</i>
            is simply the minimum number of times a series must be differenced to become stationary. Therefore, if a
            series is stationary after being differenced once, then <i>d</i> = 1. If it is
            stationary
            after being differenced twice, then <i>d</i> = 2. In my experience, a time series
            rarely
            needs to be differenced more than twice to become stationary. </p>
        <p>We can add a step such that when transformations are applied to the series, we set the value
            of <i>d</i> to the number of times the series was differenced. Then, instead of fitting
            many ARMA( <i>p</i>, <i>q</i>) models, we fit many ARIMA( <i class="calibre3">p</i>, <i>d</i>, <i>q</i>)
            models. The rest of
            the
            procedure remains the same, as we still use the AIC to select the best model and study its residuals.
            The
            resulting procedure is shown in figure 7.4. </p>
        <p>Note that in the case where <i>d</i> = 0, it is equivalent to an ARMA( <i class="calibre3">p</i>, <i>q</i>)
            model. This also means that the series did not
            need
            to be differenced to be stationary. It must also be specified that the ARMA( <i>p</i>,
            <i>q</i>) model can only be applied on a stationary series, whereas the ARIMA( <i class="calibre3">p</i>,
            <i>d</i>, <i>q</i>) model can be applied
            on a
            series that has not been differenced. </p>
        <p>Let's apply our new general modeling procedure to forecast the quarterly earnings per share
            of Johnson &amp; Johnson. </p>
        <p> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p>We are now going to apply the general modeling procedure displayed in figure 7.4 to forecast
            the quarterly earnings per share (EPS) of Johnson &amp; Johnson. We'll use the same dataset that was
            introduced in chapters 1 and 2. We will forecast 1 year's quarterly EPS, meaning that we must forecast
            four
            timesteps into the future, since there are four quarters in a year. The dataset covers the period
            between
            1960 and 1980. </p>
        <p>As always, the first step is to collect our data. Here it is done for us, so we can simply
            load it and display the series. The result is shown in figure 7.5. </p>
        <p>NOTE</p>
        <p>At any time, feel free to refer to the source for this chapter on GitHub:</p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH07">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH07">CH07</a>. </p>
        <p>df = pd.read_csv('../data/jj.csv')</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df.date, df.data)</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Earnings per share (USD)')</p>
        <p>plt.xticks(np.arange(0, 81, 8), [1960, 1962, 1964, 1966, 1968, 1970, 1972, </p>
        <p>➥ 1974, 1976, 1978, 1980])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p><a id="calibre_link-376"></a><b>146</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
        </p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Figure 7.4</p>
        <p>General modeling procedure for using </p>
        <p>Yes</p>
        <p>the ARIMA( <i><b>p</b></i>, <i><b>d</b></i>, <i><b class="calibre4">q</b></i>) model. Notice the
            addition of a step where we specify the parameter <i><b>d</b></i> for
            the
        </p>
        <p>ARIMA( <i><b>p</b></i>, <i><b>d</b></i>,
            <i><b>q</b></i>) model. Here, <i><b>d</b></i> is simply the Ready for</p>
        <p>minimum number of times a series must be </p>
        <p>forecasts</p>
        <p>differenced to become stationary. </p>
        <p><a id="calibre_link-201"></a><img src="images/000092.jpg" alt="Image 69" class="calibre2" />
        </p>
        <p> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p><b>147</b></p>
        <p>Figure 7.5</p>
        <p>Quarterly earnings per share (EPS) of Johnson &amp; Johnson between 1960 </p>
        <p>and 1980</p>
        <p>Following our procedure, we must check if the data is stationary. Figure 7.5 shows a
            positive trend, as the quarterly EPS tends to increase over time. Nevertheless, we can apply the
            augmented
            Dickey-Fuller (ADF) test to determine if it is stationary or not. </p>
        <p>By now you should be very comfortable with these steps, so they will be accompanied by
            minimal comments. </p>
        <p>ad_fuller_result = adfuller(df['data'])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This block of code returns an ADF statistic of 2.74 with a p-value of 1.0. Since the ADF</p>
        <p>statistic is not a large negative number, and the p-value is larger than 0.05, we cannot
            reject the null hypothesis, meaning that our series is not stationary. </p>
        <p>We need to determine how many times the series must be differenced to become</p>
        <p>stationary. This will then set the order of integration <i>d</i>. We can
            apply a first-order differencing and test for stationarity. </p>
        <p><b>Apply first-order </b></p>
        <p>eps_diff = np.diff(df['data'], n=1) </p>
        <p><b>differencing. </b></p>
        <p>ad_fuller_result = adfuller(eps_diff) </p>
        <p><b>Test for </b></p>
        <p><b>stationarity. </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p><a id="calibre_link-377"></a><img src="images/000152.jpg" alt="Image 70" class="calibre2" />
        </p>
        <p><b>148</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>This results in an ADF statistic of &ndash;0.41 and a p-value of 0.9. Again, the ADF
            statistic is not a large negative number, and the p-value is larger than 0.05. Therefore, we cannot
            reject
            the null hypothesis and we must conclude that after a first-order differencing, the series is not
            stationary. </p>
        <p>Let's try differencing again to see if the series becomes stationary:</p>
        <p><b>Take the differenced series </b></p>
        <p>eps_diff2 = np.diff(eps_diff, n=1) </p>
        <p><b>and difference it again. </b></p>
        <p>ad_fuller_result = adfuller(eps_diff2) </p>
        <p><b>Test for </b></p>
        <p><b>stationarity. </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This results in an ADF statistic of &ndash;3.59 and a p-value of 0.006. Now that we have a
            p-value smaller than 0.05 and a large negative ADF statistic, we can reject the null hypothesis and
            conclude
            that our series is stationary. It took two rounds of differencing to make our data stationary, which
            means
            that our order of integration is 2, so <i>d</i> = 2. </p>
        <p>Before we move on to fitting different combinations of ARIMA( <i>p</i>, <i class="calibre3">d</i>, <i>q</i>)
            models, we must separate our data into train and
            test
            sets. We will hold out the last year of data for testing. This means that we will fit the model with
            data
            from 1960 to 1979 and predict the quarterly EPS in 1980 to evaluate the quality of our model against the
            observed values in 1980. In figure 7.6 the testing period is the shaded area. </p>
        <p>Figure 7.6</p>
        <p>The train and test sets. The training period spans the years 1960 to 1979 </p>
        <p>inclusively, while the test set is the quarterly EPS reported in 1980. This test set
            corresponds to the last four data points of the dataset. </p>
        <p><a id="calibre_link-271"></a> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p><b>149</b></p>
        <p>To fit the many ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            models, we'll define the optimize_ARIMA function. It is almost identical to the
            optimize_ARMA function that we defined in chapter 6, only this time we'll add the order of integration
            <i>d</i> as an input to the function. The remainder of the function stays the same, as
            we
            fit the different models and order them by ascending AIC in order to select the model with the lowest
            AIC.
            The optimize_ARIMA function is shown in the following listing. </p>
        <p>Listing 7.1</p>
        <p>Function to fit all unique ARIMA( <i><b>p</b></i>, <i><b>d</b></i>, <i><b class="calibre4">q</b></i>)
            models <b>The function takes as inputs the time series data, the list of </b></p>
        <p><b>unique (p,q) combinations, and the order of integration d. </b></p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_ARIMA(endog: Union[pd.Series, list], order_list: list, d: int) </p>
        <p>➥ -&gt; pd.DataFrame: <b>Initialize an empty list to </b></p>
        <p></p>
        <p><b>Iterate over each unique </b></p>
        <p><b>store each order (p,q) and its </b></p>
        <p>results = [] </p>
        <p><b>(p,q) combination. The use of </b></p>
        <p><b>corresponding AIC as a tuple. </b></p>
        <p></p>
        <p><b>tqdm_notebook will display </b></p>
        <p><b>a progress bar. </b></p>
        <p>for order in tqdm_notebook(order_list): </p>
        <p>try: </p>
        <p>model = SARIMAX(endog, order=(order[0], d, order[1]), </p>
        <p>➥ simple_differencing=False).fit(disp=False) </p>
        <p><b>Fit an ARIMA(p,d,q) model </b></p>
        <p>except:</p>
        <p><b>using the SARIMAX function. </b></p>
        <p><b>Append the (p,q)</b></p>
        <p>continue</p>
        <p><b>We specify simple_differencing</b></p>
        <p><b>Calculate the</b></p>
        <p><b>combination and AIC as a</b></p>
        <p></p>
        <p><b>=False to prevent differencing. </b></p>
        <p><b>model's AIC. </b></p>
        <p><b>tuple to the results list. </b></p>
        <p>aic = model.aic </p>
        <p><b>We also specify disp=False to </b></p>
        <p>results.append([order, aic]) </p>
        <p><b>avoid printing convergence </b></p>
        <p></p>
        <p><b>messages to the console. </b></p>
        <p><b>Label the</b></p>
        <p>result_df = pd.DataFrame(results) </p>
        <p><b>columns</b></p>
        <p>result_df.columns = ['(p,q)', 'AIC'] </p>
        <p><b>Store the (p,q) </b></p>
        <p><b>of your</b></p>
        <p></p>
        <p><b>combination and AIC </b></p>
        <p><b>DataFrame. </b></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p><b>in a DataFrame. </b></p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True) </p>
        <p><b>Sort the DataFrame in ascending </b></p>
        <p></p>
        <p><b>order of AIC values. The lower the </b></p>
        <p>return result_df</p>
        <p><b>AIC, the better the model. </b></p>
        <p>With the function in place, we can define a list of possible values for the orders <i>p</i>
            and <i>q</i>. In this case, we'll try the values 0, 1, 2, and 3
            for both orders and generate the list of unique ( <i>p</i>, <i>q</i>)
            combinations. </p>
        <p><b>Create a list of possible values for p from 0 </b></p>
        <p><b>inclusively to 4 exclusively, with steps of 1. </b></p>
        <p>from itertools import product</p>
        <p><b>Create a list of possible values for q from 0 </b></p>
        <p>ps = range(0, 4, 1) </p>
        <p><b>inclusively to 4 exclusively, with steps of 1. </b></p>
        <p>qs = range(0, 4, 1) </p>
        <p>d = 2 </p>
        <p><b>Set d to 2, as the series needed to be </b></p>
        <p><b>differenced twice to become stationary. </b></p>
        <p>order_list = list(product(ps, qs)) </p>
        <p><b>Generate a list containing all unique combinations of (p,q). </b></p>
        <p><a id="calibre_link-252"></a><b>150</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>Note that we do not give a range of values for the parameter <i>d</i>
            because it has a very specific definition: it is the number of times a series must be differenced to
            become
            stationary. Hence, it must be set to a specific value, which in this case is 2. </p>
        <p>Furthermore, <i>d</i> must be constant in order to compare models using the
            AIC. Varying <i>d</i> would change the likelihood function used in the calculation of
            the
            AIC value, so comparing models using the AIC as a criterion would not be valid anymore. </p>
        <p>We can now run the optimize_ARIMA function using the training set. The function</p>
        <p>returns a DataFrame with the model that has the lowest AIC at the top. </p>
        <p><b>The training set consists of all </b></p>
        <p>train = df.data[:-4] </p>
        <p><b>Run the </b></p>
        <p><b>data points except the last four. </b></p>
        <p><b>optimize_ARIMA </b></p>
        <p>result_df = optimize_ARIMA(train, order_list, d) </p>
        <p><b>function to obtain </b></p>
        <p>result_df </p>
        <p><b>the model with the </b></p>
        <p><b>Display the resulting </b></p>
        <p><b>lowest AIC. </b></p>
        <p><b>DataFrame. </b></p>
        <p>The returned DataFrame shows that a value of 3 for both <i>p</i> and <i class="calibre3">q</i> results in the
            lowest AIC. Therefore, an ARIMA(3,2,3) model seems to be the
            most
            suitable for this situation. Now let's assess the validity of the model by studying its residuals. </p>
        <p>To do so, we'll fit an ARIMA(3,2,3) model on the training set and display the residuals'
            diagnostics using the plot_diagnostics method. The result is shown in figure 7.7. </p>
        <p>model = SARIMAX(train, order=(3,2,3), simple_differencing=False) </p>
        <p>model_fit = model.fit(disp=False)</p>
        <p><b>Fit an ARIMA(3,2,3)</b></p>
        <p><b>model on the training</b></p>
        <p>model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p><b>set, since this model</b></p>
        <p><b>Display the residuals'</b></p>
        <p><b>has the lowest AIC. </b></p>
        <p><b>diagnostics. </b></p>
        <p>In figure 7.7, the top-left plot shows the residuals over time. While there is no trend in
            the residuals, the variance does not seem to be constant, which is a discrepancy in comparison to white
            noise. At the top right is the distribution of the residuals. We can see it is fairly close to a normal
            distribution. The Q-Q plot leads us to the same conclusion, as it displays a line that is fairly
            straight,
            meaning that the residuals' distribution is close to a normal distribution. Finally, by looking at the
            correlogram at the bottom right, we can see that a coefficient seems to be significant at lag 3.
            However,
            since it is not preceded by any significant autocorrelation coefficients, we can assume that this is due
            to
            chance. Therefore, we can say that the correlogram shows no significant coefficients after lag 0, just
            like
            white noise. </p>
        <p>Thus, from a qualitative standpoint, it seems that our residuals are close to white noise,
            which is a good sign, as it means that the model's errors are random. </p>
        <p>The last step is to evaluate the residuals from a quantitative standpoint. We'll thus apply
            the Ljung-Box test to determine whether the residuals are correlated. We'll apply the test on the first
            10
            lags and study the p-values. If all p-values are greater than</p>
        <p><a id="calibre_link-378"></a><img src="images/000013.jpg" alt="Image 71" class="calibre2" />
        </p>
        <p> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p><b>151</b></p>
        <p>Figure 7.7</p>
        <p>Diagnostics of the ARIMA(3,2,3) residuals. The Q-Q plot at the bottom left displays a fairly
            straight line with some deviation at the extremities. </p>
        <p>0.05, we cannot reject the null hypothesis and we'll conclude that the residuals are not
            correlated, just like white noise. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p><b>Store the model's </b></p>
        <p>residuals = model_fit.resid </p>
        <p><b>residuals in a variable. </b></p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1)) </p>
        <p><b>Apply the Ljung-Box test</b></p>
        <p>print(pvalue)</p>
        <p><b>on the first 10 lags. </b></p>
        <p>Running the Ljung-Box test on the first 10 lags of the model's residuals returns a list of
            p-values that are all larger than 0.05. Therefore, we do not reject the null hypothesis, and we conclude
            that the residuals are not correlated, just like white noise. </p>
        <p>Our ARIMA(3,2,3) model has passed all the checks, and it can now be used for forecasting.
            Remember that our test set is the last four data points, corresponding to</p>
        <p><a id="calibre_link-379"></a><img src="images/000020.jpg" alt="Image 72" class="calibre2" />
        </p>
        <p><b>152</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>the four quarterly EPS reported in 1980. As a benchmark for our model, we will use the naive
            seasonal method. This means that we'll take the EPS of the first quarter of 1979 and use it as a
            forecast
            for the EPS of the first quarter of 1980. Then the EPS of the second quarter of 1979 will be used as a
            forecast for the EPS of the second quarter of 1980, and so on. Remember that we need a benchmark, or a
            baseline model, when modeling to determine whether the model we develop is better than a naive method.
        </p>
        <p>The performance of a model must always be assessed relative to a baseline model. </p>
        <p><b>The test set corresponds to </b></p>
        <p>test = df.iloc[-4:] </p>
        <p><b>the last four data points. </b></p>
        <p>test['naive_seasonal'] = df['data'].iloc[76:80].values </p>
        <p><b>The naive seasonal forecast is implemented by selecting the quarterly
                EPS</b></p>
        <p><b>reported in 1979 and using the same values as a forecast for the year
                1980. </b></p>
        <p>With our baseline in place, we can now make forecasts using the ARIMA(3,2,3) model and store
            the results in the ARIMA_pred column. </p>
        <p>ARIMA_pred = model_fit.get_prediction(80, 83).predicted_mean </p>
        <p><b>Get the predicted values</b></p>
        <p>test['ARIMA_pred'] = ARIMA_pred </p>
        <p><b>for the year 1980. </b></p>
        <p><b>Assign the forecasts to</b></p>
        <p><b>the ARIMA_pred column. </b></p>
        <p>Let's visualize our forecasts to see how close the predictions from each method are to the
            observed values. The resulting plot is shown in figure 7.8. </p>
        <p>Figure 7.8</p>
        <p>Forecasts of the quarterly EPS of Johnson &amp; Johnson in 1980. We can see </p>
        <p>that the predictions coming from the ARIMA(3,2,3) model, shown as a dashed line, almost
            perfectly overlap the observed data in 1980. </p>
        <p><a id="calibre_link-380"></a><img src="images/000032.jpg" alt="Image 73" class="calibre2" />
        </p>
        <p> <i><b>7.3</b></i></p>
        <p> <i><b>Forecasting a non-stationary times series</b></i>
        </p>
        <p><b>153</b></p>
        <p>In figure 7.8 we can see the naive seasonal forecast as a dotted line and the ARIMA(3,2,3)
            forecasts as a dashed line. The ARIMA(3,2,3) model predicted the quarterly EPS with a very small error.
        </p>
        <p>We can quantify that error by measuring the mean absolute percentage error (MAPE) and
            display the metric for each forecasting method in a bar plot, as shown in figure 7.9. </p>
        <p><b>Define a function to </b></p>
        <p><b>compute the MAPE. </b></p>
        <p>def mape(y_true, y_pred): </p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p>mape_naive_seasonal = mape(test['data'], test['naive_seasonal']) </p>
        <p>mape_ARIMA = mape(test['data'], test['ARIMA_pred']) </p>
        <p><b>Compute the</b></p>
        <p><b>Compute the</b></p>
        <p><b>MAPE for the</b></p>
        <p>fig, ax = plt.subplots()</p>
        <p><b>MAPE for the</b></p>
        <p><b>naive seasonal</b></p>
        <p><b>ARIMA(3,2,3)</b></p>
        <p><b>method. </b></p>
        <p>x = ['naive seasonal', 'ARIMA(3,2,3)']</p>
        <p><b>model. </b></p>
        <p>y = [mape_naive_seasonal, mape_ARIMA]</p>
        <p>ax.bar(x, y, width=0.4)</p>
        <p>ax.set_xlabel('Models')</p>
        <p>ax.set_ylabel('MAPE (%)')</p>
        <p>ax.set_ylim(0, 15)</p>
        <p>for index, value in enumerate(y):</p>
        <p>plt.text(x=index, y=value + 1, s=str(round(value,2)), ha='center')</p>
        <p>plt.tight_layout()</p>
        <p>Figure 7.9</p>
        <p>The MAPE for both forecasting methods. You can see that the ARIMA </p>
        <p>model has an error metric that is one fifth of the baseline. </p>
        <p><a id="calibre_link-85"></a><b>154</b></p>
        <p>CHAPTER 7</p>
        <p> <i><b>Forecasting non-stationary time series</b></i></p>
        <p>In figure 7.9, you can see that the MAPE for the naive seasonal forecast is 11.56%, while
            the MAPE for the ARIMA(3,2,3) model is 2.19%, which roughly one fifth of the benchmark value. This means
            that our predictions are on average 2.19% off from the actual values. The ARIMA(3,2,3) model is clearly
            a
            better model than the naive seasonal method. </p>
        <p> <i><b>7.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the ARIMA( <i>p</i>, <i>d</i>,
            <i>q</i>) model, which allows us to model and forecast non-stationary time series. </p>
        <p>The order of integration <i>d</i> defines how many times a series must be
            differenced to become stationary. This parameter then allows us to fit the model on the original series
            and
            get a forecast in the same scale, unlike the ARMA( <i>p</i>, <i>q</i>)
            model, which required the series to be stationary for the model to be applied and required us to reverse
            the
            transformations on the forecasts. </p>
        <p>To apply the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            model,
            we added an extra step to our general modeling procedure, which simply
            involves finding the value for the order of integration. </p>
        <p>This corresponds to the minimum number of times a series must be differenced to become
            stationary. </p>
        <p>Now we can add another layer to the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model that allows
            us to consider yet another property
            of time series: seasonality. We have studied the Johnson &amp; Johnson dataset enough times to realize
            that
            there are clear cyclical patterns in the series. To integrate the seasonality of a series in a model, we
            must use the <i>seasonal</i> <i>autoregressive integrated moving
                average</i> (SARIMA) model, or SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>,
            <i>D</i>, <i class="calibre3">Q</i>) <i>m</i>. </p>
        <p>This will be the subject of the next chapter. </p>
        <p> <i><b>7.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Now is the time to apply the ARIMA model on previous datasets that we have explored. </p>
        <p>The full solution to this exercise is available on <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH07">GitHub:
                https://github.com/marcopeix/</a></p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH07">TimeSeriesForecastingInPython/tree/master/CH07</a>.
        </p>
        <p> <i><b>7.5.1</b></i></p>
        <p> <i><b>Apply the ARIMA(p,d,q) model on the datasets from
                    chapters 4, </b></i></p>
        <p> <i><b>5, and 6</b></i></p>
        <p>In chapters 4, 5, and 6, non-stationary time series were introduced to show you how to apply
            the MA( <i>q</i>), AR( <i>p</i>), and ARMA( <i>p</i>,
            <i>q</i>) models. In each chapter, we transformed the series to make it stationary, fit
            the model, made forecasts, and had to reverse the transformation on the forecasts to bring them back to
            the
            original scale of the data. </p>
        <p>Now that you know how to account for non-stationary time series, revisit each dataset and
            apply the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) model.
            For
            each dataset, do the following:</p>
        <p> Apply the general modeling procedure. </p>
        <p> Is an ARIMA(0,1,2) model suitable for the dataset in chapter 4? </p>
        <p> Is an ARIMA(3,1,0) model suitable for the dataset in chapter 5? </p>
        <p> Is an ARIMA(2,1,2) model suitable for the dataset in chapter 6? </p>
        <p><a id="calibre_link-269"></a> <i><b>Summary</b></i></p>
        <p><b>155</b></p>
        <p> <i><b>Summary</b></i></p>
        <p> The autoregressive integrated moving average model, denoted as ARIMA( <i>p</i>, <i class="calibre3">d</i>,
            <i>q</i>), is the combination of the
            autoregressive model AR( <i>p</i>), the order of integration <i>d</i>,
            and
            the moving average model MA( <i>q</i>). </p>
        <p> The ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) model can
            be
            applied on non-stationary time series and has the added advantage
            of returning forecasts in the same scale as the original series. </p>
        <p> The order of integration <i>d</i> is equal to the minimum number of times
            a series must be differenced to become stationary. </p>
        <p> An ARIMA( <i>p</i>,0, <i>q</i>) model is equivalent to
            an ARMA( <i>p</i>, <i>q</i>) model. </p>
        <p><a id="calibre_link-10"></a> <i>Accounting</i></p>
        <p> <i>for seasonality</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the seasonal autoregressive </p>
        <p>integrated moving average model, </p>
        <p>SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
        </p>
        <p> Analyzing seasonal patterns in a time series</p>
        <p> Forecasting using the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>, <i
                class="calibre3">Q</i>) <i>m</i> model</p>
        <p>In the previous chapter, we covered the autoregressive integrated moving average model,
            ARIMA( <i>p</i>, <i>d</i>, <i>q</i>), which allows us
            to
            model non-stationary time series. Now we'll add another layer of complexity to the ARIMA model to
            include
            seasonal patterns in time series, leading us to the SARIMA model. </p>
        <p>The <i>seasonal autoregressive integrated moving average</i> (SARIMA)
            model, or SARIMA ( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>,
            adds another set of parameters that allows us to take into account periodic patterns when forecasting a
            time
            series, which is not always possible with an ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model.
        </p>
        <p>In this chapter, we'll examine the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>,
            <i>D</i>, <i class="calibre3">Q</i>) <i>m</i> model and adapt our general
            modeling procedure to account for the new parameters. We'll also determine how to identify seasonal
            patterns
            in a time series and apply the SARIMA model to forecast a seasonal time series. Specifically, we'll
            apply
            the model to forecast the <b>156</b></p>
        <p><a id="calibre_link-196"></a><img src="images/000145.jpg" alt="Image 74" class="calibre2" />
        </p>
        <p> <i><b>8.1</b></i></p>
        <p> <i><b>Examining the SARIMA(p,d,q)(P,D,Q)m model</b></i>
        </p>
        <p><b>157</b></p>
        <p>total number of monthly passengers for an airline. The data was recorded from January 1949
            to December 1960. The series is shown in figure 8.1. </p>
        <p>Figure 8.1</p>
        <p>Monthly total number of air passengers for an airline, from January 1949 to </p>
        <p>December 1960. You'll notice a clear seasonal pattern in the series, with peak traffic
            occurring toward the middle of each year. </p>
        <p>In figure 8.1 we can see a clear seasonal pattern in the series. The number of air
            passengers is lower at the beginning and end of the year, and it spikes up during the months of June,
            July,
            and August. Our objective is to forecast the number of monthly air passengers for the one year. It is
            important for an airline company to forecast the number of air passengers so they can better price their
            tickets and schedule flights to meet the demand for a given month. </p>
        <p> <i><b>8.1</b></i></p>
        <p> <i><b>Examining the SARIMA(p,d,q)(P,D,Q)m model</b></i>
        </p>
        <p>The SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model expands on the ARIMA( <i>p</i>,
            <i>d</i>, <i>q</i>) model from the previous chapter by adding seasonal
            parameters. You'll notice four new parameters in the model: <i>P</i>, <i class="calibre3">D</i>, <i>Q</i>,
            and <i>m</i>. The first three
            have
            the same meaning as in the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model, but they are their
            seasonal counterparts. To understand the meaning
            of
            these parameters and how they affect the final model, we must first define <i>m</i>.
        </p>
        <p>The parameter <i>m</i> stands for the frequency. In the context of a time
            series, the frequency is defined as the number of observations per cycle. The length of the cycle will
            depend on the dataset. For data that was recorded every year, quarter, month, or week, the length of a
            cycle
            is considered to be 1 year. If the data was recorded annu-ally, <i>m</i> = 1 since
            there is
            only one observation per year. If the data was recorded quarterly, <i>m</i> = 4 since
            there
            are four quarters in a year, and therefore four observations per</p>
        <p><a id="calibre_link-381"></a><b>158</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>year. Of course, if the data was recorded monthly, <i>m</i> = 12. Finally,
            for weekly data, <i>m</i> = 52. </p>
        <p>Table 8.1 indicates the appropriate value of <i>m</i> depending on the
            frequency at which the data was collected. </p>
        <p>Table 8.1</p>
        <p>Appropriate frequency <i><b>m</b></i> depending on the
            data</p>
        <p>Data collection</p>
        <p>Frequency <i><b>m</b></i></p>
        <p>Annual</p>
        <p>1</p>
        <p>Quarterly</p>
        <p>4</p>
        <p>Monthly</p>
        <p>12</p>
        <p>Weekly</p>
        <p>52</p>
        <p>When data is collected on a daily or sub-daily basis, there are multiple ways of
            interpreting the frequency. For example, daily data can have a weekly seasonality. In that case, the
            frequency is <i>m</i> = 7 because there would be seven observations in a full cycle of
            1
            week. It could also have a yearly seasonality, meaning that <i>m</i> = 365. Thus, you
            can
            see that daily and sub-daily data can have a different cycle length, and therefore a different frequency
            <i>m</i>. Table 8.2 provides the appropriate value of <i>m</i>
            depending
            on the seasonal cycle for daily and sub-daily data. </p>
        <p>Table 8.2</p>
        <p>Appropriate frequency <i><b>m</b></i> for daily and
            sub-daily data</p>
        <p>Frequency <i><b>m</b></i></p>
        <p>Data collection</p>
        <p>Minute</p>
        <p>Hour</p>
        <p>Day</p>
        <p>Week</p>
        <p>Year</p>
        <p>Daily</p>
        <p>7</p>
        <p>365</p>
        <p>Hourly</p>
        <p>24</p>
        <p>168</p>
        <p>8766</p>
        <p>Every minute</p>
        <p>60</p>
        <p>1440</p>
        <p>10080</p>
        <p>525960</p>
        <p>Every second</p>
        <p>60</p>
        <p>3600</p>
        <p>86400</p>
        <p>604800</p>
        <p>31557600</p>
        <p>Now that you understand the parameter <i>m</i>, the meanings of <i>P</i>,
            <i>D</i>, and <i>Q</i> become intuitive. As
            mentioned before, they are the seasonal counterparts of the <i>p</i>, <i class="calibre3">d</i>, and
            <i>q</i> parameters that you know from the ARIMA( <i class="calibre3">p</i>, <i>d</i>, <i>q</i>) model. </p>
        <p>Seasonal autoregressive integrated moving average (SARIMA) model</p>
        <p>The <i>seasonal autoregressive integrated moving average</i> (SARIMA) model
            adds seasonal parameters to the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model. </p>
        <p>It is denoted as SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>)
            <i>m</i>,
            where <i>P</i> is the order of the
            seasonal AR( <i>P</i>) process, <i>D</i> is the seasonal order of
            integration, <i>Q</i> is the order of the seasonal MA( <i>Q</i>)
            process,
            and <i>m</i> is the frequency, or the number of observations per seasonal cycle. </p>
        <p>Note that a SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)(0,0,0)
            <i>m</i> model is equivalent to an ARIMA( <i>p</i>, <i class="calibre3">d</i>, <i>q</i>) model. </p>
        <p><a id="calibre_link-382"></a><img src="images/000097.jpg" alt="Image 75" class="calibre2" />
        </p>
        <p> <i><b>8.1</b></i></p>
        <p> <i><b>Examining the SARIMA(p,d,q)(P,D,Q)m model</b></i>
        </p>
        <p><b>159</b></p>
        <p>Let's consider an example where <i>m</i> = 12. If <i>P</i>
            = 2, this means that we are including two past values of the series at a lag that is a multiple of <i
                class="calibre3">m</i>. Therefore, we'll include the values at <i>y<sub>t-1</sub></i>2
            and
            <i>yt </i>&ndash;24. </p>
        <p>Similarly, if <i>D</i> = 1, this means that a seasonal difference makes the
            series stationary. </p>
        <p>In this case, a seasonal difference would be expressed as equation 8.1. </p>
        <p> <i> y't</i> = <i>yt</i> &ndash; <i>yt
            </i>&ndash;12</p>
        <p>Equation 8.1</p>
        <p>In a situation where <i>Q</i> = 2, we'll include past error terms at lags
            that are a multiple of <i>m</i>. Therefore, we'll include the errors ϵ <i class="calibre3">t </i>&ndash;12
            and ϵ <i>t </i>&ndash;24. </p>
        <p>Let's put this into perspective using the airline's total monthly air passengers dataset. We
            know that this is monthly data, which means that <i>m</i> = 12. Also, we can see that
            the
            months of July and August usually have the highest numbers of air passengers in the year, as shown by
            the
            round markers in figure 8.2. Therefore, if we are to forecast the month of July in 1961, the information
            coming from the month of July in prior years is likely going to be useful, since we can intuitively
            expect
            the number of air passengers to be at its highest point in the month of July 1961. The parameters <i
                class="calibre3">P</i>, <i>D</i>, <i>Q</i>, and <i class="calibre3">m</i> allow us to capture that
            information from the previous seasonal cycle to help
            us
            forecast our time series. </p>
        <p>Figure 8.2</p>
        <p>Marking the month of July of each year. You can see how the month of July </p>
        <p>has the highest number of air passengers. Therefore, it would make sense if July of the
            following year also saw the highest number of air passengers in the year. That kind of information is
            captured by the seasonal parameters <i><b>P</b></i>, <i class="calibre3"><b class="calibre4">D</b></i>,
            <i><b>Q</b></i>,
            and
            <i><b>m</b></i> of the SARIMA( <i><b>p</b></i>, <i><b class="calibre4">d</b></i>, <i
                class="calibre3"><b>q</b></i>)( <i><b class="calibre4">P</b></i>,
            <i><b>D</b></i>, <i><b>Q</b></i>) <i class="calibre3"><b class="calibre4">m</b></i> model. </p>
        <p><a id="calibre_link-86"></a><img src="images/000125.jpg" alt="Image 76" class="calibre2" />
        </p>
        <p><b>160</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Now that we have examined the SARIMA model and you understand how it expands</p>
        <p>on the ARIMA model, let's move on to identifying the presence of seasonal patterns in a time
            series. </p>
        <p> <i><b>8.2</b></i></p>
        <p> <i><b>Identifying seasonal patterns in a time
                    series</b></i></p>
        <p>Intuitively, we know that it makes sense to apply the SARIMA model on data that exhibits a
            seasonal pattern. Therefore, it is important to determine ways to identify seasonality in time series.
        </p>
        <p>Usually, plotting the time series data is enough to observe periodic patterns. For example,
            looking at the total monthly air passengers in figure 8.3, it is easy for us to identify a repeating
            pattern
            every year, with a high number of passengers being recorded during June, July, and August of each year,
            and
            fewer passengers in November, December, and January of each year. </p>
        <p>Figure 8.3</p>
        <p>Highlighting the seasonal pattern in the monthly number of air passengers. </p>
        <p>The dashed vertical lines separate periods of twelve months. We can clearly see how a peak
            occurs in the middle of each year, and there is a very similar pattern for the beginning and end of each
            year. This observation is usually enough to determine that the dataset is seasonal. </p>
        <p>Another way of identifying seasonal patterns in a time series is using time series
            decomposition, a method that we first used in chapter 1. Time series decomposition is a statistical task
            that separates the time series into its three main components: a trend component, a seasonal component,
            and
            the residuals. </p>
        <p>The trend component represents the long-term change in the time series. This component is
            responsible for time series that increase or decrease over time. The</p>
        <p><a id="calibre_link-303"></a> <i><b>8.2</b></i></p>
        <p> <i><b>Identifying seasonal patterns in a time
                    series</b></i></p>
        <p><b>161</b></p>
        <p>seasonal component is, of course, the seasonal pattern in the time series. It represents
            repeated fluctuations that occur over a fixed period of time. Finally, the residuals, or the noise,
            express
            any irregularity that cannot be explained by the trend or the seasonal component. </p>
        <p>Time series decomposition</p>
        <p> <i>Time series decomposition</i> is a statistical task that separates the
            time series into its three main components: a trend component, a seasonal component, and the residuals.
        </p>
        <p>The trend component represents the long-term change in the time series. This component is
            responsible for time series that increase or decrease over time. The seasonal component is the periodic
            pattern in the time series. It represents repeated fluctuations that occur over a fixed period of time.
            Finally, the residuals, or the noise, express any irregularity that cannot be explained by the trend or
            the
            seasonal component. </p>
        <p>NOTE The source code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH08">https://github</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH08">.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH08.
            </a></p>
        <p>With time series decomposition, we can clearly identify and visualize the seasonal component
            of a time series. We can decompose the dataset for air passengers using the STL function from the
            statsmodels library to generate figure 8.4. </p>
        <p>from statsmodels.tsa.seasonal import STL</p>
        <p>decomposition = STL(df['Passengers'], period=12).fit() </p>
        <p>fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, </p>
        <p>➥ figsize=(10,8)) </p>
        <p><b>Plot each </b></p>
        <p><b>Decompose the series using</b></p>
        <p><b>component </b></p>
        <p><b>the STL function. The period</b></p>
        <p>ax1.plot(decomposition.observed)</p>
        <p><b>in a figure. </b></p>
        <p><b>is equal to the frequency m. </b></p>
        <p>ax1.set_ylabel('Observed')</p>
        <p><b>Since we have monthly</b></p>
        <p><b>data, the period is 12. </b></p>
        <p>ax2.plot(decomposition.trend)</p>
        <p>ax2.set_ylabel('Trend')</p>
        <p>ax3.plot(decomposition.seasonal)</p>
        <p>ax3.set_ylabel('Seasonal')</p>
        <p>ax4.plot(decomposition.resid)</p>
        <p>ax4.set_ylabel('Residuals')</p>
        <p>plt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In figure 8.4 you can see each component of our time series. You'll notice that the <i
                class="calibre3">y</i>-axis for the plots of the trend, seasonal, and residuals components are all
            slightly different</p>
        <p><a id="calibre_link-383"></a><img src="images/000009.jpg" alt="Image 77" class="calibre2" />
        </p>
        <p><b>162</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Figure 8.4</p>
        <p>Decomposing the dataset for air passengers. The first plot shows the observed data. The
            second plot shows the trend component, which tells us that the number of air passengers is increasing
            over
            time. The third plot displays the seasonal component, and we can clearly see a repeating pattern through
            time. Finally, the last plot shows the residuals, which are variations in the data that cannot be
            explained
            by the trend or the seasonal component. </p>
        <p>from the observed data. This is because each plot shows the magnitude of change that is
            attributed to that particular component. That way, the sum of the trend, seasonal, and residuals
            components
            results in the observed data shown in the top plot. This explains why the seasonal component is
            sometimes in
            the negative values and other times in the positive values, as it creates the peaks and troughs in the
            observed data. </p>
        <p>In a situation where we have a time series with no seasonal pattern, the decomposition
            process will display a flat horizontal line at 0 for the seasonal component. To demonstrate that, I
            simulated a linear time series and decomposed it into its three components using the method you just
            saw.
            The result is shown in figure 8.5. </p>
        <p>You can see how time series decomposition can help us determine if our data is seasonal or
            not. This is a graphical method and not a statistical test, but it is enough to</p>
        <p><a id="calibre_link-87"></a><img src="images/000060.jpg" alt="Image 78" class="calibre2" />
        </p>
        <p> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>163</b></p>
        <p>Figure 8.5</p>
        <p>Time series decomposition of a simulated linear series. The top plot shows the observed
            data, and you'll notice that I simulated a perfectly linear series. The second plot shows the trend
            component, which is expected to be the same as the observed data, since the series is linearly
            increasing
            over time. Since there is no seasonal pattern, the seasonal component is a flat horizontal line at 0.
            Here
            the residuals are also 0, because I simulated a perfectly linear series. </p>
        <p>determine whether a series is seasonal or not, so that we can apply the appropriate model
            for forecasting. In fact, there are no statistical tests to identify seasonality in time series. </p>
        <p>Now that you know how to identify seasonal patterns in a series, we can move on to adapting
            the general modeling procedure to include the new parameters of the SARIMA( <i>p</i>,
            <i>d</i>, <i>q</i>)( <i>P</i>, <i>D</i>,
            <i>Q</i>) <i>m</i> model and forecast the
            number of monthly air passengers. </p>
        <p> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p>In the previous chapter, we adapted our general modeling procedure to account for the new
            parameter <i>d</i> in the ARIMA model that allows us to forecast non-stationary time
            series. The steps are outlined in figure 8.6. Now we must modify it again to account for the new
            parameters
            of the SARIMA model, which are <i>P</i>, <i>D</i>, <i class="calibre3">Q</i>, and <i>m</i>. </p>
        <p><a id="calibre_link-384"></a><b>164</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>List values</p>
        <p>of <i>p </i> and <i>q</i></p>
        <p>Fit every combination</p>
        <p>of ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
        </p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 8.6</p>
        <p>General modeling procedure for an </p>
        <p>ARIMA model. We now need to adapt the steps </p>
        <p>Ready for</p>
        <p>to account for the parameters <i><b>P</b></i>, <i><b>D</b></i>, <i><b class="calibre4">Q</b></i>, and
            <i><b>m</b></i> of forecasts</p>
        <p>the SARIMA model. </p>
        <p><a id="calibre_link-88"></a> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>165</b></p>
        <p>The first step of gathering data remains untouched. Then we still check for stationarity and
            apply transformation in order to set the parameter <i>d</i>. However, we can also
            perform
            seasonal differencing to make the series stationary, and <i>D</i> will be equal to the
            minimum number of times we applied seasonal differencing. </p>
        <p>Then we set a range of possible values for <i>p</i>, <i>q</i>, <i class="calibre3">P</i>, and <i>Q</i>, as
            the SARIMA model
            can also incorporate the order of the seasonal autoregressive and seasonal moving average processes.
            Note
            that the addition of these two new parameters will increase the number of unique combinations of SARIMA(
            <i>p</i>, <i>d</i>, <i>q</i>)( <i>P</i>,
            <i>D</i>, <i>Q</i>) <i>m</i>
            models we can fit, so this step will take longer to complete. The rest of the procedure remains the
            same, as
            we still need to select the model with the lowest AIC and perform residual analysis before using the
            model
            for forecasting. The resulting modeling procedure is shown in figure 8.7. </p>
        <p>With our new modeling procedure defined, we are now ready to forecast the total</p>
        <p>number of monthly air passengers. For this scenario, we wish to forecast 1 year of monthly
            air passengers, so we will use the data from 1960 as the test set, as shown in figure 8.8. </p>
        <p>The baseline model will be the naive seasonal forecast, and we will use both the ARIMA( <i
                class="calibre3">p</i>, <i>d</i>, <i>q</i>) and SARIMA( <i class="calibre3">p</i>, <i>d</i>, <i>q</i>)(
            <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            models to verify whether the addition of seasonal components will yield better forecasts. </p>
        <p> <i><b>8.3.1</b></i></p>
        <p> <i><b>Forecasting with an ARIMA(p,d,q) model</b></i></p>
        <p>We'll first model the dataset using an ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model. That way,
            we can compare its performance to
            the SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model. </p>
        <p>Following the general modeling procedure we outlined before, we'll first test for
            stationarity. Again, we use the ADF test. </p>
        <p>ad_fuller_result = adfuller(df['Passengers'])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This prints out an ADF statistic of 0.82 and a p-value of 0.99. Therefore, we cannot reject
            the null hypothesis and the series is not stationary. We'll difference the series and test for
            stationarity
            again. </p>
        <p>df_diff = np.diff(df['Passengers'], n=1) </p>
        <p><b>First-order </b></p>
        <p><b>differencing</b></p>
        <p>ad_fuller_result = adfuller(df_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p><a id="calibre_link-385"></a><b>166</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>Set <i>D </i> equal to the</p>
        <p>number of times</p>
        <p>seasonal differencing</p>
        <p>was applied</p>
        <p>List values</p>
        <p>of <i>p, q, P, Q</i></p>
        <p>Fit every combination of</p>
        <p>SARIMA( , , </p>
        <p> <i>p d q</i>)( , , )</p>
        <p> <i>P D Q m</i></p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 8.7</p>
        <p>General modeling procedure for the </p>
        <p>Ready for</p>
        <p>SARIMA model. Note that we can set <i><b>P</b></i>, <i><b>D</b></i>, and
        </p>
        <p>forecasts</p>
        <p> <i><b>Q</b></i> to 0 to obtain an ARIMA( <i><b>p</b></i>, <i><b class="calibre4">d</b></i>, <i
                class="calibre3"><b>q</b></i>) model. </p>
        <p><a id="calibre_link-197"></a><img src="images/000068.jpg" alt="Image 79" class="calibre2" />
        </p>
        <p> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>167</b></p>
        <p>Figure 8.8</p>
        <p>Train set and test set split for the air passengers dataset. The shaded area </p>
        <p>represents the testing period, which corresponds to the full year of 1960, as our goal is to
            forecast a year of monthly air passengers. </p>
        <p>This returns an ADF statistic of &ndash;2.83 and a p-value of 0.054. Again, we cannot reject
            the null hypothesis, and differencing the series once did not make it stationary. Therefore, we'll
            difference it again and test for stationarity. </p>
        <p>df_diff2 = np.diff(df_diff, n=1) </p>
        <p><b>Series is now </b></p>
        <p><b>differenced </b></p>
        <p>ad_fuller_result = adfuller(df_diff2)</p>
        <p><b>twice</b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This returns an ADF statistic of &ndash;16.38 and a p-value of 2.73 × 10&ndash;29. Now we
            can reject the null hypothesis, and our series is considered to be stationary. Since the series was
            differenced twice to become stationary, <i>d</i> = 2. </p>
        <p>Now we can define a range of possible values for the parameters <i>p</i>
            and <i>q</i> and fit all unique ARIMA( <i>p</i>, <i class="calibre3">d</i>, <i>q</i>) models. We'll
            specifically choose a range from 0
            to
            12 to allow the ARIMA model to go back 12 timesteps in time. Since the data is sampled monthly and we
            know
            it is seasonal, we can hypothesize that the number of air passengers in January of a given year is
            likely
            predictive of the number of air passengers in January of the following year. Since these two points are
            12
            timesteps apart, we'll allow the values of <i>p</i> and <i>q</i> to
            vary
            from 0 to 12 in order to potentially capture this seasonal information in the ARIMA( <i
                class="calibre3">p</i>, <i>d</i>, <i>q</i>) model. Finally, since
            we
            are working with an ARIMA model, we'll set <i>P</i>, <i>D</i>, and <i class="calibre3">Q</i> to 0. Note the
            use of the parameter s in the following</p>
        <p><a id="calibre_link-272"></a><b>168</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>code, which is equivalent to <i>m</i>. The implementation of SARIMA in
            statsmodels simply uses s instead of <i>m</i>&mdash;they both denote the frequency.
        </p>
        <p><b>Set P and Q to 0, since </b></p>
        <p><b>Allow p and q to vary from </b></p>
        <p><b>we are working with an </b></p>
        <p><b>0 to 12 in order to capture </b></p>
        <p><b>ARIMA(p,d,q) model. </b></p>
        <p><b>seasonal information. </b></p>
        <p>ps = range(0, 13, 1) </p>
        <p>qs = range(0, 13, 1) </p>
        <p>Ps = [0] </p>
        <p><b>Set the parameter d to the number </b></p>
        <p>Qs = [0] </p>
        <p><b>of times the series was differenced </b></p>
        <p><b>to become stationary. </b></p>
        <p>d = 2 </p>
        <p>D = 0 </p>
        <p><b>D is set to 0 because we are working </b></p>
        <p><b>Generate all possible </b></p>
        <p>s = 12 </p>
        <p><b>with an ARIMA(p,d,q) model. </b></p>
        <p><b>combinations of </b></p>
        <p><b>(p,d,q)(0,0,0). </b></p>
        <p>ARIMA_order_list = list(product(ps, qs, Ps, Qs)) </p>
        <p><b>The parameter </b>s<b> is equivalent to m. They both
            </b></p>
        <p><b>denote the frequency. This is simply how the </b></p>
        <p><b>SARIMA model is implemented in the </b></p>
        <p><b>statsmodels library. </b></p>
        <p>You'll notice that we set the parameters <i>P</i>, <i>D</i>, <i class="calibre3">Q</i>, and <i>m</i>, even
            though we are
            working with an ARIMA model. This is because we are going to define an optimize_SARIMA function that
            will
            then be reused in the next section. We set <i>P</i>, <i>D</i>, and <i class="calibre3">Q</i> to 0 because a
            SARIMA( <i>p</i>, <i>d</i>,
            <i>q</i>)(0,0,0) <i>m</i> model is equivalent to an ARIMA( <i class="calibre3">p</i>, <i>d</i>, <i>q</i>)
            model. </p>
        <p>The optimize_SARIMA function builds on the optimize_ARIMA function that we defined in the
            previous chapter. This time, we'll integrate the possible values of <i>P</i> and <i class="calibre3">Q</i>,
            as well as add the seasonal order of integration <i>D</i>
            and
            the frequency <i>m</i>. The function is shown in the following listing. </p>
        <p>Listing 8.1</p>
        <p>Defining a function to select the best SARIMA model</p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_SARIMA(endog: Union[pd.Series, list], order_list: list, d: </p>
        <p>➥ int, D: int, s: int) -&gt; pd.DataFrame: </p>
        <p><b>The order_list parameter now </b></p>
        <p></p>
        <p><b>includes p, q, P, and Q orders. We also </b></p>
        <p>results = []</p>
        <p><b>add the seasonal order of differencing </b></p>
        <p></p>
        <p><b>D and the frequency. Remember that </b></p>
        <p>for order in tqdm_notebook(order_list): </p>
        <p><b>the frequency m in the SARIMA model </b></p>
        <p>try: </p>
        <p><b>is denoted as s in the implementation </b></p>
        <p>model = SARIMAX(</p>
        <p><b>in the statsmodels library. </b></p>
        <p><b>Loop over all unique </b></p>
        <p>endog, </p>
        <p><b>SARIMA(p,d,q)(P,D,Q)</b></p>
        <p>order=(order[0], </p>
        <p><b>m </b></p>
        <p>d, order[1]), </p>
        <p><b>models, fit them, and </b></p>
        <p>seasonal_order=(order[2], D, order[3], s), </p>
        <p><b>store the AICs. </b></p>
        <p>simple_differencing=False).fit(disp=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p><a id="calibre_link-386"></a> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>169</b></p>
        <p>aic = model.aic</p>
        <p>results.append([order, aic])</p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['(p,q,P,Q)', 'AIC']</p>
        <p></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df </p>
        <p><b>Return the sorted DataFrame, </b></p>
        <p><b>starting with the lowest AIC. </b></p>
        <p>With the function ready, we can launch it using the train set and get the ARIMA model with
            the lowest AIC. Despite the fact that we are using the optimize_SARIMA function, we are still fitting an
            ARIMA model because we specifically set <i>P</i>, <i>D</i>, and <i class="calibre3">Q</i> to 0. For the
            train set, we'll take all data points but the last twelve, as
            they
            will be used for the test set. </p>
        <p><b>The train set consists of all data </b></p>
        <p><b>points but the last 12, as the last </b></p>
        <p><b>year of data is used for the test set. </b></p>
        <p>train = df['Passengers'][:-12] </p>
        <p>ARIMA_result_df = optimize_SARIMA(train, ARIMA_order_list, d, D, s) </p>
        <p>ARIMA_result_df </p>
        <p><b>Display the sorted </b></p>
        <p><b>Run the</b></p>
        <p><b>DataFrame in increasing </b></p>
        <p><b>optimize_SARIMA</b></p>
        <p><b>order of AIC. </b></p>
        <p><b>function. </b></p>
        <p>This returns a DataFrame where the model with the lowest AIC is a</p>
        <p>SARIMA(11,2,3)(0,0,0)12 model, which is equivalent to an ARIMA(11,2,3) model. As you can
            see, allowing the order <i>p</i> to vary from 0 to 12 was beneficial for the model, as
            the
            model with the lowest AIC takes into account the past 11 values of the series, since <i
                class="calibre3">p</i> = 11. We will see if this is enough to capture seasonal information from the
            series, and we will compare the performance of the ARIMA model to the SARIMA model in</p>
        <p>the next section. </p>
        <p>For now, we'll focus on performing residual analysis. We can fit the ARIMA(11,2,3) model
            obtained previously and plot the residuals' diagnostics. </p>
        <p>ARIMA_model = SARIMAX(train, order=(11,2,3), simple_differencing=False)</p>
        <p>ARIMA_model_fit = ARIMA_model.fit(disp=False)</p>
        <p>ARIMA_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>The result is shown in figure 8.9. Based on the qualitative analysis, the residuals are
            close to white noise, meaning that the errors are random. </p>
        <p></p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-253"></a><img src="images/000010.jpg" alt="Image 80" class="calibre2" />
        </p>
        <p><b>170</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Figure 8.9</p>
        <p>Residuals' diagnostics of the ARIMA(11,2,3) model. In the top-left plot, the residuals have
            no trend with a variance that seems fairly constant over time, which resembles the behavior of white
            noise.
            The top-right plot shows the distribution of the residuals, which approaches a normal distribution,
            despite
            the unusual peak. This is further confirmed by the Q-Q plot at the bottom left, which displays a fairly
            straight line that lies on <i><b>y</b></i> = <i><b>x</b></i>. Finally, the
            correlogram in the bottom-right plot shows no
            significant
            autocorrelation coefficients after lag 0, which is exactly like white noise. From this analysis, the
            residuals resemble white noise. </p>
        <p>The next step is to run the Ljung-Box test on the residuals to make sure that they are
            independent and uncorrelated. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>residuals = ARIMA_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p><a id="calibre_link-89"></a> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>171</b></p>
        <p>The returned p-values are all greater than 0.05 except for the first two values. This means
            that, according to the Ljung-Box test, we reject the null hypothesis with a 5%</p>
        <p>chance of being wrong, since we set our significance boundary to 0.05. However, the third
            value and onwards are all greater than 0.05, so we reject the null hypothesis, concluding that the
            residuals
            are uncorrelated starting at lag 3. </p>
        <p>This is an interesting situation to dissect, because the graphical analysis of the residuals
            leads us to conclude that they resemble white noise, but the Ljung-Box test points to some correlation
            at
            lags 1 and 2. This means that our ARIMA model is not capturing all the information from the data. </p>
        <p>In this case, we'll move forward with the model, because we know that we are modeling
            seasonal data with a non-seasonal model. Therefore, the Ljung-Box test is really telling us that our
            model
            is not perfect, but that's okay, because part of this exercise is to compare the performance of ARIMA
            and
            SARIMA and demonstrate that SARIMA</p>
        <p>is the way to go when dealing with seasonal data. </p>
        <p>As previously mentioned, we wish to predict a full year of monthly air passengers, using the
            last 12 months of data as our test set. The baseline model is the naive seasonal forecast, where we
            simply
            use the number of air passengers for each month of 1959 as a forecast for each month of 1960. </p>
        <p><b>Create the test set. It </b></p>
        <p><b>corresponds to the last 12 </b></p>
        <p><b>The naive seasonal</b></p>
        <p><b>data points, which is the </b></p>
        <p><b>forecast simply reuses</b></p>
        <p><b>data for 1960. </b></p>
        <p><b>the data from 1959 as</b></p>
        <p>test = df.iloc[-12:] </p>
        <p><b>a forecast for 1960. </b></p>
        <p>test['naive_seasonal'] = df['Passengers'].iloc[120:132].values </p>
        <p>We can append the forecasts from our ARIMA(11,2,3) model to the test DataFrame. </p>
        <p>ARIMA_pred = ARIMA_model_fit.get_prediction(132, 143).predicted_mean </p>
        <p><b>Get predictions</b></p>
        <p>test['ARIMA_pred'] = ARIMA_pred </p>
        <p><b>Append </b></p>
        <p><b>for each month</b></p>
        <p><b>predictions </b></p>
        <p><b>of 1960. </b></p>
        <p><b>to test. </b></p>
        <p>With forecasts from the ARIMA model stored in test, we will now use a SARIMA model and later
            compare the performance of both models to see if the SARIMA model actually performs better than the
            ARIMA
            model when applied on a seasonal time series. </p>
        <p> <i><b>8.3.2</b></i></p>
        <p> <i><b>Forecasting with a SARIMA(p,d,q)(P,D,Q)m
                    model</b></i></p>
        <p>In the previous section, we used an ARIMA(11,2,3) model to forecast the number of monthly
            air passengers. Now we'll fit a SARIMA model and see if it performs better than the ARIMA model.
            Hopefully
            the SARIMA model will perform better, since it can capture seasonal information, and we know that our
            dataset exhibits clear seasonality, as shown in figure 8.10. </p>
        <p></p>
        <p><a id="calibre_link-387"></a><img src="images/000062.jpg" alt="Image 81" class="calibre2" />
        </p>
        <p><b>172</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>Figure 8.10</p>
        <p>Monthly total number of air passengers for an airline, from January 1949 </p>
        <p>to December 1960. You can see a clear seasonal pattern in the series, with peak traffic
            occurring toward the middle of the year. </p>
        <p>Following the steps in our general modeling procedure (figure 8.11), we'll first check for
            stationarity and apply the required transformations. </p>
        <p>ad_fuller_result = adfuller(df['Passengers'])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>The ADF test on the dataset returns an ADF statistic of 0.82 and a p-value of 0.99. </p>
        <p>Therefore, we cannot reject the null hypothesis and the series is not stationary. We can
            apply a first-order differencing and test for stationarity. </p>
        <p>df_diff = np.diff(df['Passengers'], n=1)</p>
        <p>ad_fuller_result = adfuller(df_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This returns an ADF statistic of &ndash;2.83 and a p-value of 0.054. Since the p-value is
            greater than 0.05, we cannot reject the null hypothesis, and the series is still non-stationary.
            Therefore,
            let's apply a seasonal difference and test for stationarity. </p>
        <p>df_diff_seasonal_diff = np.diff(df_diff, n=12) </p>
        <p><b>Seasonal differencing. Since we </b></p>
        <p><b>have monthly data, m = 12, so </b></p>
        <p>ad_fuller_result = adfuller(df_diff_seasonal_diff)</p>
        <p><b>the seasonal difference is the </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p><b>difference between two values </b></p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p><b>that are 12 timesteps apart. </b></p>
        <p><a id="calibre_link-388"></a> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>173</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>Set <i>D </i> equal to the</p>
        <p>number of times</p>
        <p>seasonal differencing</p>
        <p>was applied</p>
        <p>List values</p>
        <p>of <i>p, q, P, Q</i></p>
        <p>Fit every combination of</p>
        <p>SARIMA( , , </p>
        <p> <i>p d q</i>)( , , )</p>
        <p> <i>P D Q m</i></p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Yes</p>
        <p>Figure 8.11</p>
        <p>General modeling </p>
        <p>Ready for</p>
        <p>forecasts</p>
        <p>procedure for the SARIMA model</p>
        <p><a id="calibre_link-389"></a><b>174</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>This returns an ADF statistic of &ndash;17.63 and a p-value of 3.82 × 10&ndash;30. With a
            large and negative ADF statistic and a p-value smaller than 0.05, we can reject the null hypothesis and
            consider the transformed series as stationary. Therefore, we performed one round of differencing,
            meaning
            that <i>d</i> = 1, and one round of seasonal differencing, meaning that <i class="calibre3">D</i> = 1. </p>
        <p>With this step done, we can now define the range of possible values for <i>p</i>, <i class="calibre3">q</i>,
            <i>P</i>, and <i>Q</i>, fit each unique
            SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model, and select the one with the lowest AIC. </p>
        <p>ps = range(0, 4, 1) </p>
        <p><b>We try values of </b></p>
        <p>qs = range(0, 4, 1)</p>
        <p><b>[0,1,2,3] for p, q, </b></p>
        <p>Ps = range(0, 4, 1)</p>
        <p><b>P, and Q. </b></p>
        <p>Qs = range(0, 4, 1)</p>
        <p><b>Generate the unique </b></p>
        <p><b>combinations of </b></p>
        <p><b>orders. </b></p>
        <p>SARIMA_order_list = list(product(ps, qs, Ps, Qs)) </p>
        <p>train = df['Passengers'][:-12] </p>
        <p><b>The train set consists of all the data </b></p>
        <p><b>except the last 12 data points, which </b></p>
        <p>d = 1</p>
        <p><b>are used for the test set. </b></p>
        <p>D = 1</p>
        <p>s = 12</p>
        <p>SARIMA_result_df = optimize_SARIMA(train, SARIMA_order_list, d, D, s) </p>
        <p>SARIMA_result_df </p>
        <p><b>Display the </b></p>
        <p><b>Fit all SARIMA models</b></p>
        <p><b>result. </b></p>
        <p><b>on the training set. </b></p>
        <p>Once the function is done running, we find that the SARIMA(2,1,1)(1,1,2)12 model has the
            lowest AIC, which is a value of 892.24. We can fit this model again on the training set to perform
            residual
            analysis. </p>
        <p>We'll start by plotting the residuals' diagnostics in figure 8.12. </p>
        <p>SARIMA_model = SARIMAX(train, order=(2,1,1), seasonal_order=(1,1,2,12), </p>
        <p>➥ simple_differencing=False)</p>
        <p>SARIMA_model_fit = SARIMA_model.fit(disp=False)</p>
        <p>SARIMA_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>The results show that our residuals are completely random, which is exactly what we are
            looking for in a good model. </p>
        <p>The final test to determine whether we can use this model for forecasting or not is the
            Ljung-Box test. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>residuals = SARIMA_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p><a id="calibre_link-293"></a><img src="images/000053.jpg" alt="Image 82" class="calibre2" />
        </p>
        <p> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>175</b></p>
        <p>Figure 8.12</p>
        <p>Residuals' diagnostics of the SARIMA(2,1,1)(1,1,2)12 model. The top-left plot shows that the
            residuals do not exhibit a trend or a change in variance. The top-right plot shows that the residuals'
        </p>
        <p>distribution is very close to a normal distribution. This is further supported by the Q-Q
            plot at the bottom left, which displays a fairly straight line that lies on <i><b>y</b></i>
            = <i><b>x</b></i>. Finally, the
            correlogram at the bottom right shows no significant coefficients after lag 0. Therefore, everything
            leads
            to the conclusion that the residuals resemble white noise. </p>
        <p>The returned p-values are all greater than 0.05. Therefore, we do not reject the null
            hypothesis, and we conclude that the residuals are independent and uncorrelated, just like white noise.
        </p>
        <p>Our model has passed all the tests from the residuals analysis, and we are ready to use it
            for forecasting. Again, we'll forecast the number of monthly air passengers for the year of 1960 to
            compare
            the predicted values to the observed values in the test set. </p>
        <p>SARIMA_pred = SARIMA_model_fit.get_prediction(132, 143).predicted_mean </p>
        <p><b>Forecast the number of monthly</b></p>
        <p>test['SARIMA_pred'] = SARIMA_pred</p>
        <p><b>air passengers for the year 1960. </b></p>
        <p>Now that we have the results, we can compare the performance of each model and</p>
        <p>determine the best forecasting method for our problem. </p>
        <p><a id="calibre_link-90"></a><img src="images/000159.jpg" alt="Image 83" class="calibre2" />
        </p>
        <p><b>176</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p> <i><b>8.3.3</b></i></p>
        <p> <i><b>Comparing the performance of each forecasting
                    method</b></i></p>
        <p>We can now compare the performance of each forecasting method: the naive seasonal forecasts,
            the ARIMA model, and the SARIMA model. We'll use the mean absolute percentage error (MAPE) to evaluate
            each
            model. </p>
        <p>We can first visualize the forecasts against the observed values of the test set. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['Month'], df['Passengers'])</p>
        <p>ax.plot(test['Passengers'], 'b-', label='actual')</p>
        <p>ax.plot(test['naive_seasonal'], 'r:', label='naive seasonal')</p>
        <p>ax.plot(test['ARIMA_pred'], 'k--', label='ARIMA(11,2,3)')</p>
        <p>ax.plot(test['SARIMA_pred'], 'g-.', label='SARIMA(2,1,1)(1,1,2,12)')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Number of air passengers')</p>
        <p>ax.axvspan(132, 143, color='#808080', alpha=0.2)</p>
        <p>ax.legend(loc=2)</p>
        <p>plt.xticks(np.arange(0, 145, 12), np.arange(1949, 1962, 1))</p>
        <p>ax.set_xlim(120, 143) </p>
        <p><b>Zoom in on </b></p>
        <p><b>the test set</b></p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>The plot is shown in figure 8.13. The lines from the ARIMA and SARIMA models sit almost on
            top of the observed data, meaning that the predictions are very close to the observed data. </p>
        <p>Figure 8.13</p>
        <p>Forecasts of the number of monthly air passengers. The shaded area designates </p>
        <p>the test set. You can see that the curves coming from the ARIMA and SARIMA models almost
            obscure the observed data, which is indicative of good predictions. </p>
        <p><a id="calibre_link-390"></a><img src="images/000120.jpg" alt="Image 84" class="calibre2" />
        </p>
        <p> <i><b>8.3</b></i></p>
        <p> <i><b>Forecasting the number of monthly air
                    passengers</b></i></p>
        <p><b>177</b></p>
        <p>We can measure the MAPE of each model and display it in a bar plot, as shown in figure 8.14.
        </p>
        <p><b>Define a function to </b></p>
        <p><b>compute the MAPE. </b></p>
        <p><b>Compute the MAPE</b></p>
        <p>def mape(y_true, y_pred): </p>
        <p><b>for each forecasting</b></p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p><b>method. </b></p>
        <p>mape_naive_seasonal = mape(test['Passengers'], test['naive_seasonal']) </p>
        <p>mape_ARIMA = mape(test['Passengers'], test['ARIMA_pred'])</p>
        <p>mape_SARIMA = mape(test['Passengers'], test['SARIMA_pred'])</p>
        <p><b>Plot the MAPE </b></p>
        <p>fig, ax = plt.subplots() </p>
        <p><b>on a bar plot. </b></p>
        <p>x = ['naive seasonal', 'ARIMA(11,2,3)', 'SARIMA(2,1,1)(1,1,2,12)']</p>
        <p>y = [mape_naive_seasonal, mape_ARIMA, mape_SARIMA]</p>
        <p>ax.bar(x, y, width=0.4)</p>
        <p>ax.set_xlabel('Models')</p>
        <p>ax.set_ylabel('MAPE (%)')</p>
        <p>ax.set_ylim(0, 15)</p>
        <p><b>Display the MAPE as </b></p>
        <p><b>text in the bar plot. </b></p>
        <p>for index, value in enumerate(y): </p>
        <p>plt.text(x=index, y=value + 1, s=str(round(value,2)), ha='center')</p>
        <p>plt.tight_layout()</p>
        <p>Figure 8.14</p>
        <p>The MAPE of all forecasting methods. You can see that the best-performing </p>
        <p>model is the SARIMA model, since it has the lowest MAPE of all methods. </p>
        <p>In figure 8.14 you can see that our baseline achieves a MAPE of 9.99%. The ARIMA model
            produced forecasts with a MAPE of 3.85%, and the SARIMA model scored a</p>
        <p>MAPE of 2.85%. A MAPE closer to 0 is indicative of better predictions, so the SARIMA</p>
        <p><a id="calibre_link-91"></a><b>178</b></p>
        <p>CHAPTER 8</p>
        <p> <i><b>Accounting for seasonality</b></i></p>
        <p>model is the best-performing method for this situation. This makes sense, since our dataset
            had clear seasonality, and the SARIMA model is built to use the seasonal properties of time series to
            make
            forecasts. </p>
        <p> <i><b>8.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>,
            <i class="calibre3">Q</i>) <i>m</i> model, which allows us to
            model non-stationary seasonal time series. </p>
        <p>The addition of the parameters <i>P</i>, <i>D</i>, <i class="calibre3">Q</i>, and <i>m</i> allows us to
            include the seasonal properties
            of a
            time series in a model and use them to produce forecasts. Here, <i>P</i> is the order
            of
            the seasonal autoregressive process, <i>D</i> is the order of seasonal integration, <i
                class="calibre3">Q</i> is the order of the seasonal moving average process, and <i
                class="calibre3">m</i> is the frequency of the data. </p>
        <p>We looked at how to first detect seasonal patterns using time series decomposition, and we
            adapted our general modeling procedure to also test values for <i>P</i> and <i class="calibre3">Q</i>. </p>
        <p>In chapters 4 through 8, we have slowly built a more general and complex model, </p>
        <p>starting with the MA( <i>q</i>) and AR( <i>p</i>) models,
            combining them into the ARMA( <i>p</i>, <i>q</i>) model, which led us
            to
            the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) model, and
            finally to the SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)(
            <i>P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model. These models only consider the values of the time series itself. However, it would make sense
            that
            external variables are also predictive of our time series. For example, if we wish to model a country's
            total spending over time, looking at interest rates or the debt level could likely be predictive. How
            can we
            include those external variables in a model? </p>
        <p>This leads us to the <i>SARIMAX</i> model. Notice the addition of <i>X</i>,
            which stands for <i>exogenous variables</i>. This model will
            combine everything that we have learned so far and further expand on it by adding the effect of external
            variables to predict our target. </p>
        <p>This will be the subject of the next chapter. </p>
        <p> <i><b>8.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Take the time to experiment with the SARIMA model using this exercise. The full solution is
            on <a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH08">GitHub:
                https://github.com/marcopeix/TimeSeriesForecastingInPython/</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH08">tree/master/CH08.
            </a></p>
        <p> <i><b>8.5.1</b></i></p>
        <p> <i><b>Apply the SARIMA(p,d,q)(P,D,Q)m model on the
                    Johnson &amp; </b></i></p>
        <p> <i><b>Johnson dataset</b></i></p>
        <p>In chapter 7 we applied an ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model to the Johnson &amp;
            Johnson dataset to forecast the quarterly EPS
            over a
            year. Now use the SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)(
            <i>P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model on the same dataset, and compare its performance to the ARIMA model. </p>
        <p>1</p>
        <p>Use time series decomposition to identify the presence of a periodic pattern. </p>
        <p>2</p>
        <p>Use the optimize_SARIMA function and select the model with the lowest AIC. </p>
        <p>3</p>
        <p>Perform residual analysis. </p>
        <p>4</p>
        <p>Forecast the EPS for the last year, and measure the performance against the ARIMA model. Use
            the MAPE. Is it better? </p>
        <p><a id="calibre_link-296"></a> <i><b>Summary</b></i></p>
        <p><b>179</b></p>
        <p> <i><b>Summary</b></i></p>
        <p> The seasonal autoregressive integrated moving average model, denoted as SARIMA( <i>p</i>,
            <i>d</i>, <i>q</i>)( <i>P</i>, <i>D</i>,
            <i>Q</i>) <i>m</i>,
            adds seasonal properties to the ARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>) model. </p>
        <p> <i>P</i> is the order of the seasonal autoregressive process, <i>D</i> is
            the order of seasonal integration, <i>Q</i> is the order of
            the seasonal moving average process, and <i>m</i> is the frequency of the data. </p>
        <p> The frequency <i>m</i> corresponds to the number of observations in a
            cycle. If the data is collected every month, then <i>m</i> = 12. If data is collected
            every
            quarter, then <i>m</i> = 4. </p>
        <p> Time series decomposition can be used to identify seasonal patterns in a time</p>
        <p>series. </p>
        <p><a id="calibre_link-11"></a> <i>Adding external</i></p>
        <p> <i>variables to our model</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the SARIMAX model</p>
        <p> Exploring the use of external variables for </p>
        <p>forecasting</p>
        <p> Forecasting using the SARIMAX model</p>
        <p>In chapters 4 through 8, we have increasingly built a general model that allows us to
            consider more complex patterns in time series. We started our journey with the autoregressive and moving
            average processes before combining them into the ARMA model. Then we added a layer of complexity to
            model
            non-stationary time</p>
        <p>series, leading us to the ARIMA model. Finally, in chapter 8 we added yet another layer to
            ARIMA that allows us to consider seasonal patterns in our forecasts, which resulted in the SARIMA model.
        </p>
        <p>So far, each model that we have explored and used to produce forecasts has considered only
            the time series itself. In other words, past values of the time series were used as predictors of future
            values. However, it is possible that external variables also have an impact on our time series and can
            therefore be good predictors of future values. </p>
        <p>This brings us to the <i>SARIMAX</i> model. You'll notice the addition of
            the <i>X</i> term, which denotes exogenous variables. In statistics the term <i
                class="calibre3">exogenous</i> is used to describe <b>180</b></p>
        <p><a id="calibre_link-237"></a><img src="images/000049.jpg" alt="Image 85" class="calibre2" />
        </p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p><b>181</b></p>
        <p>predictors or input variables, while <i>endogenous</i> is used to define
            the target variable&mdash;</p>
        <p>what we are trying to predict. With the SARIMAX model, we can now consider external
            variables, or exogenous variables, when forecasting a time series. </p>
        <p>As a guiding example, we'll use a macroeconomics dataset from the United States, collected
            quarterly from 1959 to 2009, to forecast the real gross domestic product (GDP), as shown in figure 9.1.
        </p>
        <p>Figure 9.1</p>
        <p>Real gross domestic product (GDP) of the United States from 1959 to 2009. </p>
        <p>The data was collected quarterly and is expressed in thousands of US dollars. Notice the
            clear positive trend over the years with no cyclical pattern, suggesting that seasonality is not present
            in
            the series. </p>
        <p>The GDP is the total market value of all the finished goods and services produced within a
            country. The <i>real</i> GDP is an inflation-adjusted measure that removes the impact
            of
            inflation on the market value of goods. Inflation or deflation can respectively increase or decrease the
            monetary value of goods and services, hence increasing or decreasing the GDP. By removing the effect of
            inflation, we can better determine whether an economy saw an expansion of production. </p>
        <p>Without diving into the technicalities of measuring the GDP, we'll define the GDP</p>
        <p>as the sum of consumption <i>C</i>, government spending <i>G</i>,
            investments <i>I</i>, and net exports <i>NX</i>, as shown in equation
            9.1.
        </p>
        <p>GDP = <i>C</i> + <i>G</i> + <i>I</i> + <i class="calibre3">NX</i></p>
        <p>Equation 9.1</p>
        <p>Each element of equation 9.1 is likely affected by some external variable. For example,
            consumption is likely impacted by the unemployment rate, because if fewer</p>
        <p><a id="calibre_link-294"></a><b>182</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>people are employed, consumption is likely to decrease. Interest rates can also have an
            impact, because if they go up, it is harder to borrow money, and spending decreases as a result. We can
            also
            think of currency exchange rates as having an impact on net exports. A weaker local currency will
            generally
            stimulate exports and make imports more expensive. Thus, we can see how many exogenous variables can
            likely
            impact the real GDP of the United States. </p>
        <p>In this chapter, we'll first examine the SARIMAX model and explore an important</p>
        <p>caveat when using it to produce forecasts. Then we'll apply the model to forecast the real
            GDP of the United States. </p>
        <p> <i><b>9.1</b></i></p>
        <p> <i><b>Examining the SARIMAX model</b></i></p>
        <p>The SARIMAX model further extends the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>,
            <i>D</i>, <i class="calibre3">Q</i>) <i>m</i> model by adding the effect
            of exogenous variables. Therefore, we can express the present value <i>yt</i> simply as
            a
            SARIMA( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            model to which we add any number of exogenous variables <i>Xt</i> as shown in equation
            9.2.
        </p>
        <p>Equation 9.2</p>
        <p>The SARIMA model is a linear model, as it is a linear combination of past values of the
            series and error terms. Here we add another linear combination of different exogenous variables,
            resulting
            in SARIMAX being a linear model as well. Note that in SARIMAX you can include categorical variables as
            exogenous variables, but make sure you encode them (give them numerical values or binary flags) just
            like
            you would do for traditional regression tasks. </p>
        <p>We have been using the SARIMAX function from statsmodels since chapter 4 to implement
            different models. This is because SARIMAX is the most general function for forecasting a time series.
            You
            now understand how a SARIMAX model without exogenous variables is a SARIMA model. Similarly, a model
            with no
            seasonality but with exogenous variables can be denoted as an ARIMAX model, and a model with no
            seasonality
            and no exogenous variables becomes an ARIMA model. Depending on the problem, different combinations of
            each
            portion of the general SARIMAX model will be used. </p>
        <p>SARIMAX model</p>
        <p>The SARIMAX model simply adds a linear combination of exogenous variables to the SARIMA
            model. This allows us to model the impact of external variables on the future value of a time series.
        </p>
        <p>We can loosely define the SARIMAX model as follows:</p>
        <p><a id="calibre_link-92"></a> <i><b>9.1</b></i></p>
        <p> <i><b>Examining the SARIMAX model</b></i></p>
        <p><b>183</b></p>
        <p>The SARIMAX model is the most general model for forecasting time series. You can see that if
            you have no seasonal patterns, it becomes an ARIMAX model. With no</p>
        <p>exogenous variables, it is a SARIMA model. With no seasonality or exogenous variables, it
            becomes an ARIMA model. </p>
        <p>Theoretically, this sums up the SARIMAX model. Chapters 4 through 8 were purposely ordered
            in such a way that we incrementally developed the SARIMAX model, </p>
        <p>making the addition of exogenous variables easy to understand. To reinforce your learning,
            let's explore the exogenous variables of our dataset. </p>
        <p> <i><b>9.1.1</b></i></p>
        <p> <i><b>Exploring the exogenous variables of the US
                    macroeconomics </b></i></p>
        <p> <i><b>dataset</b></i></p>
        <p>Let's load the US macroeconomics dataset and explore the different exogenous variables
            available to us to forecast the real GDP. This dataset is available with the statsmodels library,
            meaning
            that you do not need to download and read an external file. </p>
        <p>You can load the dataset using the datasets module of statsmodels. </p>
        <p>NOTE</p>
        <p>The full source code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH09">https://</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH09">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH09.
            </a></p>
        <p><b>Load the US</b></p>
        <p>import statsmodels.api as sm</p>
        <p><b>macroeconomics dataset. </b></p>
        <p>macro_econ_data = sm.datasets.macrodata.load_pandas().data </p>
        <p>macro_econ_data </p>
        <p><b>Display the </b></p>
        <p><b>DataFrame. </b></p>
        <p>This displays the entire DataFrame containing the US macroeconomics dataset. </p>
        <p>Table 9.1 describes the meaning of each variable. We have our target variable, or endogenous
            variable, which is the real GDP. Then we have 11 exogenous variables that can be used for forecasting,
            such
            as personal and federal consumption expenditures, interest rate, inflation rate, population, and others.
        </p>
        <p>Table 9.1</p>
        <p>Description of all variables in the US macroeconomics dataset</p>
        <p>Variable</p>
        <p>Description</p>
        <p>realgdp</p>
        <p>Real gross domestic product (the target variable or endogenous variable)</p>
        <p>realcons</p>
        <p>Real personal consumption expenditure</p>
        <p>realinv</p>
        <p>Real gross private domestic investment</p>
        <p>realgovt</p>
        <p>Real federal consumption expenditure and investment</p>
        <p>realdpi</p>
        <p>Real private disposable income</p>
        <p>cpi</p>
        <p>Consumer price index for the end of the quarter</p>
        <p><a id="calibre_link-242"></a><b>184</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>Table 9.1</p>
        <p>Description of all variables in the US macroeconomics dataset <i><b>
                    (continued)</b></i></p>
        <p>Variable</p>
        <p>Description</p>
        <p>m1</p>
        <p>M1 nominal money stock</p>
        <p>tbilrate</p>
        <p>Quarterly monthly average of the monthly 3-month treasury bill</p>
        <p>unemp</p>
        <p>Unemployment rate</p>
        <p>pop</p>
        <p>Total population at the end of the quarter</p>
        <p>infl</p>
        <p>Inflation rate</p>
        <p>realint</p>
        <p>Real interest rate</p>
        <p>Of course, each of these variables may or may not be a good predictor of the real GDP. We do
            not have to perform feature selection because the linear model will attribute a coefficient close to 0
            for
            exogenous variables that are not significant in predicting the target. </p>
        <p>For the sake of simplicity and clarity, we will only work with six variables in this
            chapter: the real GDP, which is our target, and the next five variables listed in table 9.1</p>
        <p>(realcons to cpi) as our exogenous variables. </p>
        <p>We can visualize how each variable behaves through time to see if we can discern any
            distinctive patterns. The result is shown in figure 9.2. </p>
        <p>fig, axes = plt.subplots(nrows=3, ncols=2, dpi=300, figsize=(11,6))</p>
        <p><b>Iterate for six variables. </b></p>
        <p>for i, ax in enumerate(axes.flatten()[:6]): </p>
        <p>data = macro_econ_data[macro_econ_data.columns[i+2]] </p>
        <p><b>Skip the year and </b></p>
        <p></p>
        <p><b>quarter columns. </b></p>
        <p>ax.plot(data, color='black', linewidth=1)</p>
        <p><b>That way, we can </b></p>
        <p>ax.set_title(macro_econ_data.columns[i+2]) </p>
        <p><b>start at realgdp. </b></p>
        <p>ax.xaxis.set_ticks_position('none')</p>
        <p>ax.yaxis.set_ticks_position('none')</p>
        <p><b>Display the variable's </b></p>
        <p>ax.spines['top'].set_alpha(0)</p>
        <p><b>name at the top of </b></p>
        <p>ax.tick_params(labelsize=6)</p>
        <p><b>the plot. </b></p>
        <p>plt.setp(axes, xticks=np.arange(0, 208, 8), xticklabels=np.arange(1959, </p>
        <p>➥ 2010, 2))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>There are two ways to work with exogenous variables for time series forecasting. First, we
            could train multiple models with various combinations of exogenous variables, and see which model
            generates
            the best forecasts. Alternatively, we can simply include all exogenous variables and stick to model
            selection using the AIC, as we know this yields a good-fitting model that does not overfit. </p>
        <p><a id="calibre_link-93"></a><img class="calibre2" src="images/000079.jpg" alt="Image 86" />
        </p>
        <p> <i><b>9.1</b></i></p>
        <p> <i><b>Examining the SARIMAX model</b></i></p>
        <p><b>185</b></p>
        <p>realgdp</p>
        <p>realcons</p>
        <p>12000</p>
        <p>8000</p>
        <p>10000</p>
        <p>6000</p>
        <p>8000</p>
        <p>6000</p>
        <p>4000</p>
        <p>4000</p>
        <p>2000</p>
        <p>realinv</p>
        <p>realgovt</p>
        <p>1000</p>
        <p>2000</p>
        <p>900</p>
        <p>1500</p>
        <p>800</p>
        <p>700</p>
        <p>1000</p>
        <p>600</p>
        <p>500</p>
        <p>500</p>
        <p>realdpi</p>
        <p>cpi</p>
        <p>10000</p>
        <p>200</p>
        <p>8000</p>
        <p>150</p>
        <p>6000</p>
        <p>100</p>
        <p>4000</p>
        <p>50</p>
        <p>2000</p>
        <p>
            1959196119631965196719711973197519771979198119831985198719891991199319951997199920012003200520072009</p>
        <p>
            1959196119631965196719711973197519771979198119831985198719891991199319951997199920012003200520072009</p>
        <p>Figure 9.2</p>
        <p>Evolution of the real GDP and five exogenous variables from 1959 to 2009. You'll notice that
            <b>realgdp</b>, <b>realcons</b>, <b>realdpi</b>, and
            <b>cpi</b> all have a similar shape, which means that <b>realcons</b>,
            <b>realdpi</b>, and <b>cpi</b> are potentially good predictors,
            although
            a graphical analysis is not sufficient to confirm that idea. On the other hand, <b
                class="calibre4">realgovt</b> has peaks and troughs that do not appear in <b
                class="calibre4">realgdp</b>, so we can hypothesize that <b>realgovt</b> is a
            weaker
            predictor. </p>
        <p>Why disregard the p-value in regression analysis? </p>
        <p>The SARIMAX implementation in statsmodels comes with a regression analysis using the summary
            method. This is shown later in the chapter. </p>
        <p>In that analysis, we can see the p-value associated with each coefficient of each predictor
            of the SARIMAX model. Often the p-value is misused as a way to perform feature selection. Many
            incorrectly
            interpret the p-value as a way to determine if a predictor is correlated with the target. </p>
        <p>In fact, the p-value tests whether the coefficient is significantly different from 0 or not.
        </p>
        <p>If the p-value is less than 0.05, then we reject the null hypothesis and conclude that the
            coefficient is significantly different from 0. It does not determine whether a predictor is useful for
            forecasting. </p>
        <p>Therefore, you should not remove predictors based on their p-values. Selecting the model by
            minimizing the AIC takes care of that step. </p>
        <p>To learn more, I recommend reading Rob Hyndman's “Statistical tests for variable selection”
            blog post: <a href="https://robjhyndman.com/hyndsight/tests2/">https://robjhyndman.com/hyndsight/tests2/.
            </a></p>
        <p> <i><b>9.1.2</b></i></p>
        <p> <i><b>Caveat for using SARIMAX</b></i></p>
        <p>There is an important caveat that comes with the use of the SARIMAX model. Including
            external variables can potentially be beneficial, as you may find strong predictors</p>
        <p><a id="calibre_link-94"></a><b>186</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>for your target. However, you might encounter issues when forecasting multiple timesteps
            into the future. </p>
        <p>Recall that the SARIMAX model uses the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>,
            <i>D</i>, <i class="calibre3">Q</i>) <i>m</i> model and a linear
            combination of exogenous variables to predict one timestep into the future. But what if you wish to
            predict
            two timesteps into the future? While this is possible with a SARIMA model, the SARIMAX model requires us
            to
            forecast the exogenous variables too. </p>
        <p>To illustrate this idea, let's assume that realcons is a predictor of realgdp (this will be
            verified later in the chapter). Assume also that we have a SARIMAX model where realcons is used as an
            input
            feature to predict realgdp. Now suppose that we are at the end of 2009 and must predict the real GDP for
            2010 and 2011. The SARIMAX</p>
        <p>model allows us to use the realcons of 2009 to predict the real GDP for 2010. However,
            predicting the real GDP for 2011 will require us to predict realcons for 2010, unless we wait to observe
            the
            value at the end of 2010. </p>
        <p>Because the realcons variable is a time series itself, it can be forecast using a version of
            the SARIMA model. Nevertheless, we know that our forecast always has some error associated with it.
            Therefore, having to forecast an exogenous variable to forecast our target variable can magnify the
            prediction error of our target, meaning that our predictions can quickly degrade as we predict more
            timesteps into the future. </p>
        <p>The only way to avoid that situation is to predict only one timestep into the future and
            wait to observe the exogenous variable before predicting the target for another timestep into the
            future.
        </p>
        <p>On the other hand, if your exogenous variable is <i>easy</i> to predict,
            meaning that it follows a known function that can be accurately predicted, there is no harm in
            forecasting
            the exogenous variable and using these forecasts to predict the target. </p>
        <p>In the end, there is no clear recommendation to predict only one timestep. It is dependent
            on the situation and the exogenous variables available. This is where your expertise as a data scientist
            and
            rigorous experimenting come into play. If you determine that your exogenous variable can be accurately
            predicted, you can recommend forecasting many timesteps into the future. Otherwise, your recommendation
        </p>
        <p>must be to predict one timestep at a time and justify your decision by explaining that
            errors will accumulate as more predictions are made, meaning that the forecasts will lose accuracy. </p>
        <p>Now that we have explored the SARIMAX model in depth, let's apply it to forecast the real
            GDP. </p>
        <p> <i><b>9.2</b></i></p>
        <p> <i><b>Forecasting the real GDP using the SARIMAX
                    model</b></i></p>
        <p>We are now ready to use the SARIMAX model to forecast the real GDP. Having explored the
            exogenous variables of the dataset, we will incorporate them into our forecasting model. </p>
        <p>Before diving in, we must reintroduce the general modeling procedure. There are</p>
        <p>no major changes to the procedure. The only modification is that we will now fit a SARIMAX
            model. All the other steps remain the same, as shown in figure 9.3. </p>
        <p><a id="calibre_link-391"></a> <i><b>9.2</b></i></p>
        <p> <i><b>Forecasting the real GDP using the SARIMAX
                    model</b></i></p>
        <p><b>187</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>Set <i>D </i> equal to the</p>
        <p>number of times</p>
        <p>seasonal differencing</p>
        <p>was applied</p>
        <p>List values</p>
        <p>of <i>p, q, P, Q</i></p>
        <p>Fit every combination of</p>
        <p>SARIMAX( , , </p>
        <p> <i>p d q</i>)( , , )</p>
        <p> <i>P D Q m</i></p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Figure 9.3</p>
        <p>General modeling procedure for the </p>
        <p>SARIMAX model. This procedure can be applied </p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>to any problems, as the SARIMAX model is the </p>
        <p>most general forecasting model and can </p>
        <p>accommodate all the different processes and </p>
        <p>properties of time series that we have explored. </p>
        <p>Yes</p>
        <p>Notice that the only change here is that we are </p>
        <p>fitting a SARIMAX model instead of a SARIMA </p>
        <p>model as we did in chapter 8. The rest of the </p>
        <p>Ready for</p>
        <p>forecasts</p>
        <p>procedure remains the same. </p>
        <p><a id="calibre_link-202"></a><b>188</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>Following the modeling procedure of figure 9.3, we'll first check for the stationarity of
            our target using the augmented Dickey-Fuller (ADF) test. </p>
        <p><b>Define the target variable. In </b></p>
        <p><b>this case, it is the real GDP. </b></p>
        <p>target = macro_econ_data['realgdp'] </p>
        <p>exog = macro_econ_data[['realcons', 'realinv', 'realgovt', 'realdpi', </p>
        <p>➥ 'cpi']] </p>
        <p><b>Define the exogenous </b></p>
        <p><b>variables. Here we limit it to </b></p>
        <p>ad_fuller_result = adfuller(target)</p>
        <p><b>five variables for simplicity. </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This returns an ADF statistic of 1.75 and a p-value of 1.00. Since the ADF statistic is not
            a large negative number, and the p-value is larger than 0.05, we cannot reject the null hypothesis and
            conclude that the series is not stationary. </p>
        <p>Therefore, we must apply a transformation and test for stationarity again. Here we will
            difference the series once:</p>
        <p><b>Difference </b></p>
        <p>target_diff = target.diff() </p>
        <p><b>the series. </b></p>
        <p>ad_fuller_result = adfuller(target_diff[1:])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This now returns an ADF statistic of &ndash;6.31 and p-value of 3.32 × 10&ndash;8. With a
            large negative ADF statistic and a p-value smaller than 0.05, we can reject the null hypothesis and
            conclude
            that the series is now stationary. Therefore, we know that <i>d</i> = 1. Since we did
            not
            need to take a seasonal difference to make the series stationary, <i>D</i> = 0. </p>
        <p>We will now define the optimize_SARIMAX function, which will fit all unique com-</p>
        <p>binations of the model and return a DataFrame in ascending order of AIC. </p>
        <p>Listing 9.1</p>
        <p>Function to fit all unique SARIMAX models</p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_SARIMAX(endog: Union[pd.Series, list], exog: Union[pd.Series, </p>
        <p>➥ list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:</p>
        <p></p>
        <p>results = []</p>
        <p></p>
        <p>for order in tqdm_notebook(order_list):</p>
        <p>try: </p>
        <p>model = SARIMAX(</p>
        <p>endog, </p>
        <p><a id="calibre_link-392"></a> <i><b>9.2</b></i></p>
        <p> <i><b>Forecasting the real GDP using the SARIMAX
                    model</b></i></p>
        <p><b>189</b></p>
        <p>exog, </p>
        <p><b>Notice the addition of</b></p>
        <p>order=(order[0], d, order[1]), </p>
        <p><b>the exogenous variables</b></p>
        <p>seasonal_order=(order[2], D, order[3], s), </p>
        <p><b>when fitting the model. </b></p>
        <p>simple_differencing=False).fit(disp=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p>aic = model.aic</p>
        <p>results.append([order, aic])</p>
        <p></p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['(p,q,P,Q)', 'AIC']</p>
        <p></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df</p>
        <p>Next we'll define the range of possible values for the orders <i>p</i>, <i class="calibre3">q</i>, <i>P</i>,
            and <i>Q</i>. We'll try values
            from
            0 to 3, but feel free to try a different set of values. Also, since the data is collected quarterly, <i
                class="calibre3">m</i> = 4. </p>
        <p>p = range(0, 4, 1)</p>
        <p>d = 1</p>
        <p>q = range(0, 4, 1)</p>
        <p>P = range(0, 4, 1)</p>
        <p>D = 0</p>
        <p><b>Remember that s in the </b></p>
        <p><b>implementation of SARIMAX from </b></p>
        <p>Q = range(0, 4, 1)</p>
        <p><b>statsmodels is equivalent to m. </b></p>
        <p>s = 4 </p>
        <p>parameters = product(p, q, P, Q)</p>
        <p>parameters_list = list(parameters) </p>
        <p>To train the model, we will use the first 200 instances of both the target and exogenous
            variables. We'll then run the optimize_SARIMAX function and select the model with the lowest AIC. </p>
        <p>target_train = target[:200]</p>
        <p>exog_train = exog[:200]</p>
        <p>result_df = optimize_SARIMAX(target_train, exog_train, parameters_list, d, </p>
        <p>➥ D, s)</p>
        <p>result_df</p>
        <p>Once it's completed, the function returns the verdict that the SARIMAX(3,1,3)(0,0,0)4</p>
        <p>model is the model with the lowest AIC. Notice that the seasonal component of the model has
            only orders of 0. This makes sense, as there is no visible seasonal pattern in</p>
        <p><a id="calibre_link-393"></a><img src="images/000022.jpg" alt="Image 87" class="calibre2" />
        </p>
        <p><b>190</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>the plot of real GDP, as shown in figure 9.4. Therefore, the seasonal component is null, and
            we have an ARIMAX(3,1,3) model. </p>
        <p>Figure 9.4</p>
        <p>Real gross domestic product (GDP) of the United States between 1959 and </p>
        <p>2009. The data is collected quarterly and is expressed in thousands of US dollars. Notice
            the clear positive trend over the years with no cyclical pattern, suggesting that seasonality is not
            present
            in the series. </p>
        <p>Now we can fit the selected model and display a summary table to see the coefficients
            associated with our exogenous variables. The result is shown in figure 9.5. </p>
        <p>best_model = SARIMAX(target_train, exog_train, order=(3,1,3), </p>
        <p>➥ seasonal_order=(0,0,0,4), simple_differencing=False)</p>
        <p>best_model_fit = best_model.fit(disp=False)</p>
        <p><b>Display the summary </b></p>
        <p>print(best_model_fit.summary()) </p>
        <p><b>table of the model. </b></p>
        <p>In figure 9.5 you'll notice that all exogenous variables have a p-value smaller than 0.05,
            except for realdpi, which has a p-value of 0.712. This means that the coefficient of realdpi is not
            significantly different from 0. You'll also notice that its coefficient is 0.0091. However, the
            coefficient
            is kept in the model, as the p-value does not determine the relevance of this predictor in forecasting
            our
            target. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-254"></a><img src="images/000121.jpg" alt="Image 88" class="calibre2" />
        </p>
        <p> <i><b>9.2</b></i></p>
        <p> <i><b>Forecasting the real GDP using the SARIMAX
                    model</b></i></p>
        <p><b>191</b></p>
        <p>Figure 9.5</p>
        <p>Summary table of the selected model. You can see that our exogenous variables were assigned
            coefficients. You can also see their p-values under the column P&gt;|z|. </p>
        <p>Moving on with the modeling procedure, we'll now study the residuals of the model, which are
            shown in figure 9.6. Everything points to the residuals being completely random, just like white noise.
            Our
            model passes the visual check. </p>
        <p>best_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>Now we'll apply the Ljung-Box test to make sure the residuals are not correlated. We
            therefore want to see p-values that are greater than 0.05, since the null hypothesis of the Ljung-Box
            test
            is that residuals are independent and uncorrelated. </p>
        <p>residuals = best_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p><a id="calibre_link-289"></a><img src="images/000137.jpg" alt="Image 89" class="calibre2" />
        </p>
        <p><b>192</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>Figure 9.6</p>
        <p>Residual analysis of the selected model. You can see that the residuals have no trend and a
            fairly constant variance over time, just like white noise. In the top-right plot, the distribution of
            residuals is very close to a normal distribution. This is further supported by the Q-Q plot at the
            bottom
            left, which shows a fairly straight line that lies on <i><b>y</b></i>
            = <i><b>x</b></i>. Finally, the correlogram shows no significant
            coefficients after lag 0, just like white noise. Therefore, from a graphical analysis, the residuals of
            this
            model resemble white noise. </p>
        <p>All the p-values are greater than 0.05. Therefore, we do not reject the null hypothesis, and
            we conclude that the residuals are independent and uncorrelated. Having passed both residual checks, our
            model can be used for forecasting. </p>
        <p>As mentioned before, the caveat of using a SARIMAX model is that it is reasonable to predict
            only the next timestep, to avoid predicting the exogenous variables as well, which would lead us to
            accumulate prediction errors in the final forecast. </p>
        <p>Instead, to test our model, we predict the next timestep multiple times and average the
            errors of each prediction. This is done using the rolling_forecast function, which we defined and worked
            with in chapters 4&ndash;6. As a baseline model, we will use the last known value method. </p>
        <p></p>
        <p><a id="calibre_link-300"></a> <i><b>9.2</b></i></p>
        <p> <i><b>Forecasting the real GDP using the SARIMAX
                    model</b></i></p>
        <p><b>193</b></p>
        <p>Listing 9.2</p>
        <p>Function to forecast the next timestep multiple times</p>
        <p>def rolling_forecast(endog: Union[pd.Series, list], exog: </p>
        <p>➥ Union[pd.Series, list], train_len: int, horizon: int, window: int, </p>
        <p>➥ method: str) -&gt; list:</p>
        <p></p>
        <p>total_len = train_len + horizon</p>
        <p>if method == 'last':</p>
        <p>pred_last_value = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>last_value = endog[:i].iloc[-1]</p>
        <p>pred_last_value.extend(last_value for _ in range(window))</p>
        <p></p>
        <p>return pred_last_value</p>
        <p></p>
        <p>elif method == 'SARIMAX':</p>
        <p>pred_SARIMAX = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>model = SARIMAX(endog[:i], exog[:i], order=(3,1,3), </p>
        <p>➥ seasonal_order=(0,0,0,4), simple_differencing=False)</p>
        <p>res = model.fit(disp=False)</p>
        <p>predictions = res.get_prediction(exog=exog)</p>
        <p>oos_pred = predictions.predicted_mean.iloc[-window:]</p>
        <p>pred_SARIMAX.extend(oos_pred)</p>
        <p></p>
        <p>return pred_SARIMAX</p>
        <p>The recursive_forecast function allows us to predict the next timestep over a certain period
            of time. Specifically, we will use it to forecast the next timestep starting in 2008 and going to the
            third
            quarter of 2009. </p>
        <p><b>We fit the model on the data </b></p>
        <p><b>from 1959 to the end of 2007. </b></p>
        <p>target_train = target[:196] </p>
        <p>target_test = target[196:] </p>
        <p><b>The test set contains the values </b></p>
        <p><b>starting in 2008 to the third </b></p>
        <p>pred_df = pd.DataFrame({'actual': target_test})</p>
        <p><b>quarter of 2009. There is a total </b></p>
        <p><b>of seven values to predict. </b></p>
        <p>TRAIN_LEN = len(target_train)</p>
        <p>HORIZON = len(target_test)</p>
        <p><b>This specifies that we predict </b></p>
        <p>WINDOW = 1 </p>
        <p><b>the next timestep only. </b></p>
        <p>pred_last_value = recursive_forecast(target, exog, TRAIN_LEN, HORIZON, </p>
        <p>➥ WINDOW, 'last')</p>
        <p>pred_SARIMAX = recursive_forecast(target, exog, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'SARIMAX')</p>
        <p>pred_df['pred_last_value'] = pred_last_value</p>
        <p>pred_df['pred_SARIMAX'] = pred_SARIMAX</p>
        <p>pred_df</p>
        <p><a id="calibre_link-263"></a><img src="images/000148.jpg" alt="Image 90" class="calibre2" />
        </p>
        <p><b>194</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p>With the predictions done, we can visualize which model has the lowest mean absolute
            percentage error (MAPE). The result is shown in figure 9.7. </p>
        <p>def mape(y_true, y_pred):</p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p>mape_last = mape(pred_df.actual, pred_df.pred_last_value)</p>
        <p>mape_SARIMAX = mape(pred_df.actual, pred_df.pred_SARIMAX)</p>
        <p>fig, ax = plt.subplots()</p>
        <p>x = ['naive last value', 'SARIMAX']</p>
        <p>y = [mape_last, mape_SARIMAX]</p>
        <p>ax.bar(x, y, width=0.4)</p>
        <p>ax.set_xlabel('Models')</p>
        <p>ax.set_ylabel('MAPE (%)')</p>
        <p>ax.set_ylim(0, 1)</p>
        <p>for index, value in enumerate(y):</p>
        <p>plt.text(x=index, y=value + 0.05, s=str(round(value,2)), ha='center')</p>
        <p>plt.tight_layout()</p>
        <p>Figure 9.7</p>
        <p>The mean absolute percentage error (MAPE) of the forecasts of each </p>
        <p>method. You can see that the SARIMAX model only has a slightly smaller MAPE than the
            baseline. This highlights the importance of using a baseline, as a MAPE of 0.70% </p>
        <p>is extremely good, but a naive forecast achieves a MAPE of 0.74%, meaning that the SARIMAX
            model only has a small advantage. </p>
        <p>In figure 9.7 you'll see that the SARIMAX model is the winning model by only 0.04%. </p>
        <p>You'll appreciate the importance of a baseline here, as both methods achieve an</p>
        <p><a id="calibre_link-95"></a> <i><b>9.3</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p><b>195</b></p>
        <p>extremely low MAPE, showing that the SARIMAX model is only slightly better than simply
            predicting the last value. This is where the business context comes into play. In our case, since we are
            predicting the real GDP of the United States, a difference of 0.04% represents thousands of dollars.
            This
            difference might be relevant in this particular context, justifying the use of the SARIMAX model, even
            though it is only slightly better than the baseline. </p>
        <p> <i><b>9.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the SARIMAX model, which allows us to include external variables
            when forecasting our target time series. </p>
        <p>The addition of exogenous variables comes with a caveat: if we need to predict many
            timesteps into the future, we must also predict the exogenous variables, which can magnify the
            prediction
            error on the target. To avoid that, we must only predict the next timestep. </p>
        <p>In considering exogenous variables for predicting real GDP, we can also hypothe-</p>
        <p>size that real GDP can be a predictor for other variables. For example, the variable cpi was
            a predictor for realgdp, but we could also show that realgdp can predict cpi. </p>
        <p>In a situation where we wish to show that two variables varying in time can impact one
            another, we must use the <i>vector autoregression</i> (VAR) model. This model allows
            for
            multivariate time series forecasting, unlike the SARIMAX model, which is for univariate time series
            forecasting. In the next chapter we will explore the VAR model in detail, and you'll see that it can
            also be
            extended to become a <i>VARMA</i> model and a <i>VARMAX</i> model.
        </p>
        <p> <i><b>9.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Take the time to test your knowledge with this exercise. The full solution is on GitHub:</p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH09">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH09</a>.
        </p>
        <p> <i><b>9.4.1</b></i></p>
        <p> <i><b>Use all exogenous variables in a SARIMAX model to
                    predict </b></i></p>
        <p> <i><b>the real GDP</b></i></p>
        <p>In this chapter we limited the number of exogenous variables when forecasting for the real
            GDP. This exercise is an occasion to fit a SARIMAX model using all exogenous variables and to verify if
            you
            can achieve better performance. </p>
        <p>1</p>
        <p>Use all exogenous variables in the SARIMAX model. </p>
        <p>2</p>
        <p>Perform residual analysis. </p>
        <p>3</p>
        <p>Produce forecasts for the last seven timesteps in the dataset. </p>
        <p>4</p>
        <p>Measure the MAPE. Is it better, worse, or identical to what was achieved with a</p>
        <p>limited number of exogenous variables? </p>
        <p><a id="calibre_link-241"></a><b>196</b></p>
        <p>CHAPTER 9</p>
        <p> <i><b>Adding external variables to our model</b></i></p>
        <p> <i><b>Summary</b></i></p>
        <p> The SARIMAX model allows you to include external variables, also termed exogenous
            variables, to forecast your target. </p>
        <p> Transformations are applied only on the target variable, not on the exogenous</p>
        <p>variables. </p>
        <p> If you wish to forecast multiple timesteps into the future, the exogenous variables must
            also be forecast. This can magnify the errors on the final forecast. To avoid that, you must predict
            only
            the next timestep. </p>
        <p><a id="calibre_link-12"></a> <i>Forecasting</i></p>
        <p> <i>multiple time series</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the VAR model</p>
        <p> Exploring Granger causality to validate the use </p>
        <p>of the VAR model</p>
        <p> Forecasting multiple time series using the VAR </p>
        <p>model</p>
        <p>In the last chapter, you saw how the SARIMAX model can be used to include the</p>
        <p>impact of exogenous variables on a time series. With the SARIMAX model, the relationship is
            unidirectional: we assume that the exogenous variable has an impact on the target only. </p>
        <p>However, it is possible that two time series have a bidirectional relationship, meaning that
            time series t1 is a predictor of time series t2, and time series t2 is also a predictor for time series
            t1.
            In such a case, it would be useful to have a model that can take this bidirectional relationship into
            account and output predictions for <i>both</i> time series simultaneously. </p>
        <p><b>197</b></p>
        <p><a id="calibre_link-394"></a><img src="images/000165.jpg" alt="Image 91" class="calibre2" />
        </p>
        <p><b>198</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>This brings us to the <i>vector autoregression</i> (VAR) model. This
            particular model allows us to capture the relationship between multiple time series as they change over
            time. </p>
        <p>That, in turn, allows us to produce forecasts for many time series simultaneously, therefore
            performing multivariate forecasting. </p>
        <p>Throughout this chapter, we will use the same US macroeconomics dataset as in</p>
        <p>chapter 9. This time we'll explore the relationship between real disposable income and real
            consumption, as shown in figure 10.1. </p>
        <p>Figure 10.1</p>
        <p>Real disposable income (<b>realdpi</b>) and real consumption (<b class="calibre4">realcons</b>) in the United
            States from 1959 to 2009. The data is collected
            quarterly
            and is expressed in thousands of US dollars. Both series have a similar shape and trend over time. </p>
        <p>Real consumption expresses how much money people spend, while real disposable income
            represents how much money is available to spend. Therefore, it is a reasonable hypothesis that a higher
            amount of disposable income could signal higher consumption. The opposite can also be true, with higher
            consumption meaning that more</p>
        <p><a id="calibre_link-283"></a> <i><b>10.1</b></i></p>
        <p> <i><b>Examining the VAR model</b></i></p>
        <p><b>199</b></p>
        <p>income is available for spending. This bidirectional relationship can be captured by a VAR
            model. </p>
        <p>In this chapter, we'll first explore the VAR model in detail. Then, we'll introduce the
            Granger-causality test, which will help us validate the hypothesis that two time series have an impact
            on
            one another. Finally, we'll apply the VAR model to produce forecasts for both real consumption and real
            disposable income. </p>
        <p> <i><b>10.1</b></i></p>
        <p> <i><b>Examining the VAR model</b></i></p>
        <p>The vector autoregression (VAR) model captures the relationship between multiple series as
            they change over time. In this model, each series has an impact on the other, unlike the SARIMAX model
            where
            the exogenous variable had an impact on the target, but not the other way around. Recall in chapter 9
            that
            we used the variables realcons, realinv, realgovt, realdpi, cpi, m1, and tbilrate as predictors for
            realgdp,
            but we did not consider how realgdp can affect any of those variables. That is why we used the SARIMAX
            model
            in that case. </p>
        <p>You might have noticed the return of <i>autoregression</i>, which brings us
            back to the AR( <i>p</i>) model of chapter 5. This is a good intuition, as the VAR
            model
            can be seen as a generalization of the AR( <i>p</i>) model to allow for the forecast of
            multiple time series. </p>
        <p>Therefore, we can also denote the VAR model as VAR( <i>p</i>), where <i class="calibre3">p</i> is the order
            and has the same meaning as in the AR( <i>p</i>)
            model. </p>
        <p>Recall that AR( <i>p</i>) expressed the value of a time series as a linear
            combination of a constant C, the present error term ϵ <i>t</i>, which is also white
            noise,
            and the past values of the series <i>yt &ndash;p</i>. The magnitude of the influence of
            the
            past values on the present value is denoted as φ <i>p</i>, which represents the
            coefficients of the AR( <i>p</i>) model, as shown in equation 10.1. </p>
        <p> <i>yt</i> = C + φ1 <i>y<sub>t-1</sub></i> + φ2 <i>yt
            </i>&ndash;2 +⋅⋅⋅+ φ <i>pyt&ndash;p</i> + ϵ <i>t</i> Equation 10.1</p>
        <p>We can simply extend equation 10.1 to allow for multiple time series to be modeled, where
            each has an impact on the others. </p>
        <p>For simplicity, let's consider a system with two time series, denoted as <i>y</i> 1, <i
                class="calibre3">t</i> and <i>y</i> 2, <i>t</i>, and an order of
            1,
            meaning that <i>p</i> = 1. Then, using
            matrix notation, the VAR(1) model can be expressed as equation 10.2. </p>
        <p>C</p>
        <p>Equation 10.2</p>
        <p>C</p>
        <p>Carrying out the matrix multiplication, the mathematical expression for <i>y</i> 1, <i class="calibre3">t</i>
            is shown in equation 10.3, and that for <i>y</i> 2, <i class="calibre3">t</i> is shown in equation 10.4.
        </p>
        <p> <i> y</i> 1, <i>t</i> = C1 + φ1,1 <i>y</i> 1, <i class="calibre3">t </i>&ndash;1 + φ1,2 <i>y</i> 2, <i>t
            </i>&ndash;1 +
            ϵ1, <i>t</i> Equation 10.3</p>
        <p> <i> y</i> 2, <i>t</i> = C2 + φ2,1 <i>y</i> 1, <i class="calibre3">t </i>&ndash;1 + φ2,2 <i>y</i> 2, <i>t
            </i>&ndash;1 +
            ϵ2, <i>t</i> Equation 10.4</p>
        <p><a id="calibre_link-395"></a><b>200</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>In equation 10.3 you'll notice that the expression for <i>y</i> 1, <i>t</i>
            includes the past value of <i>y</i> 2, <i>t</i>. </p>
        <p>Similarly, in equation 10.4, the expression for <i>y</i> 2, <i>t</i>
            includes the past value of <i>y</i> 1, <i>t</i>. Hence, you can see
            how
            the VAR model captures the impact of each series on the
            other. </p>
        <p>We can extend equation 10.3 to express a general VAR( <i>p</i>) model that
            considers <i>p</i> lagged values, resulting in equation 10.5. Note that the superscript
            does not represent an exponent but is used for indexing. For simplicity, we'll again consider only two
            time
            series. </p>
        <p>Equation 10.5</p>
        <p>Just like with the AR( <i>p</i>) model, the VAR( <i>p</i>)
            model requires each time series to be stationary. </p>
        <p>Vector autoregression model</p>
        <p>The vector autoregression model VAR( <i>p</i>) models the relationship of
            two or more time series. In this model, each time series has an impact on the others. This means that
            past
            values of one time series affect the other time series, and vice versa. </p>
        <p>The VAR( <i>p</i>) model can be seen as a generalization of the AR( <i class="calibre3">p</i>) model that
            allows for multiple time series. Just like in the AR( <i class="calibre3">p</i>) model, the order <i>p</i>
            of the VAR( <i class="calibre3">p</i>) model determines how many lagged values impact the present value of a
            series. In
            this model, however, we also include lagged values of other time series. </p>
        <p>For two time series, the general equation for the VAR( <i>p</i>) model is a
            linear combination of a vector of constants, past values of both time series, and a vector of error
            terms:
            Note that the time series must be stationary to apply the VAR model. </p>
        <p>You have seen how the VAR(p) model is expressed mathematically, with their lagged values
            included in each expression, as shown in equations 10.3 and 10.4. This should give you a sense of how
            each
            series has an impact on the others. The VAR( <i>p</i>) model is</p>
        <p><a id="calibre_link-96"></a> <i><b>10.2</b></i></p>
        <p> <i><b>Designing a modeling procedure for the VAR(p)
                    model</b></i></p>
        <p><b>201</b></p>
        <p>only valid if both series are useful in predicting one another. Looking at the general shape
            of the series over time is not sufficient to support that hypothesis. Instead, we must apply the <i
                class="calibre3">Granger causality</i> test, which is a statistical hypothesis test to determine
            whether
            one time series is predictive of another. Only upon the success of this test can we apply the VAR model
            to
            make predictions. This is an important step in our modeling procedure when using a VAR model. </p>
        <p> <i><b>10.2</b></i></p>
        <p> <i><b>Designing a modeling procedure for the VAR(p)
                    model</b></i></p>
        <p>The VAR( <i>p</i>) model requires a slightly modified version of the
            modeling procedure we have been using. The most notable modification is the addition of the Granger
            causality test, since the VAR model assumes that past values of both time series are significantly
            predictive of the other time series. </p>
        <p>The complete modeling procedure for the VAR( <i>p</i>) model is shown in
            figure 10.2. </p>
        <p>As you can see, the modeling procedure for the VAR( <i>p</i>) model is very
            similar to the modeling procedures we have been using since the introduction of the ARMA( <i
                class="calibre3">p</i>, <i>q</i>) model. </p>
        <p>The main difference here is that we list values only for the order <i>p</i>, since we
            are
            fitting different VAR( <i>p</i>) models on the
            data. Then, once the model with the lowest AIC</p>
        <p>has been selected, we perform the Granger causality test. This test determines whether past
            values of a time series are statistically significant in forecasting another time series. It is
            important to
            test for this relationship because the VAR( <i>p</i>) model uses past values of one
            time
            series to forecast another. </p>
        <p>If the Granger causality test fails, we cannot say that past values of one time series are
            predictive of the other time series. In that case, the VAR( <i>p</i>) model becomes
            invalid, and we must revert to using a variation of the SARIMAX model to forecast the time series. On
            the
            other hand, if the Granger causality test passes, we can resume the procedure with residual analysis. As
            before, if the residuals are close to white noise, we can use the selected VAR( <i>p</i>)
            model to make forecasts. </p>
        <p>Before we move on to applying this modeling procedure, it is worth spending some time
            exploring the Granger causality test in more detail. </p>
        <p> <i><b>10.2.1 Exploring the Granger causality test</b></i>
        </p>
        <p>As shown in the previous section, the VAR( <i>p</i>) model assumes that
            each time series has an impact on another. Therefore, it is important to test if this relationship
            actually
            exists. Otherwise, we would be assuming a relationship that does not exist, which would introduce
            mistakes
            in the model and make our predictions invalid and unreliable. </p>
        <p>Hence, we use the Granger causality test. This is a statistical test that helps us determine
            if past values of a time series <i>y</i> 2, <i>t</i> can help forecast
            time series <i>y</i> 1, <i>t</i>. If that is the case, then we say
            that <i>y</i> 2, <i>t</i> <i>Granger-causes</i> <i class="calibre3">y</i> 1, <i>t</i>. </p>
        <p>Note that the Granger causality test is restricted to predictive causality, as we are only
            determining whether past values of a time series are statistically significant in predicting another
            time
            series. Furthermore, the test requires both time series to be</p>
        <p><a id="calibre_link-396"></a><b>202</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>List values</p>
        <p>for <i>p</i></p>
        <p>Fit every VAR( <i>p</i>) model</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Granger causality test</p>
        <p>VAR( <i>p</i>) model</p>
        <p>No</p>
        <p>passed? </p>
        <p>is invalid. </p>
        <p>Yes</p>
        <p>Residual analysis</p>
        <p>No</p>
        <p>No</p>
        <p>Figure 10.2</p>
        <p>Modeling procedure for the </p>
        <p>VAR( <i><b>p</b></i>) model. It is very similar to the
            modeling </p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>procedures we have been using since the </p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>introduction of the ARMA( <i><b>p</b></i>, <i><b>q</b></i>) model, but this
        </p>
        <p>time we are fitting different VAR( <i><b>p</b></i>) models
            and </p>
        <p>selecting the one with the lowest AIC. Then we </p>
        <p>run the Granger causality test. If it fails, the </p>
        <p>VAR( <i><b>p</b></i>) model is invalid, and we will not go
        </p>
        <p>Yes</p>
        <p>forward with the procedure. On the other hand, </p>
        <p>if the test passes, we perform residual analysis. </p>
        <p>Ready for</p>
        <p>If the residuals are similar to white noise, the </p>
        <p>forecasts</p>
        <p>VAR( <i><b>p</b></i>) model can be used for forecasting.
        </p>
        <p><a id="calibre_link-97"></a> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>203</b></p>
        <p>stationary in order for the results to be valid. Also, the Granger causality test tests
            causality only in one direction; we must repeat the test to verify that <i>y</i> 1, <i
                class="calibre3">t</i> also Grangercauses <i>y</i> 2, <i>t</i> in
            order for the VAR model to be valid. Otherwise, we must resort to the SARIMAX model and predict each
            time
            series separately. </p>
        <p>The null hypothesis for this test states that <i>y</i> 2, <i>t</i> does
            not
            Granger-cause <i>y</i> 1, <i>t</i>.
            Again, we will use the p-value with a critical value of 0.05 to determine whether we will reject the
            null
            hypothesis or not. In the case where the returned p-value of the Granger causality test is less than
            0.05,
            we can reject the null hypothesis and say that <i>y</i> 2, <i>t</i>
            Grangercauses <i>y</i> 1, <i>t</i>. </p>
        <p>You saw that the Granger causality test is performed after the VAR( <i>p</i>) model is
            selected. This is because the test requires us to specify the number of
            lags to include in the test, which is equivalent to the order of the model. For example, if the selected
            VAR( <i>p</i>) model is of order 3, the Granger causality test will determine if the
            past
            three values of a time series are statistically significant in forecasting the other time series. </p>
        <p>The statsmodels library conveniently includes the Granger causality test, which we will
            apply in the next section when we forecast both real consumption and real disposable income. </p>
        <p> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                </b></i></p>
        <p> <i><b>consumption</b></i></p>
        <p>Having examined the VAR( <i>p</i>) model and designed a modeling procedure
            for it, we are now ready to apply it to forecasting both the real disposable income and real consumption
            in
            the United States. We will use the same dataset as in the previous chapter, which contains the
            macroeconomics data between 1959 and 2009. </p>
        <p>NOTE</p>
        <p>The source code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH10">https://github</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH10">.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH10.
            </a></p>
        <p>macro_econ_data = sm.datasets.macrodata.load_pandas().data</p>
        <p>macro_econ_data</p>
        <p>We can now plot our two variables of interest, which are real disposable income, denoted as
            realdpi in the dataset, and real consumption, denoted as realcons. The result is shown in figure 10.3.
        </p>
        <p>fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(10,8))</p>
        <p>ax1.plot(macro_econ_data['realdpi'])</p>
        <p>ax1.set_xlabel('Date')</p>
        <p>ax1.set_ylabel('Real disposable income (k$)')</p>
        <p>ax1.set_title('realdpi')</p>
        <p>ax1.spines['top'].set_alpha(0)</p>
        <p>ax2.plot(macro_econ_data['realcons'])</p>
        <p>ax2.set_xlabel('Date')</p>
        <p>ax2.set_ylabel('Real consumption (k$)')</p>
        <p><a id="calibre_link-397"></a><img src="images/000127.jpg" alt="Image 92" class="calibre2" />
        </p>
        <p><b>204</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>ax2.set_title('realcons')</p>
        <p>ax2.spines['top'].set_alpha(0)</p>
        <p>plt.xticks(np.arange(0, 208, 16), np.arange(1959, 2010, 4))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 10.3</p>
        <p>Real disposable income and real consumption in the United States, between 1959 and 2009. The
            data was collected quarterly and is expressed in thousands of US dollars. You can see that both curves
            have
            a similar shape through time. </p>
        <p>In figure 10.3 you can see that both curves have a very similar shape through time, which
            intuitively makes them good candidates for a VAR( <i>p</i>) model. It is reasonable to
            think that with a higher disposable income, consumption is likely to be high, just as higher consumption
            can
            be a sign of higher disposable income. Of course, this hypothesis will have to be tested using the
            Granger
            causality test later in the modeling procedure. </p>
        <p><a id="calibre_link-203"></a> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>205</b></p>
        <p>We have gathered the data, so now we must determine if the time series are stationary. In
            figure 10.3 both of them exhibit a positive trend through time, meaning that they are non-stationary.
            Nevertheless, we'll apply the augmented Dickey-Fuller (ADF) test to make sure. </p>
        <p>ad_fuller_result_1 = adfuller(macro_econ_data['realdpi'])</p>
        <p>print('realdpi') </p>
        <p><b>ADF test </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result_1[0]}')</p>
        <p><b>for realdpi</b></p>
        <p>print(f'p-value: {ad_fuller_result_1[1]}')</p>
        <p>print('\n---------------------\n')</p>
        <p>ad_fuller_result_2 = adfuller(macro_econ_data['realcons'])</p>
        <p><b>ADF test for realcons. Note </b></p>
        <p>print('realcons') </p>
        <p><b>that both time series must </b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result_2[0]}')</p>
        <p><b>be stationary before they are </b></p>
        <p>print(f'p-value: {ad_fuller_result_2[1]}')</p>
        <p><b>used in the VAR(p) model. </b></p>
        <p>For both variables, the ADF test outputs a p-value of 1.0. Therefore, we cannot reject the
            null hypothesis, and we conclude that both time series are not stationary, as expected. </p>
        <p>We'll apply a transformation to make them stationary. Specifically, we'll difference both
            series and test for stationarity again. </p>
        <p>ad_fuller_result_1 = adfuller(macro_econ_data['realdpi'].diff()[1:]) </p>
        <p><b>First-order</b></p>
        <p>print('realdpi')</p>
        <p><b>differencing</b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result_1[0]}')</p>
        <p><b>for realdpi</b></p>
        <p>print(f'p-value: {ad_fuller_result_1[1]}')</p>
        <p>print('\n---------------------\n')</p>
        <p>ad_fuller_result_2 = adfuller(macro_econ_data['realcons'].diff()[1:]) </p>
        <p><b>First-order</b></p>
        <p>print('realcons')</p>
        <p><b>differencing</b></p>
        <p>print(f'ADF Statistic: {ad_fuller_result_2[0]}')</p>
        <p><b>for realcons</b></p>
        <p>print(f'p-value: {ad_fuller_result_2[1]}')</p>
        <p>The ADF test for realdpi returns a p-value of 1.45 × 10&ndash;14, while the ADF test for
            realcons returns a p-value of 0.0006. In both cases, the p-value is smaller than 0.05. </p>
        <p>Therefore, we reject the null hypothesis and conclude that both time series are stationary.
            As mentioned before, the VAR( <i>p</i>) model requires the time series to be
            stationary.
        </p>
        <p>We can thus use the transformed series for modeling, and we will need to integrate the
            forecasts to bring them back to their original scales. </p>
        <p>We are now at the step of fitting many VAR( <i>p</i>) models to select the
            one with the smallest Akaike information criterion (AIC). We'll write a function, optimize_VAR, to fit
            many
            VAR( <i>p</i>) models while varying the order <i>p</i>. This function
            will
            return an ordered DataFrame in ascending order of AIC. This function is shown in the following listing.
        </p>
        <p><a id="calibre_link-398"></a><b>206</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>Listing 10.1</p>
        <p>Function to fit many VAR( <i><b>p</b></i>) models and
            select the one with the lowest AIC</p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.varmax import VARMAX</p>
        <p>def optimize_VAR(endog: Union[pd.Series, list]) -&gt; pd.DataFrame:</p>
        <p></p>
        <p>results = []</p>
        <p><b>Vary the order </b></p>
        <p></p>
        <p><b>p from 0 to 14. </b></p>
        <p>for i in tqdm_notebook(range(15)): </p>
        <p>try:</p>
        <p>model = VARMAX(endog, order=(i, 0)).fit(dips=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p>aic = model.aic</p>
        <p>results.append([i, aic])</p>
        <p></p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['p', 'AIC']</p>
        <p></p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df</p>
        <p>We can now use this function to select the order <i>p</i> that minimizes
            the AIC. </p>
        <p>First, though, we must define the train and test sets. In this case, we'll use 80% of the
            data for training and 20% for testing. This means that the last 40 data points will be used for testing,
            and
            the rest is used for training. Remember that the VAR( <i>p</i>) model requires both
            series
            to be stationary. Therefore, we'll split on the differenced dataset and feed the differenced training
            set to
            the optimize_VAR function. </p>
        <p><b>Select only realdpi and realcons, as they are </b></p>
        <p><b>Difference both series, </b></p>
        <p><b>the only two variables of interest in this case. </b></p>
        <p><b>as the ADF test shows that</b></p>
        <p><b>a first-order differencing</b></p>
        <p>endog = macro_econ_data[['realdpi', 'realcons']] </p>
        <p><b>makes them stationary. </b></p>
        <p>endog_diff = macro_econ_data[['realdpi', 'realcons']].diff()[1:] </p>
        <p><b>The first 162 data points go </b></p>
        <p>train = endog_diff[:162] </p>
        <p><b>for training. This is roughly </b></p>
        <p>test = endog_diff[162:] </p>
        <p><b>80% of the dataset. </b></p>
        <p>result_df = optimize_VAR(train) </p>
        <p>result_df</p>
        <p><b>Run the optimize_VAR function using </b></p>
        <p><b>the differenced data stored in train. </b></p>
        <p><b>The last 40 data points go for the test </b></p>
        <p><b>This is required for the VAR(p) model. </b></p>
        <p><b>set. This is roughly 20% of the dataset. </b></p>
        <p>Running the function returns a DataFrame in which we see that <i>p</i> = 3
            has the lowest AIC value of all. Therefore, the selected model is a VAR(3) model, meaning that the past
            three values of each time series are used to forecast the other time series. </p>
        <p><a id="calibre_link-245"></a> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>207</b></p>
        <p>Following the modeling procedure, we must now use the Granger causality test Recall that the
            VAR model assumes that past values of realcons are useful in predicting realdpi and that past values of
            realdpi are useful in predicting realcons. This relationship must be tested. If the Granger causality
            test
            returns a p-value greater than 0.05, we cannot reject the null hypothesis, meaning that the variables do
            not
            Granger-cause each other, and the model is invalid. On the other hand, a p-value smaller than 0.05 will
            allow us to reject the null hypothesis, thus validating the VAR(3) model, meaning that we can move on
            with
            the modeling procedure. </p>
        <p>We'll run the Granger causality test for both variables, using the grangercausalitytests
            function from the statsmodels library. Remember that the series must be stationary for the Granger
            causality
            test, which is why they are differenced when passed in to the function. Also, we specify the number of
            lags
            for the test, which in this case is 3, since the model selection step returned <i>p</i>
            =
            3. </p>
        <p><b>The function tests if the second variable Granger-causes the first one.
                Here we thus test</b> <b>if realcons Granger-causes realdpi. We then pass the
                number of
                lags in a list, which in</b> <b>our case is 3. Note that the series are differenced
                to
                make them stationary. </b></p>
        <p>print('realcons Granger-causes realdpi?\n')</p>
        <p>print('------------------')</p>
        <p>granger_1 = grangercausalitytests(macro_econ_data[['realdpi', </p>
        <p>➥ 'realcons']].diff()[1:], [3]) </p>
        <p>print('\nrealdpi Granger-causes realcons?\n')</p>
        <p>print('------------------')</p>
        <p>granger_2 = grangercausalitytests(macro_econ_data[['realcons', </p>
        <p>➥ 'realdpi']].diff()[1:], [3]) </p>
        <p><b>Here we test if realdpi </b></p>
        <p><b>Granger-causes realcons. </b></p>
        <p>Running the Granger causality test for both variables returns a p-value smaller than 0.05 in
            both cases. Therefore, we can reject the null hypothesis and conclude that realdpi Granger-causes
            realcons,
            and realcons Granger-causes realdpi. Our VAR(3) model is thus valid. In the event that one variable does
            not
            Granger-cause the other, the VAR( <i>p</i>) model becomes invalid, and it cannot be
            used.
            In that case, we must use the SARIMAX model and predict each time series individually. </p>
        <p>We can now move on to residual analysis. For this step, we first fit the VAR(3) model on our
            train set. </p>
        <p>best_model = VARMAX(train, order=(3,0))</p>
        <p>best_model_fit = best_model.fit(disp=False)</p>
        <p>Then we can use the plot_diagnostics function to plot a histogram of the residuals, the Q-Q
            plot, and the correlogram. However, we must study the residuals of two variables here, since we are
            modeling
            both realdpi and realcons. </p>
        <p>Let's focus on the residuals for realdpi first. </p>
        <p><b>Passing variable=0 specifies that we want plots for the residuals of</b>
        </p>
        <p><b>realdpi, since it is the first variable that was passed to the VAR
                model. </b></p>
        <p>best_model_fit.plot_diagnostics(figsize=(10,8), variable=0); </p>
        <p><a id="calibre_link-255"></a><img src="images/000007.jpg" alt="Image 93" class="calibre2" />
        </p>
        <p><b>208</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>The output in figure 10.4 shows that the residuals are close to white noise. </p>
        <p>Figure 10.4</p>
        <p>Residuals analysis of <b>realdpi</b>. The standardized residuals seem to
            have no trend and constant variance, which is in line with white noise. The histogram also closely
            resembles
            the shape of a normal distribution. This is further supported by the Q-Q plot, which shows a fairly
            straight
            line that lies on <i><b>y</b></i> = <i><b>x</b></i>, although we can see
            some curvature at the extremities. Finally, the
            correlogram shows no significant coefficients except at lag 5. However, this is likely due to chance,
            since
            there are no preceding significant coefficients. Thus, we can conclude that the residuals are close to
            white
            noise. </p>
        <p>Now we can move on to analyzing the residuals of realcons. </p>
        <p><b>Passing variable=1 specifies that we want the plots of the residuals</b>
        </p>
        <p><b>for realcons, since it was the second variable passed in the model. </b>
        </p>
        <p>best_model_fit.plot_diagnostics(figsize=(10,8), variable=1); </p>
        <p>The output in figure 10.5 shows that the residuals of realcons closely resemble white noise.
        </p>
        <p>Once the qualitative analysis is done, we can move on to the quantitative analysis using the
            Ljung-Box test. Recall that the null hypothesis of the Ljung-Box test states</p>
        <p><a id="calibre_link-399"></a><img src="images/000058.jpg" alt="Image 94" class="calibre2" />
        </p>
        <p> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>209</b></p>
        <p>Figure 10.5</p>
        <p>Residuals analysis of <b>realcons</b>. The top-left plot shows the
            residuals over time, and you can see that there is no trend and constant variance, which is in line with
            the
            behavior of white noise. </p>
        <p>At the top right, the distribution is very close to a normal distribution. This is further
            supported by the Q-Q plot at the bottom left, which displays a fairly straight line that lies on <i
                class="calibre3"><b>y</b></i> = <i><b>x</b></i>.
            Finally, the correlogram at the bottom right shows that there are no significant autocorrelation
            coefficients after lag 0. </p>
        <p>Therefore, the residuals are close to white noise. </p>
        <p>that the residuals are independent and uncorrelated. Therefore, for the residuals to behave
            like white noise, the test must return p-values that are larger than 0.05, in which case we do not
            reject
            the null hypothesis. </p>
        <p>The test must be applied on both realdpi and realcons:</p>
        <p>realgdp_residuals = best_model_fit.resid['realdpi']</p>
        <p>lbvalue, pvalue = acorr_ljungbox(realgdp_residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p>Running the Ljung-Box test on the residuals of realdpi returns p-values that are all larger
            than 0.05. Thus, we do not reject the null hypothesis, meaning that the residuals are uncorrelated and
            independent, just like white noise. </p>
        <p><a id="calibre_link-290"></a><b>210</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>realcons_residuals = best_model_fit.resid['realcons']</p>
        <p>lbvalue, pvalue = acorr_ljungbox(realcons_residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p>Next, we'll run the test on the residuals of realcons. This test returns p-values that are
            all greater than 0.05. Again, we do not reject the null hypothesis, meaning that the residuals are not
            correlated and independent, just like white noise. </p>
        <p>Since the model passed both the qualitative and quantitative aspects of residual analysis,
            we can move on to forecasting realcons and realdpi using a VAR(3) model. </p>
        <p>We will compare the VAR(3) model to a baseline that simply predicts the last observed value.
            We'll forecast four steps into the future, which is equivalent to forecasting one full year as the data
            is
            sampled quarterly. We'll thus perform a rolling forecast four steps into the future over the entire
            length
            of the test set. </p>
        <p>To do so, we'll use the rolling_forecast function that we have defined many times over the
            last several chapters. This time, we'll apply some slight modifications to accommodate the VAR(3) model.
            It
            will need to output predictions for both realdpi and realcons, so we must return two lists containing
            forecasts. The following listing shows the code for the rolling_forecast function. </p>
        <p>Listing 10.2</p>
        <p>Function for rolling forecasts over a test set</p>
        <p>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, </p>
        <p>➥ window: int, method: str) -&gt; list:</p>
        <p></p>
        <p>total_len = train_len + horizon</p>
        <p>end_idx = train_len</p>
        <p></p>
        <p>if method == 'VAR':</p>
        <p><b>Initialize two empty lists </b></p>
        <p><b>to hold the predictions for </b></p>
        <p><b>realdpi and realcons. </b></p>
        <p>realdpi_pred_VAR = [] </p>
        <p>realcons_pred_VAR = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>model = VARMAX(df[:i], order=(3,0))</p>
        <p>res = model.fit(disp=False)</p>
        <p><b>Extract the </b></p>
        <p>predictions = res.get_prediction(0, i + window - 1)</p>
        <p><b>predictions </b></p>
        <p><b>for </b> </p>
        <p><b>realdpi. </b></p>
        <p></p>
        <p>oos_pred_realdpi = predictions.predicted_mean.iloc[-</p>
        <p>➥ window:]['realdpi'] </p>
        <p>oos_pred_realcons = predictions.predicted_mean.iloc[-</p>
        <p>➥ window:]['realcons'] </p>
        <p></p>
        <p><b>Extend the lists with </b></p>
        <p><b>the new predictions </b></p>
        <p></p>
        <p><b>Extract the </b></p>
        <p>realdpi_pred_VAR.extend(oos_pred_realdpi) </p>
        <p><b>for each variable. </b></p>
        <p></p>
        <p><b>predictions </b></p>
        <p>realcons_pred_VAR.extend(oos_pred_realcons)</p>
        <p><b>for </b> </p>
        <p><b>realcons. </b></p>
        <p>return realdpi_pred_VAR, realcons_pred_VAR </p>
        <p></p>
        <p><b>Return both lists of predictions</b></p>
        <p><b>for realdpi and realcons. </b></p>
        <p><a id="calibre_link-400"></a> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>211</b></p>
        <p>elif method == 'last': </p>
        <p><b>For the baseline, we'll </b></p>
        <p>realdpi_pred_last = []</p>
        <p><b>also use two lists to hold </b></p>
        <p>realcons_pred_last = []</p>
        <p><b>the predictions for each </b></p>
        <p></p>
        <p><b>variable and return them </b></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p><b>at the end. </b></p>
        <p></p>
        <p>realdpi_last = df[:i].iloc[-1]['realdpi']</p>
        <p>realcons_last = df[:i].iloc[-1]['realcons']</p>
        <p></p>
        <p>realdpi_pred_last.extend(realdpi_last for _ in range(window))</p>
        <p>realcons_pred_last.extend(realcons_last for _ in range(window))</p>
        <p></p>
        <p>return realdpi_pred_last, realcons_pred_last</p>
        <p>We can now use this function to produce the forecasts for realdpi and realcons using the
            VAR(3) model. </p>
        <p>TRAIN_LEN = len(train)</p>
        <p><b>The window is 4, since we want to forecast </b></p>
        <p><b>four time steps into the future at a time, </b></p>
        <p>HORIZON = len(test)</p>
        <p><b>which is equivalent to 1 year. </b></p>
        <p>WINDOW = 4 </p>
        <p>realdpi_pred_VAR, realcons_pred_VAR = rolling_forecast(endog_diff, </p>
        <p>➥ TRAIN_LEN, HORIZON, WINDOW, 'VAR')</p>
        <p>Recall that the VAR(3) model requires the series to be stationary, meaning that we have
            transformed forecasts. We must then integrate them using the cumulative sum to bring them back to the
            original scale of the data. </p>
        <p>test = endog[163:]</p>
        <p>test['realdpi_pred_VAR'] = pd.Series()</p>
        <p><b>Integrate the </b></p>
        <p><b>forecasts using the </b></p>
        <p>test['realdpi_pred_VAR'] = endog.iloc[162]['realdpi'] + </p>
        <p>➥</p>
        <p><b>cumulative sum. </b></p>
        <p>np.cumsum(realdpi_pred_VAR) </p>
        <p>test['realcons_pred_VAR'] = pd.Series()</p>
        <p>test['realcons_pred_VAR'] = endog.iloc[162]['realcons'] + </p>
        <p>➥ np.cumsum(realcons_pred_VAR)</p>
        <p><b>Display the test </b></p>
        <p>test </p>
        <p><b>DataFrame. </b></p>
        <p>At this point, test contains the actual values of the test set and the predictions from the
            VAR(3) model. We can now add the forecasts from our baseline method, which</p>
        <p>simply predicts the last known value for the next four timesteps. </p>
        <p>realdpi_pred_last, realcons_pred_last = rolling_forecast(endog, </p>
        <p>➥ TRAIN_LEN, HORIZON, WINDOW, 'last') </p>
        <p><b>Use rolling_forecast </b></p>
        <p><b>to obtain the baseline </b></p>
        <p>test['realdpi_pred_last'] = realdpi_pred_last</p>
        <p><b>predictions using the </b></p>
        <p>test['realcons_pred_last'] = realcons_pred_last</p>
        <p><b>last known value </b></p>
        <p><b>method. </b></p>
        <p><b>Display the test </b></p>
        <p>test </p>
        <p><b>DataFrame. </b></p>
        <p><a id="calibre_link-264"></a><img src="images/000154.jpg" alt="Image 95" class="calibre2" />
        </p>
        <p><b>212</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p>Now test holds the actual values of the test set, the predictions from the VAR(3) model, and
            the predictions from the baseline method. Everything is set for us to visualize the forecasts and
            evaluate
            the forecasting methods using the mean absolute percentage error (MAPE). The forecasts are shown in
            figure
            10.6. </p>
        <p>Figure 10.6</p>
        <p>Forecasts of <b>realdpi</b> and <b>realcons</b>. You can
            see that the predictions from the VAR(3) model, shown as a dashed line, closely follow the actual values
            of
            the test set. You'll also notice that the dotted curve from the baseline method shows little steps,
            which
            makes sense since we are forecasting a constant value over four timesteps. </p>
        <p>In figure 10.6 the dashed line represents the forecasts from the VAR(3) model, and the
            dotted line shows the predictions from the last known value method. You can see that both lines are very
            close to the actual values of the test set, making it hard for us to visually determine which method is
            better. </p>
        <p>We will now calculate the MAPE. The result is shown in figure 10.7. </p>
        <p><a id="calibre_link-227"></a><img src="images/000122.jpg" alt="Image 96" class="calibre2" />
        </p>
        <p> <i><b>10.3</b></i></p>
        <p> <i><b>Forecasting real disposable income and real
                    consumption</b></i></p>
        <p><b>213</b></p>
        <p>def mape(y_true, y_pred):</p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p>mape_realdpi_VAR = mape(test['realdpi'], test['realdpi_pred_VAR'])</p>
        <p>mape_realdpi_last = mape(test['realdpi'], test['realdpi_pred_last'])</p>
        <p>mape_realcons_VAR = mape(test['realcons'], test['realcons_pred_VAR'])</p>
        <p>mape_realcons_last = mape(test['realcons'], test['realcons_pred_last'])</p>
        <p>Figure 10.7</p>
        <p>The MAPE of the forecast for <b>realdpi</b> and <b>realcons</b>. You
            can
            see that the VAR(3) model performs worse than the baseline in the
            case of <b>realdpi</b>. However, the VAR(3) model performs better than the baseline for
            <b>realcons</b>. </p>
        <p>In figure 10.7 you can see that the VAR(3) model performs worse than the baseline in the
            case of realdpi but better than the baseline for realcons. This is an ambiguous situation. There is no
            clear
            result, since the model does not outperform the baseline in both situations. </p>
        <p>We can hypothesize that in the case of realdpi, realcons is not predictive enough to make
            more accurate forecasts than the baseline, even though the Granger causality test passed. Therefore, we
            should resort to using a variation of the SARIMAX model to predict realdpi. Thus, I would conclude that
            the
            VAR(3) model is not sufficient to accurately forecast realdpi and realcons. I would suggest using two
            separate models, which could include realdpi and realcons as exogenous variables, while also potentially
            including moving average terms. </p>
        <p><a id="calibre_link-98"></a><b>214</b></p>
        <p>CHAPTER 10</p>
        <p> <i><b>Forecasting multiple time series</b></i></p>
        <p> <i><b>10.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the VAR( <i>p</i>) model, which allows us to
            forecast multiple time series at once. </p>
        <p>The VAR( <i>p</i>) model stands for vector autoregression, and it assumes
            that the past values of some time series are predictive of the future values of other time series. This
            bidirectional relationship is tested using the Granger causality test. If the test fails, meaning that
            the
            returned p-values are larger than 0.05, the VAR( <i>p</i>) model is invalid, and it
            cannot
            be used. </p>
        <p>Congratulations on making it this far&mdash;we have covered a wide array of statistical
            methods for forecasting time series! These statistical methods are great for smaller datasets with low
            dimensionality. However, when datasets start getting large, starting at 10,000 data points or more, and
            they
            have many features, deep learning can be a great tool for obtaining accurate forecasts and leveraging
            all
            the available data. </p>
        <p>In the next chapter, we'll go through a capstone project to consolidate our knowledge of
            statistical methods. Then we'll start a new section and apply deep learning forecasting models on large
            datasets. </p>
        <p> <i><b>10.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Go above and beyond the VAR( <i>p</i>) model with these exercises. The full
            solutions are available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH10">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH10">tree/master/CH10.
            </a></p>
        <p> <i><b>10.5.1 Use a VARMA model to predict realdpi and
                    realcons</b></i></p>
        <p>In this chapter, we used a VAR( <i>p</i>) model. However, we used the
            VARMAX function from statsmodels to do so, meaning that we can easily extend the VAR( <i
                class="calibre3">p</i>) model to a VARMA( <i>p</i>, <i>q</i>)
            model.
            In this exercise, use a VARMA( <i>p</i>, <i>q</i>) model to forecast
            realdpi and realcons. </p>
        <p>1</p>
        <p>Use the same train and test sets as in this chapter. </p>
        <p>2</p>
        <p>Generate a list of unique ( <i>p</i>, <i>q</i>)
            combinations. </p>
        <p>3</p>
        <p>Rename the optimize_VAR function to optimize_VARMA, and adapt it to loop over all unique (
            <i>p</i>, <i>q</i>) combinations. </p>
        <p>4</p>
        <p>Select the model with the lowest AIC, and perform the Granger causality test. </p>
        <p>Pass in the largest order among ( <i>p</i>, <i>q</i>). Is
            the VARMA( <i>p</i>, <i>q</i>) model valid? </p>
        <p>5</p>
        <p>Perform residual analysis. </p>
        <p>6</p>
        <p>Make forecasts on a four-step window over the test set. Use the last known value method as a
            baseline. </p>
        <p>7</p>
        <p>Calculate the MAPE. Is it lower or higher than that of our VAR(3) model? </p>
        <p><a id="calibre_link-99"></a> <i><b>Summary</b></i></p>
        <p><b>215</b></p>
        <p> <i><b>10.5.2 Use a VARMAX model to predict realdpi and
                    realcons</b></i></p>
        <p>Again, since we used the VARMAX function from statsmodels, we know that we can also add
            exogenous variables to the model, just like in SARIMAX. In this exercise, use the VARMAX model to
            forecast
            realdpi and realcons. </p>
        <p>1</p>
        <p>Use the same train and test sets as in this chapter. </p>
        <p>2</p>
        <p>Generate a list of unique ( <i>p</i>, <i>q</i>)
            combinations. </p>
        <p>3</p>
        <p>Rename the optimize_VAR function to optimize_VARMAX, and adapt it to loop</p>
        <p>over all the unique ( <i>p</i>, <i>q</i>) combinations and
            exogenous variables. </p>
        <p>4</p>
        <p>Select the model with the lowest AIC, and perform the Granger causality test. </p>
        <p>Pass in the largest order among ( <i>p</i>, <i>q</i>). Is
            the VARMAX( <i>p</i>, <i>q</i>) model valid? </p>
        <p>5</p>
        <p>Perform residual analysis. </p>
        <p>6</p>
        <p>Make forecasts on a one-step window over the test set. Use the last known value</p>
        <p>method as a baseline. </p>
        <p>7</p>
        <p>Calculate the MAPE. Did the model perform better than the baseline? </p>
        <p> <i><b>Summary</b></i></p>
        <p> The vector autoregression model, VAR( <i>p</i>), captures the
            relationship between multiple series as they change over time. In this model, each series has an impact
            on
            the others. </p>
        <p> A VAR( <i>p</i>) model is valid only if each time series Granger-causes
            the others. This is determined using the Granger causality test. </p>
        <p> The null hypothesis of the Granger causality test states that one time series does not
            Granger-cause the other. If the p-value is less than 0.05, we reject the null hypothesis and conclude
            that
            the first time series Granger-causes the other. </p>
        <p><a id="calibre_link-13"></a> <i>Capstone: Forecasting the</i></p>
        <p> <i>number of antidiabetic drug</i></p>
        <p> <i>prescriptions in Australia</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Developing a forecasting model to predict the </p>
        <p>number of antidiabetic drug prescriptions in </p>
        <p>Australia</p>
        <p> Applying the modeling procedure with a SARIMA </p>
        <p>model</p>
        <p> Evaluating our model against a baseline</p>
        <p> Determining the champion model</p>
        <p>We have covered a lot of statistical models for time series forecasting. Back in chapters 4
            and 5, you learned how to model moving average processes and autoregres-</p>
        <p>sive processes. We then combined these models to form the ARMA model and added a parameter
            to forecast non-stationary time series, leading us to the ARIMA model. We then added a seasonal
            component
            with the SARIMA model. Adding the</p>
        <p>effect of exogenous variables culminated in the SARIMAX model. Finally, we cov-</p>
        <p>ered multivariate time series forecasting using the VAR model. Thus, you now have access to
            many statistical models that allow you to forecast a wide variety of time series, from simple to more
            complex. This is a good time to consolidate your learning and put your knowledge into practice with a
            capstone project. </p>
        <p><b>216</b></p>
        <p><a id="calibre_link-401"></a><img src="images/000031.jpg" alt="Image 97" class="calibre2" />
        </p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p><b>217</b></p>
        <p>The objective of the project in this chapter is forecasting the number of antidiabetic drug
            prescriptions in Australia, from 1991 to 2008. In a professional setting, solving this problem would
            allow
            us to gauge the production of antidiabetic drugs, such as to produce enough to meet the demand and but
            also
            avoid overproduction. The data we'll use was recorded by the Australian Health Insurance Commission. We
            can
            visualize the time series in figure 11.1. </p>
        <p>Figure 11.1</p>
        <p>Monthly number of antidiabetic drug prescriptions in Australia between </p>
        <p>1991 and 2008. </p>
        <p>In figure 11.1 you'll see a clear trend in the time series, as the number of prescriptions
            increases over time. Furthermore, you'll observe strong seasonality, as each year seems to start at a
            low
            value and end at a high value. By now, you should intuitively know which model is potentially the most
            suitable for solving this problem. </p>
        <p>To solve this problem, refer to the following steps:</p>
        <p>1</p>
        <p>The objective is to forecast 12 months of antidiabetic drug prescriptions. Use the last 36
            months of the dataset as a test set to allow for rolling forecasts. </p>
        <p>2</p>
        <p>Visualize the time series. </p>
        <p>3</p>
        <p>Use time series decomposition to extract the trend and seasonal components. </p>
        <p>4</p>
        <p>Based on your exploration, determine the most suitable model. </p>
        <p>5</p>
        <p>Model the series with the usual steps:</p>
        <p>a</p>
        <p>Apply transformations to make it stationary</p>
        <p>b</p>
        <p>Set the values of <i>d</i> and <i>D</i>. Set the value of
            <i>m</i>. </p>
        <p><a id="calibre_link-100"></a><b>218</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>c</p>
        <p>Find the optimal ( <i>p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>
            parameters. </p>
        <p>d</p>
        <p>Perform residual analysis to validate your model. </p>
        <p>6</p>
        <p>Perform rolling forecasts of 12 months on the test set. </p>
        <p>7</p>
        <p>Visualize your forecasts. </p>
        <p>8</p>
        <p>Compare the model's performance to a baseline. Select an appropriate baseline and error
            metric. </p>
        <p>9</p>
        <p>Conclude whether the model should be used or not. </p>
        <p>To get the most out of this capstone project, you are highly encouraged to complete it on
            your own by referring to the preceding steps. This will help you assess your auton-omy in the modeling
            process and your understanding. </p>
        <p>If you ever feel stuck or want to validate your reasoning, the rest of this chapter walks
            through the completion of this project. Also, the full solution is available on GitHub if you wish to
            refer
            to the code directly: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH11">https://github.com/marcopeix/</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH11">TimeSeriesForecastingInPython/tree/master/CH11</a>.
        </p>
        <p>I wish you luck on this project! </p>
        <p> <i><b>11.1</b></i></p>
        <p> <i><b>Importing the required libraries and loading the
                    data</b></i></p>
        <p>The natural first step is to import the libraries that will be needed to complete the
            project. We can then load the data and store it in a DataFrame to be used throughout the project. </p>
        <p>Thus, we'll import the following libraries and specify the magic function %matplotlib inline
            to display the plots in the notebook:</p>
        <p>from sklearn.metrics import mean_squared_error, mean_absolute_error</p>
        <p>from statsmodels.graphics.tsaplots import plot_acf, plot_pacf</p>
        <p>from statsmodels.tsa.seasonal import seasonal_decompose, STL</p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>from statsmodels.tsa.arima_process import ArmaProcess</p>
        <p>from statsmodels.graphics.gofplots import qqplot</p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from itertools import product</p>
        <p>from typing import Union</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>import statsmodels.api as sm</p>
        <p>import pandas as pd</p>
        <p>import numpy as np</p>
        <p>import warnings</p>
        <p>warnings.filterwarnings('ignore')</p>
        <p>%matplotlib inline</p>
        <p>Once the libraries are imported, we can read the data and store it in a DataFrame. We can
            also display the shape of the DataFrame to determine the number of data points. </p>
        <p><a id="calibre_link-101"></a> <i><b>11.2</b></i></p>
        <p> <i><b>Visualizing the series and its components</b></i>
        </p>
        <p><b>219</b></p>
        <p>df = pd.read_csv('data/AusAnti-diabeticDrug.csv')</p>
        <p>print(df.shape) </p>
        <p><b>Displays the shape of a DataFrame. The first </b></p>
        <p><b>value is the number of rows, and the second </b></p>
        <p><b>value is the number of columns. </b></p>
        <p>The data is now ready to be used throughout the project. </p>
        <p> <i><b>11.2</b></i></p>
        <p> <i><b>Visualizing the series and its components</b></i>
        </p>
        <p>With the data loaded, we can now easily visualize the series. This essentially recreates
            figure 11.1. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df.y)</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Number of anti-diabetic drug prescriptions')</p>
        <p>plt.xticks(np.arange(6, 203, 12), np.arange(1992, 2009, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Next we can perform decomposition to visualize the different components of the time series.
            Remember that time series decomposition allows us to visualize the trend component, seasonal component,
            and
            the residuals. </p>
        <p>decomposition = STL(df.y, period=12).fit() </p>
        <p>fig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=4, ncols=1, sharex=True, </p>
        <p>➥ figsize=(10,8))</p>
        <p><b>Column y holds the number of monthly</b></p>
        <p>ax1.plot(decomposition.observed)</p>
        <p><b>antidiabetic prescriptions. Also, the period</b></p>
        <p><b>is set to 12, since we have monthly data. </b></p>
        <p>ax1.set_ylabel('Observed')</p>
        <p>ax2.plot(decomposition.trend)</p>
        <p>ax2.set_ylabel('Trend')</p>
        <p>ax3.plot(decomposition.seasonal)</p>
        <p>ax3.set_ylabel('Seasonal')</p>
        <p>ax4.plot(decomposition.resid)</p>
        <p>ax4.set_ylabel('Residuals')</p>
        <p>plt.xticks(np.arange(6, 203, 12), np.arange(1992, 2009, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>The result is shown in figure 11.2. Everything seems to suggest that a SARIMA( <i>p</i>,
            <i>d</i>, <i>q</i>) ( <i>P</i>, <i class="calibre3">D</i>, <i>Q</i>) <i>m</i>
            model would be the optimal solution for forecasting this time series. We have a</p>
        <p><a id="calibre_link-102"></a><img src="images/000030.jpg" alt="Image 98" class="calibre2" />
        </p>
        <p><b>220</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>Figure 11.2</p>
        <p>Time series decomposition on the antidiabetic drug prescriptions dataset. The first plot
            shows the observed data. The second plot shows the trend component, which tells us that the number of
            antidiabetic drug prescriptions is increasing over time. The third plot shows the seasonal component,
            where
            we can see a repeating pattern over time, indicating the presence of seasonality. The last plot shows
            the
            residuals, which are variations that are not explained by the trend of the seasonal component. </p>
        <p>trend as well as clear seasonality. Plus, we do not have any exogenous variables to work
            with, so the SARIMAX model cannot be applied. Finally, we wish to predict only one target, meaning that
            a
            VAR model is also not relevant in this case. </p>
        <p> <i><b>11.3</b></i></p>
        <p> <i><b>Modeling the data</b></i></p>
        <p>We've decided that a SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>, <i
                class="calibre3">Q</i>) <i>m</i> model is the most suitable for modeling and
            forecasting this time series. Therefore, we'll follow the general modeling procedure for a SARIMAX
            model, as
            a SARIMA model is a special case of the SARIMAX</p>
        <p>model. The modeling procedure is shown in figure 11.3. </p>
        <p></p>
        <p><a id="calibre_link-402"></a> <i><b>11.3</b></i></p>
        <p> <i><b>Modeling the data</b></i></p>
        <p><b>221</b></p>
        <p>Gather data</p>
        <p>Apply</p>
        <p>Is it stationary? </p>
        <p>No</p>
        <p>transformations</p>
        <p>Yes</p>
        <p>Set <i>d </i> equal to the</p>
        <p>number of times the</p>
        <p>series was differenced</p>
        <p>Set <i>D </i> equal to the</p>
        <p>number of times</p>
        <p>seasonal differencing</p>
        <p>was applied</p>
        <p>List values</p>
        <p>of <i>p, q, P, Q</i></p>
        <p>Fit every combination of</p>
        <p>SARIMAX( , , </p>
        <p> <i>p d q</i>)( , , )</p>
        <p> <i>P D Q m</i></p>
        <p>No</p>
        <p>No</p>
        <p>Select model with</p>
        <p>lowest AIC</p>
        <p>Residual analysis</p>
        <p>Q-Q plot shows a</p>
        <p>Uncorrelated</p>
        <p>straight line? </p>
        <p>residuals? </p>
        <p>Figure 11.3</p>
        <p>The SARIMA modeling </p>
        <p>Yes</p>
        <p>procedure. This procedure is the most general </p>
        <p>modeling procedure, and it can be used for a </p>
        <p>SARIMA, ARIMA, or ARMA model, as they are </p>
        <p>Ready for</p>
        <p>forecasts</p>
        <p>simply special cases of the SARIMAX model. </p>
        <p><a id="calibre_link-103"></a><b>222</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>Following the modeling procedure outlined in figure 11.3, we'll first determine whether the
            series is stationary using the augmented Dickey-Fuller (ADF) test. </p>
        <p>ad_fuller_result = adfuller(df.y)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This returns a p-value of 1.0, meaning that we cannot reject the null hypothesis, and we
            conclude that the series is not stationary. Thus, we must apply transformations to make it stationary.
        </p>
        <p>We'll first apply a first-order differencing on the data and test for stationarity again.
        </p>
        <p>y_diff = np.diff(df.y, n=1)</p>
        <p>ad_fuller_result = adfuller(y_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>This returns a p-value of 0.12. Again, the p-value is greater than 0.05, meaning that the
            series is not stationary. Let's try applying a seasonal difference, since we noticed a strong seasonal
            pattern in the data. Recall that we have monthly data, meaning that <i>m</i> = 12.
            Thus, a
            seasonal difference subtracts values that are 12 timesteps apart. </p>
        <p>y_diff_seasonal_diff = np.diff(y_diff, n=12) </p>
        <p><b>We have monthly </b></p>
        <p><b>data, so n = 12. </b></p>
        <p>ad_fuller_result = adfuller(y_diff_seasonal_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>The returned p-value is 0.0. Thus, we can reject the null hypothesis and conclude that our
            time series is stationary. </p>
        <p>Since we differenced the series once and took one seasonal difference, <i>d</i> = 1 and
            <i>D</i> = 1. Also, since we have monthly data, we know
            that <i>m</i> = 12. Therefore, we know that our final model will be a SARIMA( <i class="calibre3">p</i>,1,
            <i>q</i>)( <i>P</i>,1, <i class="calibre3">Q</i>)12 model. </p>
        <p> <i><b>11.3.1 Performing model selection</b></i></p>
        <p>We have established that our model will be a SARIMA( <i>p</i>,1, <i>q</i>)(
            <i>P</i>,1, <i>Q</i>)12 model. Now we need to
            find the optimal values of <i>p</i>, <i>q</i>, <i>P</i>,
            and <i>Q</i>. This is the model selection step where we choose the parameters that
            minimize
            the Akaike information criterion (AIC). </p>
        <p>To do so, we'll first split the data into train and test sets. As specified in the steps in
            the chapter introduction, the test set will consist of the last 36 months of data. </p>
        <p>train = df.y[:168]</p>
        <p>test = df.y[168:]</p>
        <p><b>Print out the length of the </b></p>
        <p><b>test set to make sure that it </b></p>
        <p>print(len(test)) </p>
        <p><b>contains the last 36 months. </b></p>
        <p><a id="calibre_link-274"></a> <i><b>11.3</b></i></p>
        <p> <i><b>Modeling the data</b></i></p>
        <p><b>223</b></p>
        <p>With our split done, we can now use the optimize_SARIMAX function to find the values of <i
                class="calibre3">p</i>, <i>q</i>, <i>P</i>, and <i class="calibre3">Q</i> that minimize the AIC. Note
            that we can use optimize_SARIMAX</p>
        <p>here because SARIMA is a special case of the more general SARIMAX model. The</p>
        <p>function is shown in the following listing. </p>
        <p>Listing 11.1</p>
        <p>Function to find the values of <i><b>p</b></i>, <i><b>q</b></i>, <i><b class="calibre4">P</b></i>, and
            <i><b>Q</b></i> that minimize the AIC</p>
        <p>from typing import Union</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>def optimize_SARIMAX(endog: Union[pd.Series, list], exog: Union[pd.Series, </p>
        <p>➥ list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:</p>
        <p></p>
        <p>results = []</p>
        <p></p>
        <p>for order in tqdm_notebook(order_list):</p>
        <p>try: </p>
        <p>model = SARIMAX(</p>
        <p>endog, </p>
        <p>exog, </p>
        <p>order=(order[0], d, order[1]), </p>
        <p>seasonal_order=(order[2], D, order[3], s), </p>
        <p>simple_differencing=False).fit(disp=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p>aic = model.aic</p>
        <p>results.append([order, model.aic])</p>
        <p></p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['(p,q,P,Q)', 'AIC']</p>
        <p></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df</p>
        <p>With the function defined, we can now decide on the range of values to try for <i>p</i>,
            <i>q</i>, <i>P</i>, and <i>Q</i>. Then we'll generate
            a
            list of unique combinations of parameters. Feel free to
            test a different range of values than I've used here. Simply note that the larger the range, the longer
            it
            will take to run the optimize_SARIMAX function. </p>
        <p>ps = range(0, 5, 1)</p>
        <p>qs = range(0, 5, 1)</p>
        <p>Ps = range(0, 5, 1)</p>
        <p>Qs = range(0, 5, 1)</p>
        <p>order_list = list(product(ps, qs, Ps, Qs))</p>
        <p><a id="calibre_link-104"></a><b>224</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>d = 1</p>
        <p>D = 1</p>
        <p>s = 12</p>
        <p>We can now run the optimize_SARIMAX function. In this example, 625 unique combi-</p>
        <p>nations are tested, since we have 5 possible values for 4 parameters. </p>
        <p>SARIMA_result_df = optimize_SARIMAX(train, None, order_list, d, D, s)</p>
        <p>SARIMA_result_df</p>
        <p>Once the function is finished, the result shows that the minimum AIC is achieved with <i
                class="calibre3">p</i> = 2, <i>q</i> = 3, <i>P</i> = 1, and <i class="calibre3">Q</i> = 3. Therefore,
            the optimal model is a SARIMA(2,1,3)(1,1,3)12</p>
        <p>model. </p>
        <p> <i><b>11.3.2 Conducting residual analysis</b></i></p>
        <p>Now that we have the optimal model, we must analyze its residuals to determine whether the
            model can be used or not. This will depend on the residuals, which should behave like white noise. If
            that
            is the case, the model can be used for forecasting. </p>
        <p>We can fit the model and use the plot_diagnostics method to qualitatively ana-</p>
        <p>lyze its residuals. </p>
        <p>SARIMA_model = SARIMAX(train, order=(2,1,3), </p>
        <p>➥ seasonal_order=(1,1,3,12), simple_differencing=False)</p>
        <p>SARIMA_model_fit = SARIMA_model.fit(disp=False)</p>
        <p>SARIMA_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>The result is shown in figure 11.4, and we can conclude from this qualitative analysis that
            the residuals closely resemble white noise. </p>
        <p>The next step is to perform the Ljung-Box test, which determines whether the</p>
        <p>residuals are independent and uncorrelated. The null hypothesis of the Ljung-Box test states
            that the residuals are uncorrelated, just like white noise. Thus, we want the test to return p-values
            larger
            than 0.05. In that case, we cannot reject the null hypothesis and conclude that our residuals are
            independent, and therefore behave like white noise. </p>
        <p>residuals = SARIMA_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p>In this case, all the p-values are above 0.05, so we do not reject the null hypothesis, and
            we conclude that the residuals are independent and uncorrelated. We can conclude that the model can used
            for
            forecasting. </p>
        <p><a id="calibre_link-105"></a><img src="images/000116.jpg" alt="Image 99" class="calibre2" />
        </p>
        <p> <i><b>11.4</b></i></p>
        <p> <i><b>Forecasting and evaluating the model's
                    performance</b></i></p>
        <p><b>225</b></p>
        <p>Figure 11.4</p>
        <p>Visual diagnostics of the residuals. In the top-left plot, the residuals have no trend over
            time, and the variance seems constant. At the top right, the distribution of the residuals is very close
            to
            a normal distribution. This is further supported by the Q-Q plot at the bottom left, which displays a
            fairly
            straight line that sits on <i><b>y</b></i> = <i><b>x</b></i>. Finally, the
            correlogram at the bottom right shows no significant
            coefficients after lag 0, just like white noise. </p>
        <p> <i><b>11.4</b></i></p>
        <p> <i><b>Forecasting and evaluating the model's
                    performance</b></i></p>
        <p>We have a model that can be used for forecasting, so we'll now perform rolling forecasts of
            12 months over the test set of 36 months. That way we'll have a better evaluation of our model's
            performance, as testing on fewer data points might lead to skewed results. We'll use the naive seasonal
            forecast as a baseline; it will simply take the last 12</p>
        <p>months of data and use them as forecasts for the next 12 months. </p>
        <p>We'll first define the rolling_forecast function to generate the predictions over the entire
            test set with a window of 12 months. The function is shown in the following listing. </p>
        <p>Listing 11.2</p>
        <p>Function to perform a rolling forecast over a horizon</p>
        <p>def rolling_forecast(df: pd.DataFrame, train_len: int, horizon: int, </p>
        <p>➥ window: int, method: str) -&gt; list:</p>
        <p><a id="calibre_link-403"></a><b>226</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>total_len = train_len + horizon</p>
        <p>end_idx = train_len</p>
        <p>if method == 'last_season':</p>
        <p>pred_last_season = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>last_season = df['y'][i-window:i].values</p>
        <p>pred_last_season.extend(last_season)</p>
        <p></p>
        <p>return pred_last_season</p>
        <p></p>
        <p>elif method == 'SARIMA':</p>
        <p>pred_SARIMA = []</p>
        <p></p>
        <p>for i in range(train_len, total_len, window):</p>
        <p>model = SARIMAX(df['y'][:i], order=(2,1,3), </p>
        <p>➥ seasonal_order=(1,1,3,12), simple_differencing=False)</p>
        <p>res = model.fit(disp=False)</p>
        <p>predictions = res.get_prediction(0, i + window - 1)</p>
        <p>oos_pred = predictions.predicted_mean.iloc[-window:]</p>
        <p>pred_SARIMA.extend(oos_pred)</p>
        <p></p>
        <p>return pred_SARIMA</p>
        <p>Next, we'll create a DataFrame to hold the predictions as well as the actual values. This is
            simply a copy of the test set. </p>
        <p>pred_df = df[168:]</p>
        <p>Now we can define the parameters to be used for the rolling_forecast function. </p>
        <p>The dataset contains 204 rows, and the test set contains 36 data points, which means the
            length of the training set is 204 &ndash; 36 = 168. The horizon is 36, since our test set contains 36
            months
            of data. Finally, the window is 12 months, as we are forecasting 12</p>
        <p>months at a time. </p>
        <p>With those values set, we can record the predictions coming from our baseline, which is a
            naive seasonal forecast. It simply takes the last 12 months of observed data and uses them as forecasts
            for
            the next 12 months. </p>
        <p>TRAIN_LEN = 168</p>
        <p>HORIZON = 36</p>
        <p>WINDOW = 12</p>
        <p>pred_df['last_season'] = rolling_forecast(df, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'last_season')</p>
        <p>Next, we'll compute the forecasts from the SARIMA model. </p>
        <p>pred_df['SARIMA'] = rolling_forecast(df, TRAIN_LEN, HORIZON, WINDOW, </p>
        <p>➥ 'SARIMA')</p>
        <p>At this point, pred_df contains the actual values, the forecasts from the naive seasonal
            method, and the forecasts from the SARIMA model. We can use this to visualize our</p>
        <p><a id="calibre_link-404"></a><img src="images/000072.jpg" alt="Image 100" class="calibre2" /></p>
        <p> <i><b>11.4</b></i></p>
        <p> <i><b>Forecasting and evaluating the model's
                    performance</b></i></p>
        <p><b>227</b></p>
        <p>forecasts against the actual values. For clarity, we'll limit the <i>x</i>-axis to zoom
            in
            on the test period. The resulting plot is shown in figure 11.5.
        </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df.y)</p>
        <p>ax.plot(pred_df.y, 'b-', label='actual')</p>
        <p>ax.plot(pred_df.last_season, 'r:', label='naive seasonal')</p>
        <p>ax.plot(pred_df.SARIMA, 'k--', label='SARIMA')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Number of anti-diabetic drug prescriptions')</p>
        <p>ax.axvspan(168, 204, color='#808080', alpha=0.2)</p>
        <p>ax.legend(loc=2)</p>
        <p>plt.xticks(np.arange(6, 203, 12), np.arange(1992, 2009, 1))</p>
        <p>plt.xlim(120, 204)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 11.5</p>
        <p>Forecasts of the number of antidiabetic drug prescriptions in Australia. </p>
        <p>The predictions from the baseline are shown as a dotted line, while the predictions from the
            SARIMA model are shown as a dashed line. </p>
        <p>In figure 11.5 you can see that the predictions from the SARIMA model (the dashed line)
            follow the actual values more closely than the naive seasonal forecasts (the dotted line). We can
            therefore
            intuitively expect the SARIMA model to have performed better than the baseline method. </p>
        <p>To evaluate the performance quantitatively, we'll use the mean absolute percent-</p>
        <p>age error (MAPE). The MAPE is easy to interpret, as it returns a percentage error. </p>
        <p><a id="calibre_link-405"></a><img src="images/000112.jpg" alt="Image 101" class="calibre2" /></p>
        <p><b>228</b></p>
        <p>CHAPTER 11</p>
        <p> <i><b>Capstone: Forecasting the number of antidiabetic
                    drug prescriptions in Australia</b></i></p>
        <p>def mape(y_true, y_pred):</p>
        <p>return np.mean(np.abs((y_true - y_pred) / y_true)) * 100</p>
        <p>mape_naive_seasonal = mape(pred_df.y, pred_df.last_season)</p>
        <p>mape_SARIMA = mape(pred_df.y, pred_df.SARIMA)</p>
        <p>print(mape_naive_seasonal, mape_SARIMA)</p>
        <p>This prints out a MAPE of 12.69% for the baseline and 7.90% for the SARIMA model. </p>
        <p>We can optionally plot the MAPE of each model in a bar chart for a nice visualization, as
            shown in figure 11.6. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>x = ['naive seasonal', 'SARIMA(2,1,3)(1,1,3,12)']</p>
        <p>y = [mape_naive_seasonal, mape_SARIMA]</p>
        <p>ax.bar(x, y, width=0.4)</p>
        <p>ax.set_xlabel('Models')</p>
        <p>ax.set_ylabel('MAPE (%)')</p>
        <p>ax.set_ylim(0, 15)</p>
        <p>for index, value in enumerate(y):</p>
        <p>plt.text(x=index, y=value + 1, s=str(round(value,2)), ha='center')</p>
        <p>plt.tight_layout()</p>
        <p>Figure 11.6</p>
        <p>The MAPE for the naive seasonal forecast and the SARIMA model. Since </p>
        <p>the MAPE of the SARIMA model is lower than the MAPE of the baseline, we can </p>
        <p>conclude that the SARIMA model should be used to forecast the number of antidiabetic drug
            prescriptions. </p>
        <p><a id="calibre_link-106"></a> <i><b>Next steps</b></i></p>
        <p><b>229</b></p>
        <p>Since the SARIMA model achieves the lowest MAPE, we can conclude that the
            SARIMA(2,1,3)(1,1,3)12 model should be used to forecast the monthly number of antidiabetic drug
            prescriptions in Australia. </p>
        <p> <i><b>Next steps</b></i></p>
        <p>Congratulations on completing this capstone project. I hope that you were able to complete
            it on your own and that you now feel confident in your skills and knowledge of time series forecasting
            using
            statistical models. </p>
        <p>Of course, practice makes perfect, so I highly encourage you to find other time series
            datasets and practice modeling and forecasting them. This will help you build your intuition and hone
            your
            skills. </p>
        <p>In the next chapter, we'll start a new section where we'll use deep learning models to model
            and forecast complex time series with high dimensionality. </p>
        <p><a id="calibre_link-406"></a> </p>
        <p><a id="calibre_link-14"></a> <i>Part 3</i></p>
        <p> <i>Large-scale forecasting</i></p>
        <p> <i>with deep learning</i></p>
        <p>Statistical models have their limitations, especially when a dataset is large</p>
        <p>and has many features and nonlinear relationships. In such cases, deep learning</p>
        <p>is the perfect tool for time series forecasting. In this part of the book, we'll work with a
            massive dataset and apply different deep learning architectures, such as</p>
        <p>long short-term memory (LSTM), a convolutional neural network (CNN), and</p>
        <p>an autoregressive deep neural network, to predict the future of our series. </p>
        <p>Again, we'll conclude this part with a capstone project to test your skills. </p>
        <p>Deep learning is a subset of machine learning, and it is therefore possible to</p>
        <p>use more traditional machine learning algorithms for time series forecasting, such as
            gradient-boosted trees. To keep this section reasonable, we won't cover</p>
        <p>those techniques specifically, although data windowing is required to forecast time series
            with machine learning, and we'll apply this concept numerous times. </p>
        <p><a id="calibre_link-407"></a> </p>
        <p><a id="calibre_link-15"></a> <i>Introducing deep learning</i></p>
        <p> <i>for time series forecasting</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Using deep learning for forecasting</p>
        <p> Exploring different types of deep learning models</p>
        <p> Getting ready to apply deep learning to time </p>
        <p>series forecasting</p>
        <p>In the last chapter, we concluded the part of the book on time series forecasting using
            statistical models. Those models work particularly well when you have small datasets (usually less than
            10,000 data points), and when the seasonal period is monthly, quarterly, or yearly. In situations where
            you
            have daily seasonality or where the dataset is very large (more than 10,000 data points), those
            statistical
            models become very slow, and their performance degrades. </p>
        <p>Thus, we turn to deep learning. <i>Deep learning</i> is a subset of machine
            learning that focuses on building models on the neural network architecture. Deep learning has the
            advantage
            that it tends to perform better as more data is available, making it a great choice for forecasting
            high-dimensional time series. </p>
        <p>In this part of the book, we'll explore various model architectures so you'll have a set of
            tools for tackling virtually any time series forecasting problem. Note that I'll assume you have some
            familiarity with deep learning, so topics such as activation functions, loss functions, batches, layers,
            and
            epochs should be known. This part of <b>233</b></p>
        <p><a id="calibre_link-107"></a><b>234</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>the book will not serve as an introduction to deep learning, but rather focuses on applying
            deep learning to time series forecasting. Of course, each model architecture will be thoroughly
            explained,
            and you will gain an intuition as to why a particular architecture might work better than another in
            particular situations. Throughout these chapters, we will use TensorFlow, or more specifically Keras, to
            build different deep learning models. </p>
        <p>In this chapter specifically, we identify the conditions that justify the use of deep
            learning and explore the different types of models that can be built, such as single-step, multi-step,
            and
            multi-output models. We'll conclude the chapter with the initial setup that will get us ready to apply
            deep
            learning models in the following chapters. </p>
        <p>Finally, we'll explore the data, perform feature engineering, and split the data into
            training, validation, and testing sets. </p>
        <p> <i><b>12.1</b></i></p>
        <p> <i><b>When to use deep learning for time series
                    forecasting</b></i></p>
        <p>Deep learning shines when we have large complex datasets. In those situations, deep learning
            can leverage all the available data to infer relationships between each feature and the target, usually
            resulting in good forecasts. </p>
        <p>In the context of time series, a dataset is considered to be large when we have more than
            10,000 data points. Of course, this is an approximation rather than a hard-set limit, so if you have
            8,000
            data points, deep learning could be a viable option. </p>
        <p>When the size of the dataset is large, any declination of the SARIMAX model will take a long
            time to fit, which is not ideal for model selection, as we usually fit many models during that step.
        </p>
        <p>If your data has multiple seasonal periods, the SARIMAX model cannot be used. </p>
        <p>For example, suppose you must forecast the hourly temperature. It is reasonable to assume
            that there will be daily seasonality, as temperature tends to be lower at night and higher during the
            day,
            but there is also yearly seasonality, due to temperatures being lower in winter and higher during
            summer. In
            such a case, deep learning can be used to leverage the information from both seasonal periods to make
            forecasts. In fact, from experience, fitting a SARIMA model in such a case will usually result in
            residuals
            that are not normally distributed and still correlated, meaning that the model cannot be used at all.
        </p>
        <p>Ultimately, deep learning is used either when statistical models take too much time to fit
            or when they result in correlated residuals that do not approximate white noise. </p>
        <p>This can be due to the fact that there is another seasonal period that cannot be considered
            in the model, or simply because there is a nonlinear relationship between the features and the target.
            In
            those cases, deep learning models can be used to capture this nonlinear relationship, and they have the
            added advantage of being very fast to train. </p>
        <p> <i><b>12.2</b></i></p>
        <p> <i><b>Exploring the different types of deep learning
                    models</b></i></p>
        <p>There are three main types of deep learning models that we can build for time series
            forecasting: single-step models, multi-step models, and multi-output models. </p>
        <p><a id="calibre_link-408"></a> <i><b>12.2</b></i></p>
        <p> <i><b>Exploring the different types of deep learning
                    models</b></i></p>
        <p><b>235</b></p>
        <p>The single-step model is the simplest of the three. Its output is a single value
            representing the forecast of one variable one step into the future. The model therefore simply returns a
            scalar, as shown in figure 12.1. </p>
        <p><b>Single-step model</b></p>
        <p>1 timestep</p>
        <p>Figure 12.1</p>
        <p>The single-step model outputs the </p>
        <p>value of one target one timestep into the future. The </p>
        <p>1 target</p>
        <p>output is therefore a scalar. </p>
        <p>Single-step model</p>
        <p>The single-step model outputs a single value representing the prediction for the next
            timestep. The input can be of any length, but the output remains a single prediction for the next
            timestep.
        </p>
        <p>Next we can have a multi-step model, meaning that we output the value for one target, but
            for many timesteps into the future. For example, given hourly data, we may want to forecast the next 24
            hours. In that case, we have a multi-step model, since we are forecasting 24 timesteps into the future.
            The
            output is a 24 × 1 matrix, as shown in figure 12.2. </p>
        <p><b>Multi-step model</b></p>
        <p>24 timesteps</p>
        <p>Figure 12.2</p>
        <p>A multi-step model outputs the </p>
        <p>predictions for 1 variable multiple timesteps into </p>
        <p>the future. This example predicts 24 timesteps, </p>
        <p>1 target</p>
        <p>resulting in a 24 x 1 output matrix. </p>
        <p><a id="calibre_link-409"></a><b>236</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>Multi-step model</p>
        <p>In a multi-step model, the output of the model is a sequence of values representing
            predictions for many timesteps into the future. For example, if the model predicts the next 6 hours, 24
            hours, or 12 months, it is a multi-step model. </p>
        <p>Finally, the multi-output model generates predictions for more than one target. For example,
            if we were to predict both the temperature and humidity, we would use a multi-output model. This model
            can
            output as many timesteps as desired. In figure 12.3, a multi-output model returning predictions for two
            features for the next 24 timesteps is shown. In that particular case, the output is a 24 x 2 matrix.
        </p>
        <p><b>Multi-output model</b></p>
        <p>24 timesteps</p>
        <p>Figure 12.3</p>
        <p>A multi-output model makes </p>
        <p>predictions for more than one target for one </p>
        <p>or more timesteps in the future. Here the </p>
        <p>model outputs predictions for two targets </p>
        <p>2 targets</p>
        <p>for the next 24 timesteps. </p>
        <p>Multi-output model</p>
        <p>A multi-output model generates predictions for more than one target. For example, if we
            forecast the temperature and wind speed, it is a multi-output model. </p>
        <p><a id="calibre_link-108"></a> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p><b>237</b></p>
        <p>Each of these models can have different architectures. For example, a convolutional neural
            network can be used as a single-step model, a multi-step model, or a multi-output model. In the
            following
            chapters, we will implement different model architectures and apply them for all three model types. </p>
        <p>This brings us to the stage where we'll do the initial setup for the different deep learning
            models we'll implement in the next five chapters. </p>
        <p> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p>From here through chapter 17, we will use the metro interstate traffic volume dataset
            available on the UCI machine learning repository. The original dataset recorded the hourly westbound
            traffic
            on I-94 between Minneapolis and St. Paul in Minnesota, from 2012 to 2018. For the purpose of learning
            how to
            apply deep learning for time series forecasting, the dataset has been shortened and cleaned to get rid
            of
            missing values. While the cleaning steps are not covered in this chapter, you can still consult the
            preprocessing code in the GitHub repository for this chapter. Our main forecasting goal is to predict
            the
            hourly traffic volume. In the case of multi-output models, we will also forecast the hourly temperature.
            In
            this initial setup for the next several chapters, we'll load the data, perform feature engineering, and
            split it into training, validation, and testing sets. </p>
        <p>We will use TensorFlow, or more specifically Keras, in this part of the book. At the time of
            writing, the latest stable version of TensorFlow was 2.6.0, which is what I'll use in this and the
            following
            chapters. </p>
        <p>NOTE</p>
        <p>The full source code for this chapter is available on <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH12">GitHub:
                https://</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH12">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH12.
            </a></p>
        <p> <i><b>12.3.1 Performing data exploration</b></i></p>
        <p>We will first load the data using pandas. </p>
        <p>df = </p>
        <p>➥ pd.read_csv('../data/metro_interstate_traffic_volume_preprocessed.csv')</p>
        <p>df.head()</p>
        <p>As mentioned, this dataset is a shortened and cleaned version of the original dataset
            available on the UCI machine learning repository. In this case, the dataset starts on September 29,
            2016, at
            5 p.m. and ends on September 30, 2018, at 11 p.m. Using df.shape, we can see that we have a total of six
            features and 17,551 rows. </p>
        <p>The features include the date and time, the temperature, the amount of rain and</p>
        <p>snow, the cloud coverage, as well as the traffic volume. Table 12.1 describes each column in
            more detail. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-410"></a><b>238</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>Table 12.1</p>
        <p>The variables in the metro interstate traffic volume dataset</p>
        <p>Feature</p>
        <p>Description</p>
        <p>date_time</p>
        <p>Date and time of the data, recorded in the CST time zone. The format is </p>
        <p>YYYY-MM-DD HH:MM:SS. </p>
        <p>temp</p>
        <p>Average temperature recorded in the hour, expressed in Kelvin. </p>
        <p>rain_1h</p>
        <p>Amount of rain that occurred in the hour, expressed in millimeters. </p>
        <p>snow_1h</p>
        <p>Amount of snow that occurred in the hour, expressed in millimeters. </p>
        <p>clouds_all</p>
        <p>Percentage of cloud cover during the hour. </p>
        <p>traffic_volume</p>
        <p>Volume of traffic reported westbound on I-94 during the hour. </p>
        <p>Now, let's visualize the evolution of the traffic volume over time. Since our dataset is
            very large, with more than 17,000 records, we'll plot only the first 400 data points, which is roughly
            equivalent to two weeks of data. The result is shown in figure 12.4. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['traffic_volume'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Traffic volume')</p>
        <p>plt.xticks(np.arange(7, 400, 24), ['Friday', 'Saturday', 'Sunday', </p>
        <p>➥ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', </p>
        <p>➥ 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', </p>
        <p>➥ 'Saturday', 'Sunday'])</p>
        <p>plt.xlim(0, 400)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In figure 12.4 you'll notice clear daily seasonality, since the traffic volume is lower at
            the start and end of each day. You'll also see a smaller traffic volume during the weekends. As for the
            trend, two weeks of data is likely insufficient to draw a reasonable conclusion, but it seems that the
            volume is neither increasing nor decreasing over time in the figure. </p>
        <p>We can also plot the hourly temperature, as it will be a target for our multi-output models.
            Here, we'll expect to see both yearly and daily seasonality. The yearly seasonality should be due to the
            seasons in the year, while the daily seasonality will be due to the fact that temperatures tend to be
            lower
            at night and higher during the day. </p>
        <p></p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-411"></a><img src="images/000111.jpg" alt="Image 102" class="calibre2" /></p>
        <p> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p><b>239</b></p>
        <p>Figure 12.4</p>
        <p>Westbound traffic volume on I-94 between Minneapolis and St. Paul in Minnesota, starting on
            September 29, 2016, at 5 p.m. You'll notice clear daily seasonality, with traffic being lower at the
            start
            and end of each day. </p>
        <p>Let's first visualize the hourly temperature over the entire dataset to see if we can
            identify any yearly seasonality. The result is shown in figure 12.5. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['temp'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Temperature (K)')</p>
        <p>plt.xticks([2239, 10999], [2017, 2018])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In figure 12.5 you'll see a yearly seasonal pattern in the hourly temperature, since
            temperatures are lower at the end and beginning of the year (winter in Minnesota), and</p>
        <p><a id="calibre_link-412"></a><img src="images/000070.jpg" alt="Image 103" class="calibre2" /></p>
        <p><b>240</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>Figure 12.5</p>
        <p>Hourly temperature (in Kelvin) from September 29, 2016, to September 30, 2018. Although
            there is noise, we can see a yearly seasonal pattern. </p>
        <p>higher in the middle of the year (summer). Thus, as expected, the temperature has yearly
            seasonality. </p>
        <p>Now let's verify whether we can observe daily seasonality in temperature. The result is
            shown in figure 12.6. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['temp'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_ylabel('Temperature (K)')</p>
        <p>plt.xticks(np.arange(7, 400, 24), ['Friday', 'Saturday', 'Sunday', </p>
        <p>➥ 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', </p>
        <p>➥ 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', </p>
        <p>➥ 'Saturday', 'Sunday'])</p>
        <p>plt.xlim(0, 400)</p>
        <p><a id="calibre_link-109"></a><img src="images/000038.jpg" alt="Image 104" class="calibre2" /></p>
        <p> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p><b>241</b></p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 12.6</p>
        <p>Hourly temperature (in Kelvin) starting on September 29, 2016, at 5 p.m. CST. Although it is
            a bit noisy, we can see that temperatures are indeed lower at the start and end of each day and peak
            during
            midday, suggesting daily seasonality. </p>
        <p>In figure 12.6 you'll notice that the temperature is indeed lower at the start and end of
            each day and peaks toward the middle of each day. This suggests daily seasonality, just as we observed
            for
            traffic volume in figure 12.4. </p>
        <p> <i><b>12.3.2 Feature engineering and data
                    splitting</b></i></p>
        <p>With our data exploration done, we'll move on to feature engineering and data splitting. In
            this section, we will study each feature and create new ones that will help our models forecast the
            traffic
            volume and hourly temperature. Finally, we'll split the data and save each set as a CSV file for later
            use.
        </p>
        <p>A great way to study the features of a dataset is to use the describe method from pandas.
            This method returns the number of records for each feature, allowing us to</p>
        <p><a id="calibre_link-212"></a><b>242</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>quickly identify missing values, the mean, standard deviation, quartiles, and maximum and
            minimum values for each feature. </p>
        <p><b>The transpose method puts </b></p>
        <p>df.describe().transpose() </p>
        <p><b>each feature on its own row. </b></p>
        <p>From the output, you'll notice that rain_1h is mostly 0 throughout the dataset, as its third
            quartile is still at 0. Since at least 75% of the values for rain_1h are 0, it is unlikely that it is a
            strong predictor of traffic volume. Thus, this feature will be removed. </p>
        <p>Looking at snow_1h, you'll notice that this variable is at 0 through the entire dataset.
            This is easily observable, since its minimum and maximum values are both 0. </p>
        <p>Thus, this is not predictive of the variation in traffic volume over time. This feature will
            also be removed from the dataset. </p>
        <p>cols_to_drop = ['rain_1h', 'snow_1h']</p>
        <p>df = df.drop(cols_to_drop, axis=1)</p>
        <p>Now we reach the interesting problem of encoding time as a usable feature for our deep
            learning models. Right now, the date_time feature is not usable by our models, since it is a datetime
            string. We will thus convert it into a numerical value. </p>
        <p>A simple way to do that is to express the date as a number of seconds. This is achieved
            through the use of the timestamp method from the datetime library. </p>
        <p>timestamp_s = </p>
        <p>➥ pd.to_datetime(df['date_time']).map(datetime.datetime.timestamp)</p>
        <p>Unfortunately, we are not done, as this simply expresses each date in seconds, as shown in
            figure 12.7. This leads us to losing the cyclical nature of time, because the number of seconds simply
            increases linearly with time. </p>
        <p>Therefore, we must apply a transformation to recover the cyclical behavior of time. A simple
            way to do that is to apply a sine transformation. We know that the sine function is cyclical, bounded
            between &ndash;1 and 1. This will help us regain part of the cyclical property of time. </p>
        <p><b>The timestamp is in seconds, so we must </b></p>
        <p><b>calculate the number of seconds in a day </b></p>
        <p><b>Application of the sine </b></p>
        <p><b>before applying the sine transformation. </b></p>
        <p><b>transformation. Notice </b></p>
        <p>day = 24 * 60 * 60 </p>
        <p><b>that we use radians in </b></p>
        <p><b>the sine function. </b></p>
        <p>df['day_sin'] = (np.sin(timestamp_s * (2*np.pi/day))).values </p>
        <p>With a single sine transformation, we regain some of the cyclical property that was lost
            when converting to seconds. However, at this point, 12 p.m. is equivalent to 12 a.m., and 5 p.m. is
            equivalent to 5 a.m. This is undesired, as we want to distinguish between morning and afternoon. Thus,
            we'll
            apply a cosine transformation. We know that cosine is out of phase with the sine function. This allows
            us to
            distinguish between</p>
        <p><a id="calibre_link-413"></a><img src="images/000094.jpg" alt="Image 105" class="calibre2" /></p>
        <p> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p><b>243</b></p>
        <p>Figure 12.7</p>
        <p>Number of seconds expressing each date in the dataset. The number of seconds linearly
            increases with time, meaning that we lose the cyclical property of time. </p>
        <p>5 a.m. and 5 p.m., expressing the cyclical nature of time in a day. At this point, we can
            remove the date_time column from the DataFrame. </p>
        <p>df['day_cos'] = (np.cos(timestamp_s * (2*np.pi/day))).values </p>
        <p>df = df.drop(['date_time'], axis=1) </p>
        <p><b>Apply the cosine</b></p>
        <p><b>Remove the </b></p>
        <p><b>transformation to the</b></p>
        <p><b>date_time column. </b></p>
        <p><b>timestamp in seconds. </b></p>
        <p>We can quickly convince ourselves that these transformations worked by plotting a sample of
            day_sin and day_cos. The result is shown in figure 12.8. </p>
        <p>df.sample(50).plot.scatter('day_sin','day_cos').set_aspect('equal'); </p>
        <p>In figure 12.8 you'll notice that the points form a circle, just like a clock. Therefore, we
            have successfully expressed each timestamp as a point on the clock, meaning that we now have numerical
            values that retain the cyclical nature of time in a day, and this</p>
        <p><a id="calibre_link-414"></a><img src="images/000091.jpg" alt="Image 106" class="calibre2" /></p>
        <p><b>244</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p>Figure 12.8</p>
        <p>Plot of a sample of the <b>day_sin</b> and <b>day_cos</b>
            encoding. We have successfully encoded the time as a numerical value while keeping the daily cycle. </p>
        <p>can be used in our deep learning models. This will be useful since we observed daily
            seasonality for both the temperature and the volume of traffic. </p>
        <p>With the feature engineering complete, we can now split our data train, validation, and test
            sets. The train set is the sample of data used to fit the model. The validation set is a bit like a test
            set
            that the model can peek at to tune its hyperparameters and improve its performance during the model's
            training. The test set is completely separate from the model's training procedure and is used for an
            unbiased evaluation of the model's performance. </p>
        <p>Here we'll use a simple 70:20:10 split for the train, validation, and test sets. While 10%
            of the data seems like a small portion for the test set, remember that we have</p>
        <p><a id="calibre_link-218"></a> <i><b>12.3</b></i></p>
        <p> <i><b>Getting ready to apply deep learning for
                    forecasting</b></i></p>
        <p><b>245</b></p>
        <p>more than 17,000 records, meaning that we will evaluate the model on more than 1,000 data
            points, which is more than enough. </p>
        <p>n = len(df)</p>
        <p><b>First 70% goes to </b></p>
        <p><b>the train set. </b></p>
        <p># Split 70:20:10 (train:validation:test)</p>
        <p>train_df = df[0:int(n*0.7)] </p>
        <p><b>Next 20% goes to </b></p>
        <p>val_df = df[int(n*0.7):int(n*0.9)] </p>
        <p><b>the validation set. </b></p>
        <p>test_df = df[int(n*0.9):] </p>
        <p><b>The remaining 10% </b></p>
        <p><b>goes to the test set. </b></p>
        <p>Before saving the data, we must scale it so all values are between 0 and 1. This decreases
            the time required for training deep learning models, and it improves their performance. We'll use
            MinMaxScaler from sklearn to scale our data. </p>
        <p>Note that we will fit the scaler on the training set to avoid data leakage. That way, we are
            simulating the fact that we only have the training data available when we're using the model, and no
            future
            information is known by the model. The evaluation of the model remains unbiased. </p>
        <p>from sklearn.preprocessing import MinMaxScaler</p>
        <p>scaler = MinMaxScaler()</p>
        <p><b>Fit only on the </b></p>
        <p><b>training set. </b></p>
        <p>scaler.fit(train_df) </p>
        <p>train_df[train_df.columns] = scaler.transform(train_df[train_df.columns])</p>
        <p>val_df[val_df.columns] = scaler.transform(val_df[val_df.columns])</p>
        <p>test_df[test_df.columns] = scaler.transform(test_df[test_df.columns])</p>
        <p>It is worth mentioning why the data is scaled and not normalized. Scaling and normalization
            can be confusing terms for data scientists, as they are often used interchange-ably. In short, scaling
            the
            data affects only its <i>scale</i> and not its <i>distribution</i>.
            Thus,
            it simply forces the values into a certain range. In our case, we force the values to be between 0 and
            1.
        </p>
        <p>Normalizing the data, on the other hand, affects its <i>distribution</i>
            and its <i>scale</i>. Thus, normalizing the data would force it to have a normal
            distribution or a Gaussian distribution. The original range would also change, and plotting the
            frequency of
            each value would generate a classic bell curve. </p>
        <p>Normalizing the data is only useful when the models we use require the data to</p>
        <p>be normal. For example, linear discriminant analysis (LDA) is derived from the assumption of
            a normal distribution, so it is better to normalize data before using LDA. However, in the case of deep
            learning, no assumptions are made, so normalizing is not required. </p>
        <p>Finally, we'll save each set as a CSV file for use in the following chapters. </p>
        <p>train_df.to_csv('../data/train.csv')</p>
        <p>val_df.to_csv('../data/val.csv')</p>
        <p>test_df.to_csv('../data/test.csv')</p>
        <p><a id="calibre_link-110"></a><b>246</b></p>
        <p>CHAPTER 12</p>
        <p> <i><b>Introducing deep learning for time series
                    forecasting</b></i></p>
        <p> <i><b>12.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we looked at the use of deep learning for forecasting and covered the three
            main types of deep learning models. We then explored the data we'll be using and performed feature
            engineering so the data is ready to be used in the next chapter, where we'll apply deep learning models
            to
            forecast traffic volume. </p>
        <p>In the next chapter, we will start by implementing baseline models that will serve as
            benchmarks for more complex deep learning architectures. We will also implement</p>
        <p>linear models, the simplest models that can be built, followed by deep neural networks,
            which have at least one hidden layer. Baselines, linear models, and deep neural networks will be
            implemented
            as single-step models, multi-step models, and multi-output models. You should be excited for the next
            chapter, as we'll start modeling and forecasting using deep learning. </p>
        <p> <i><b>12.5</b></i></p>
        <p> <i><b>Exercise</b></i></p>
        <p>As an exercise, we will prepare some data for use in deep learning exercises in chapters 12
            through 18. This data will be used to develop a deep learning model to forecast the air quality in
            Beijing
            at the Aotizhongxin station. </p>
        <p>Specifically, for univariate modeling, we will ultimately predict the concentration of
            nitrogen dioxide (NO2). For the multivariate problem, we will predict the concentration of nitrogen
            dioxide
            and temperature. </p>
        <p>NOTE</p>
        <p>Predicting the concentration of air pollutants is an important prob-</p>
        <p>lem, as they can have negative health effects on a population, such as cough-</p>
        <p>ing, wheezing, inflammation, and reduced lung function. Temperature also</p>
        <p>plays an important role, because hot air tends to rise, creating a convection</p>
        <p>effect and moving pollutants from the ground to higher altitudes. With accu-</p>
        <p>rate models, we can better manage air pollution and better inform the popu-</p>
        <p>lation to take the right precautions. </p>
        <p>The original dataset is available in the UCI Machine Learning Repository: <a
                href="https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data">https://</a></p>
        <p><a href="https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data">archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data.
            </a>It has been preprocessed and cleaned to treat missing data and make it easy to work with (the
            preprocessing steps are available on GitHub). You will find the data in a CSV file on GitHub:</p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH12">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH12.
            </a></p>
        <p>The objective of this exercise is to prepare the data for deep learning. Follow these steps:
        </p>
        <p>1</p>
        <p>Read the data. </p>
        <p>2</p>
        <p>Plot the target. </p>
        <p>3</p>
        <p>Remove unnecessary columns. </p>
        <p>4</p>
        <p>Identify whether there is daily seasonality and encode the time accordingly. </p>
        <p>5</p>
        <p>Split your data into training, validation, and testing sets. </p>
        <p>6</p>
        <p>Scale the data using MinMaxScaler. </p>
        <p>7</p>
        <p>Save the train, validation, and test sets to be used later. </p>
        <p><a id="calibre_link-306"></a> <i><b>Summary</b></i></p>
        <p><b>247</b></p>
        <p> <i><b>Summary</b></i></p>
        <p> Deep learning for forecasting is used when:</p>
        <p>&ndash; The dataset is large (more than 10,000 data points). </p>
        <p>&ndash; Declination of the SARIMAX model takes a long time to fit. </p>
        <p>&ndash; The residuals of the statistical model still show some correlation. </p>
        <p>&ndash; There is more than one seasonal period. </p>
        <p> There are three types of models for forecasting:</p>
        <p>&ndash; Single-step model: Predicts one step into the future for one variable. </p>
        <p>&ndash; Multi-step model: Predicts many steps into the future for one variable. </p>
        <p>&ndash; Multi-output model: Predicts many variables one or more steps into the future. </p>
        <p><a id="calibre_link-16"></a> <i>Data windowing</i></p>
        <p> <i>and creating baselines</i></p>
        <p> <i>for deep learning</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Creating windows of data</p>
        <p> Implementing baseline models for deep learning</p>
        <p>In the last chapter, I introduced deep learning for forecasting by covering the situations
            where deep learning is ideal and by outlining the three main types of deep learning models: single-step,
            multi-step, and multi-output. We then proceeded with data exploration and feature engineering to remove
            useless features and create new features that will help us forecast traffic volume. With that setup
            done, we
            are now ready to implement deep learning to forecast our target variable, which is the traffic volume.
        </p>
        <p>In this chapter, we'll build a reusable class that will create windows of data. This step is
            probably the most complicated and most useful topic in this part of the book on deep learning. Applying
            deep
            learning for forecasting relies on creating appropriate time windows and specifying the inputs and
            labels.
            Once that is done, you will see that implementing different models becomes incredibly easy, and this
            framework can be reused for different situations and datasets. </p>
        <p>Once you know how to create windows of data, we'll move on to implement baseline models,
            linear models, and deep neural networks. This will let us measure the performance of these models, and
            we
            can then move on to more complex architectures in the following chapters. </p>
        <p><b>248</b></p>
        <p><a id="calibre_link-111"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>249</b></p>
        <p> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p>We'll start off by creating the DataWindow class, which will allow us to format the data
            appropriately to be fed to our deep learning models. We'll also add a plotting method to this class so
            that
            we can visualize the predictions and the actual values. </p>
        <p>Before diving into the code and building the DataWindow class, however, it is important to
            understand why we must perform data windowing for deep learning. </p>
        <p>Deep learning models have a particular way of fitting on data, which we'll explore in the
            next section. Then we'll move on and implement the DataWindow class. </p>
        <p> <i><b>13.1.1 Exploring how deep learning models are
                    trained </b></i></p>
        <p> <i><b>for time series forecasting</b></i></p>
        <p>In the first half of this book, we fit statistical models, such as SARIMAX, on training sets
            and made predictions. We were, in reality, fitting a set of predefined functions of a certain order ( <i
                class="calibre3">p</i>, <i>d</i>, <i>q</i>)( <i class="calibre3">P</i>, <i>D</i>, <i>Q</i>) <i>m</i>,
            and finding out which order resulted in the best fit. </p>
        <p>For deep learning models, we do not have a set of functions to try. Instead, we let the
            neural network derive its own function such that when it takes the inputs, it generates the best
            predictions
            possible. To achieve that, we perform what is called <i>data windowing</i>. This is a
            process in which we define a sequence of data points on our time series and define which are inputs and
            which are labels. That way, the deep learning model can fit on the inputs, generate predictions, compare
            them to the labels, and repeat this process until it cannot improve the accuracy of its predictions.
        </p>
        <p>Let's walk through an example of data windowing. Our data window will use 24</p>
        <p>hours of data to predict the next 24 hours. You probably wonder why are we using just 24
            hours of data to generate predictions. After all, deep learning is data hungry and is used for large
            datasets. The key lies in the data window. A single window has 24 timesteps as input to generate an
            output
            of 24 timesteps. However, the entire training set is separated into multiple windows, meaning that we
            have
            many windows with inputs and labels, as shown in figure 13.1. </p>
        <p>In figure 13.1 you can see the first 400 timesteps of our training set for traffic volume.
            Each data window consists of 24 input timesteps and 24 label timesteps (as shown in figure 13.2), giving
            us
            a total length of 48 timesteps. We can generate many data windows with the training set, so we are, in
            fact,
            leveraging this large quantity of data. </p>
        <p>As you can see in figure 13.2, the data window's total length is the sum of the lengths of
            each sequence. In this case, since we have 24 timesteps as input and 24</p>
        <p>labels, the total length of the data window is 48 timesteps. </p>
        <p>You might think that we are wasting a lot of training data, since in figure 13.2</p>
        <p>timesteps 24 to 47 are labels. Are those never going to be used as inputs? Of course, they
            will be. The DataWindow class that we'll implement in the next section generates data windows with
            inputs
            starting at <i>t</i> = 0. Then it will create another set of data windows, but this
            time
            starting at <i>t </i>= 1. Then it will start at <i>t</i> = 2. This
            goes on
            until it</p>
        <p><a id="calibre_link-415"></a><img class="calibre2" src="images/000003.png" alt="Image 107" /></p>
        <p><b>250</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>raffic volumeT</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>Friday</p>
        <p>Friday</p>
        <p>Friday</p>
        <p>SaturdaySundayMondayTuesday</p>
        <p>ednesday</p>
        <p>Thursday</p>
        <p>SaturdaySundayMondayTuesday</p>
        <p>ednesday</p>
        <p>Thursday</p>
        <p>SaturdaySunday</p>
        <p>W</p>
        <p>W</p>
        <p>Time</p>
        <p>Figure 13.1</p>
        <p>Visualizing the data windows on the training set. The inputs are shown with square markers,
            and the labels are shown with crosses. Each data window consists of 24 timesteps with square markers
            followed by 24 labels with crosses. </p>
        <p>Input</p>
        <p>Label</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Data window</p>
        <p>Figure 13.2</p>
        <p>An example of a data window. Our data window has 24 timesteps as input and 24 </p>
        <p>timesteps as output. The model will then use 24 hours of input to generate 24 hours of
            predictions. </p>
        <p>The total length of the data window is the sum of the length of inputs and labels. In this
            case, we have a total length of 48 timesteps. </p>
        <p>cannot have a sequence of 24 consecutive labels in the training set, as illustrated in
            figure 13.3. </p>
        <p><a id="calibre_link-219"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>251</b></p>
        <p>Input</p>
        <p>Label</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Data window #1</p>
        <p>Input</p>
        <p>Label</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= 2</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= 26</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 47</p>
        <p> <i>t </i>= 48</p>
        <p>Data window #2</p>
        <p>Input</p>
        <p>Label</p>
        <p> <i>t </i>= 2</p>
        <p> <i>t </i>= 3</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= 26</p>
        <p> <i>t </i>= 27</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 48</p>
        <p> <i>t </i>= 49</p>
        <p>Data window #3</p>
        <p>Figure 13.3</p>
        <p>Visualizing the different data windows that are generated by the <b>DataWindow</b>
            class.
        </p>
        <p>You can see that by repeatedly shifting the starting point by one timestep, we use as much
            of the training data as possible to fit our deep learning models. </p>
        <p>To make computation more efficient, deep learning models are trained with <i>batches</i>. A
            batch is simply a collection of data windows that are fed to the model
            for training, as shown in figure 13.4. </p>
        <p>Figure 13.4 shows an example of a batch with a batch size of 32. That means that 32 data
            windows are grouped together and used to train the model. Of course, this is only one batch&mdash;the
            DataWindow class generates as many batches as possible with the given training set. In our case, we have
            a
            training set with 12,285 rows. If each batch has 32 data windows, that means that we will have 12285/32
            =
            384 batches. </p>
        <p>Training the model on all 384 batches once is called one <i>epoch</i>. One
            epoch often does not result in an accurate model, so the model will train for as many epochs as
            necessary
            until it cannot improve the accuracy of its predictions. </p>
        <p>The final important concept in data windowing for deep learning is <i>shuffling</i>. I
            mentioned in the very first chapter of this book that time series data
            cannot be shuffled. Time series data has an order, and that order must be kept, so why are we shuffling
            the
            data here? </p>
        <p>In this context, shuffling occurs at the batch level, not inside the data window&mdash;</p>
        <p>the order of the time series itself is maintained within each data window. Each data window
            is independent of all others. Therefore, in a batch, we can shuffle the data</p>
        <p><a id="calibre_link-416"></a><b>252</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Data window #1</p>
        <p>Data window #2</p>
        <p>Data window #3</p>
        <p><b>Batch #</b>1</p>
        <p><b>Batch size = 32</b></p>
        <p>Data window #30</p>
        <p>Data window #31</p>
        <p>Data window #32</p>
        <p>Figure 13.4</p>
        <p>A batch is simply a collection of data windows that are used for training the deep learning
            model. </p>
        <p>windows and still keep the order of our time series, as shown in figure 13.5. Shuffling the
            data is not essential, but it is recommended as it tends to make more robust models. </p>
        <p>shuffle = False</p>
        <p>shuffle = True</p>
        <p>Data window #1</p>
        <p>Data window #11</p>
        <p>Data window #2</p>
        <p>Data window #24</p>
        <p>Data window #3</p>
        <p>Data window #3</p>
        <p><b>Batch</b></p>
        <p><b>Batch</b></p>
        <p>Data window #30</p>
        <p>Data window #23</p>
        <p>Data window #31</p>
        <p>Data window #15</p>
        <p>Data window #32</p>
        <p>Data window #8</p>
        <p>Figure 13.5</p>
        <p>Shuffling the data windows in a batch. Each data window is independent of all others, so it
            is safe to shuffle the data windows within a batch. Note that the order of the time series is maintained
            within each data window. </p>
        <p><a id="calibre_link-112"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>253</b></p>
        <p>Now that you understand the inner working of data windowing and how it is used for training
            deep learning models, let's implement the DataWindow class. </p>
        <p> <i><b>13.1.2 Implementing the DataWindow class </b></i>
        </p>
        <p>We are now ready to implement the DataWindow class. This class has the advantage of being
            flexible, meaning that you can use it in a wide variety of scenarios to apply deep learning. The full
            code
            is available on Gi<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">tHub:
                https://github.com/marcopeix/</a></p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">TimeSeriesForecastingInPython/tree/master/CH13%26CH14.
            </a></p>
        <p>The class is based on the width of the input, the width of the label, and the shift. </p>
        <p>The width of the input is simply the number of timesteps that are fed into the model to make
            predictions. For example, given that we have hourly data in our dataset, if we feed the model with 24
            hours
            of data to make a prediction, the input width is 24. If we feed only 12 hours of data, the input width
            is
            12. </p>
        <p>The label width is equivalent to the number of timesteps in the predictions. If we predict
            only one timestep, the label width is 1. If we predict a full day of data (with hourly data), the label
            width is 24. </p>
        <p>Finally, the shift is the number of timesteps separating the input and the predictions. If
            we predict the next timestep, the shift is 1. If we predict the next 24 hours (with hourly data), the
            shift
            is 24. </p>
        <p>Let's visualize some windows of data to better understand these parameters. Fig-</p>
        <p>ure 13.6 shows a window of data where the model predicts the next data point, given a single
            data point. </p>
        <p>Input width = 1</p>
        <p>Label width = 1</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p>Figure 13.6</p>
        <p>A data window where the model predicts one </p>
        <p>timestep in the future, given a single point of data. The input </p>
        <p>width is 1, since the model takes only 1 data point as input. </p>
        <p>Shift = 1</p>
        <p>The label width is also only 1, since the model outputs the </p>
        <p>prediction for 1 timestep only. Since the model predicts the </p>
        <p>next timestep, the shift is also 1. Finally, the total window size </p>
        <p>T otal window size = 2</p>
        <p>is the sum of the shift and the input widths, which equals 2. </p>
        <p>Now let's consider the situation where we feed 24 hours of data to the model in order to
            predict the next 24 hours. The data window in that situation is shown in figure 13.7. </p>
        <p>Now that you understand the concept of input width, label width, and shift, we can create
            the DataWindow class and define its initialization function in listing 13.1. The function will also take
            in
            the training, validation, and test sets, as the windows of data will come from our dataset. Finally,
            we'll
            allow the target column to be specified. </p>
        <p><a id="calibre_link-417"></a><b>254</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Input width = 24</p>
        <p>Label width = 24</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Shift = 24</p>
        <p>Total window size = 48</p>
        <p>Figure 13.7</p>
        <p>Data window where the model predicts the next 24 hours using the last 24 hours of data. </p>
        <p>The input width is 24 and the label width is also 24. Since there are 24 timesteps
            separating the inputs and the predictions, the shift is also 24. This gives a total window size of 48
            timesteps. </p>
        <p>Listing 13.1</p>
        <p>Defining the initialization function of <b>DataWindow</b></p>
        <p><b>Create a dictionary with the name and index of </b></p>
        <p><b>the label column. This will be used for plotting. </b></p>
        <p>class DataWindow():</p>
        <p>def __init__(self, input_width, label_width, shift, </p>
        <p>train_df=train_df, val_df=val_df, test_df=test_df, </p>
        <p>label_columns=None):</p>
        <p></p>
        <p>self.train_df = train_df</p>
        <p>self.val_df = val_df</p>
        <p>self.test_df = test_df</p>
        <p><b>Name of the </b></p>
        <p><b>column that we </b></p>
        <p></p>
        <p><b>wish to predict</b></p>
        <p>self.label_columns = label_columns </p>
        <p>if label_columns is not None:</p>
        <p>self.label_columns_indices = {name: i for i, name in </p>
        <p>➥ enumerate(label_columns)} </p>
        <p>self.column_indices = {name: i for i, name in </p>
        <p>➥ enumerate(train_df.columns)} </p>
        <p><b>Create a dictionary with the name </b></p>
        <p></p>
        <p><b>and index of each column. This will </b></p>
        <p>self.input_width = input_width</p>
        <p><b>be used to separate the features </b></p>
        <p>self.label_width = label_width</p>
        <p><b>from the target variable. </b></p>
        <p>self.shift = shift</p>
        <p><b>Assign </b> </p>
        <p><b>indices </b></p>
        <p><b>The slice function returns a </b></p>
        <p><b>to the inputs. </b></p>
        <p>self.total_window_size = input_width + shift</p>
        <p><b>slice object that specifies how </b></p>
        <p><b>These are </b></p>
        <p></p>
        <p><b>to slice a sequence. In this </b></p>
        <p><b>useful for </b></p>
        <p>self.input_slice = slice(0, input_width) </p>
        <p><b>case, it says that the input </b></p>
        <p><b>plotting. </b> self.input_indices = </p>
        <p><b>slice starts at 0 and ends when </b></p>
        <p>➥ np.arange(self.total_window_size)[self.input_slice] <b>we reach the
                input_width. </b></p>
        <p></p>
        <p>self.label_start = self.total_window_size - self.label_width </p>
        <p>self.labels_slice = slice(self.label_start, None) </p>
        <p>self.label_indices = </p>
        <p>➥ np.arange(self.total_window_size)[self.labels_slice]</p>
        <p><b>Get the index at which the </b></p>
        <p><b>The same steps that were</b></p>
        <p><b>label starts. In this case, it is </b></p>
        <p><b>applied for the inputs are</b></p>
        <p><b>the total window size minus </b></p>
        <p><b>applied for labels. </b></p>
        <p><b>the width of the label. </b></p>
        <p><a id="calibre_link-301"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>255</b></p>
        <p>In listing 13.1 you can see that the initialization function basically assigns the variables
            and manages the indices of the inputs and the labels. Our next step is to split our window between
            inputs
            and labels, so that our models can make predictions based on the inputs and measure an error metric
            against
            the labels. The following split_to_</p>
        <p>inputs_labels function is defined within the DataWindow class. </p>
        <p><b>Slice the window to get the labels using </b></p>
        <p><b>the labels_slice defined in __init__. </b></p>
        <p><b>Slice the window to get the </b></p>
        <p><b>inputs using the input_slice </b></p>
        <p>def split_to_inputs_labels(self, features):</p>
        <p><b>defined in __init__. </b></p>
        <p>inputs = features[:, self.input_slice, :] </p>
        <p>labels = features[:, self.labels_slice, :] </p>
        <p><b>If we have more than one </b></p>
        <p>if self.label_columns is not None: </p>
        <p><b>target, we stack the labels. </b></p>
        <p>labels = tf.stack(</p>
        <p>[labels[:,:,self.column_indices[name]] for name in </p>
        <p>➥ self.label_columns], </p>
        <p>axis=-1</p>
        <p><b>The shape will be [batch, </b></p>
        <p>)</p>
        <p><b>time, features]. At this point, </b></p>
        <p>inputs.set_shape([None, self.input_width, None]) </p>
        <p><b>we only specify the time </b></p>
        <p>labels.set_shape([None, self.label_width, None])</p>
        <p><b>dimension and allow the </b></p>
        <p><b>batch and feature dimensions </b></p>
        <p></p>
        <p>return inputs, labels</p>
        <p><b>to be defined later. </b></p>
        <p>The split_to_inputs_labels function will separate the big data window into two windows: one
            for the inputs and the other for the labels, as shown in figure 13.8. </p>
        <p>Input width = 24</p>
        <p>Label width = 24</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Shift = 24</p>
        <p>Total window size = 48</p>
        <p>split_to_inputs_labels</p>
        <p>Input =</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p>Label =</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Figure 13.8</p>
        <p>The <b>split_to_inputs_labels</b> function simply separates the big data
            window into two windows, where one contains the inputs and the other the labels. </p>
        <p><a id="calibre_link-305"></a><b>256</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Next we'll define a function to plot the input data, the predictions, and the actual values
            (listing 13.2). Since we will be working with many time windows, we'll show only the plot of three time
            windows, but this parameter can easily be changed. Also, the default label will be traffic volume, but
            we
            can change that by specifying any column we choose. Again, this function should be included in the
            DataWindow class. </p>
        <p>Listing 13.2</p>
        <p>Method to plot a sample of data windows</p>
        <p>def plot(self, model=None, plot_col='traffic_volume', max_subplots=3):</p>
        <p>inputs, labels = self.sample_batch</p>
        <p></p>
        <p>plt.figure(figsize=(12, 8))</p>
        <p>plot_col_index = self.column_indices[plot_col]</p>
        <p>max_n = min(max_subplots, len(inputs))</p>
        <p></p>
        <p><b>Plot the inputs. They will</b></p>
        <p>for n in range(max_n):</p>
        <p><b>appear as a continuous</b></p>
        <p>plt.subplot(3, 1, n+1)</p>
        <p><b>blue line with dots. </b></p>
        <p>plt.ylabel(f'{plot_col} [scaled]')</p>
        <p>plt.plot(self.input_indices, inputs[n, :, plot_col_index], </p>
        <p>label='Inputs', marker='.', zorder=-10) </p>
        <p>if self.label_columns:</p>
        <p>label_col_index = self.label_columns_indices.get(plot_col, </p>
        <p>➥ None)</p>
        <p>else:</p>
        <p>label_col_index = plot_col_index</p>
        <p>if label_col_index is None:</p>
        <p>continue</p>
        <p>plt.scatter(self.label_indices, labels[n, :, label_col_index], </p>
        <p>edgecolors='k', marker='s', label='Labels', </p>
        <p>➥ c='green', s=64) </p>
        <p><b>Plot the labels or actual </b></p>
        <p>if model is not None:</p>
        <p><b>values. They will appear </b></p>
        <p>predictions = model(inputs)</p>
        <p><b>as green squares. </b></p>
        <p>plt.scatter(self.label_indices, predictions[n, :, </p>
        <p>➥ label_col_index], </p>
        <p>marker='X', edgecolors='k', label='Predictions', </p>
        <p>c='red', s=64) </p>
        <p><b>Plot the predictions. </b></p>
        <p><b>They will appear as </b></p>
        <p>if n == 0:</p>
        <p><b>red crosses. </b></p>
        <p>plt.legend()</p>
        <p>plt.xlabel('Time (h)')</p>
        <p>We are almost done building the DataWindow class. The last main piece of logic will format
            our dataset into tensors so that they can be fed to our deep learning models. </p>
        <p>TensorFlow comes with a very handy function called timeseries_dataset_from_</p>
        <p>array, which creates a dataset of sliding windows, given an array. </p>
        <p></p>
        <p><a id="calibre_link-260"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>257</b></p>
        <p></p>
        <p><b>Pass in the data. This </b></p>
        <p><b>Targets are set to None, as</b></p>
        <p><b>corr</b></p>
        <p></p>
        <p><b>esponds to our training </b></p>
        <p><b>they are handled by the</b></p>
        <p><b>set, validation set, or test set. </b></p>
        <p><b>split_to_input_labels</b></p>
        <p>def make_dataset(self, data):</p>
        <p><b>function. </b></p>
        <p>data = np.array(data, dtype=np.float32)</p>
        <p>ds = tf.keras.preprocessing.timeseries_dataset_from_array(</p>
        <p>data=data, </p>
        <p>targets=None, </p>
        <p>sequence_length=self.total_window_size, </p>
        <p><b>Define the total length of </b></p>
        <p>sequence_stride=1, </p>
        <p><b>the array, which is equal to </b></p>
        <p>shuffle=True, </p>
        <p><b>the total window length. </b></p>
        <p>batch_size=32 </p>
        <p>)</p>
        <p></p>
        <p><b>Define the number of timesteps </b></p>
        <p>ds = ds.map(self.split_to_inputs_labels)</p>
        <p><b>separating each sequence. In our </b></p>
        <p><b>case, we want the sequences to be </b></p>
        <p>return ds</p>
        <p><b>consecutive, so sequence_stride=1. </b></p>
        <p><b>Define the number of </b></p>
        <p><b>sequences in a single batch. </b></p>
        <p><b>Shuffle the sequences. Keep in mind that the </b></p>
        <p><b>data is still in chronological order. We are simply </b></p>
        <p><b>shuffling the order of the sequences, which </b></p>
        <p><b>makes the model more robust. </b></p>
        <p>Remember that we are shuffling the sequences in a batch. This means that within each
            sequence, the data is in chronological order. However, in a batch of 32</p>
        <p>sequences, we can and should shuffle them to make our model more robust and less prone to
            overfitting. </p>
        <p>We'll conclude our DataWindow class by defining some properties to apply the make_dataset
            function on the training, validation, and testing sets. We'll also create a sample batch that we'll
            cache
            within the class for plotting purposes. </p>
        <p>@property</p>
        <p>def train(self):</p>
        <p>return self.make_dataset(self.train_df)</p>
        <p>@property</p>
        <p>def val(self):</p>
        <p>return self.make_dataset(self.val_df)</p>
        <p>@property</p>
        <p>def test(self):</p>
        <p><b>Get a sample batch of data for </b></p>
        <p>return self.make_dataset(self.test_df)</p>
        <p><b>plotting purposes. If the sample </b></p>
        <p></p>
        <p><b>batch does not exist, we'll retrieve </b></p>
        <p>@property</p>
        <p><b>a sample batch and cache it. </b></p>
        <p>def sample_batch(self): </p>
        <p>result = getattr(self, '_sample_batch', None)</p>
        <p>if result is None:</p>
        <p>result = next(iter(self.train))</p>
        <p>self._sample_batch = result</p>
        <p>return result</p>
        <p><a id="calibre_link-418"></a><b>258</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Our DataWindow class is now complete. The full class with all methods and properties is
            shown in listing 13.3. </p>
        <p>Listing 13.3</p>
        <p>The complete <b>DataWindow</b> class</p>
        <p>class DataWindow():</p>
        <p>def __init__(self, input_width, label_width, shift, </p>
        <p>train_df=train_df, val_df=val_df, test_df=test_df, </p>
        <p>label_columns=None):</p>
        <p></p>
        <p>self.train_df = train_df</p>
        <p>self.val_df = val_df</p>
        <p>self.test_df = test_df</p>
        <p></p>
        <p>self.label_columns = label_columns</p>
        <p>if label_columns is not None:</p>
        <p>self.label_columns_indices = {name: i for i, name in </p>
        <p>➥ enumerate(label_columns)}</p>
        <p>self.column_indices = {name: i for i, name in </p>
        <p>➥ enumerate(train_df.columns)}</p>
        <p></p>
        <p>self.input_width = input_width</p>
        <p>self.label_width = label_width</p>
        <p>self.shift = shift</p>
        <p></p>
        <p>self.total_window_size = input_width + shift</p>
        <p></p>
        <p>self.input_slice = slice(0, input_width)</p>
        <p>self.input_indices = </p>
        <p>➥ np.arange(self.total_window_size)[self.input_slice]</p>
        <p>self.label_start = self.total_window_size - self.label_width</p>
        <p>self.labels_slice = slice(self.label_start, None)</p>
        <p>self.label_indices = </p>
        <p>➥ np.arange(self.total_window_size)[self.labels_slice]</p>
        <p></p>
        <p>def split_to_inputs_labels(self, features):</p>
        <p>inputs = features[:, self.input_slice, :]</p>
        <p>labels = features[:, self.labels_slice, :]</p>
        <p>if self.label_columns is not None:</p>
        <p>labels = tf.stack(</p>
        <p>[labels[:,:,self.column_indices[name]] for name in </p>
        <p>➥ self.label_columns], </p>
        <p>axis=-1</p>
        <p>)</p>
        <p>inputs.set_shape([None, self.input_width, None])</p>
        <p>labels.set_shape([None, self.label_width, None])</p>
        <p></p>
        <p>return inputs, labels</p>
        <p></p>
        <p>def plot(self, model=None, plot_col='traffic_volume', max_subplots=3):</p>
        <p>inputs, labels = self.sample_batch</p>
        <p><a id="calibre_link-419"></a> <i><b>13.1</b></i></p>
        <p> <i><b>Creating windows of data</b></i></p>
        <p><b>259</b></p>
        <p>plt.figure(figsize=(12, 8))</p>
        <p>plot_col_index = self.column_indices[plot_col]</p>
        <p>max_n = min(max_subplots, len(inputs))</p>
        <p></p>
        <p>for n in range(max_n):</p>
        <p>plt.subplot(3, 1, n+1)</p>
        <p>plt.ylabel(f'{plot_col} [scaled]')</p>
        <p>plt.plot(self.input_indices, inputs[n, :, plot_col_index], </p>
        <p>label='Inputs', marker='.', zorder=-10)</p>
        <p>if self.label_columns:</p>
        <p>label_col_index = self.label_columns_indices.get(plot_col, </p>
        <p>➥ None)</p>
        <p>else:</p>
        <p>label_col_index = plot_col_index</p>
        <p>if label_col_index is None:</p>
        <p>continue</p>
        <p>plt.scatter(self.label_indices, labels[n, :, label_col_index], </p>
        <p>edgecolors='k', marker='s', label='Labels', </p>
        <p>➥ c='green', s=64)</p>
        <p>if model is not None:</p>
        <p>predictions = model(inputs)</p>
        <p>plt.scatter(self.label_indices, predictions[n, :, </p>
        <p>➥ label_col_index], </p>
        <p>marker='X', edgecolors='k', label='Predictions', </p>
        <p>c='red', s=64)</p>
        <p>if n == 0:</p>
        <p>plt.legend()</p>
        <p>plt.xlabel('Time (h)')</p>
        <p>def make_dataset(self, data):</p>
        <p>data = np.array(data, dtype=np.float32)</p>
        <p>ds = tf.keras.preprocessing.timeseries_dataset_from_array(</p>
        <p>data=data, </p>
        <p>targets=None, </p>
        <p>sequence_length=self.total_window_size, </p>
        <p>sequence_stride=1, </p>
        <p>shuffle=True, </p>
        <p>batch_size=32</p>
        <p>)</p>
        <p></p>
        <p>ds = ds.map(self.split_to_inputs_labels)</p>
        <p>return ds</p>
        <p></p>
        <p>@property</p>
        <p>def train(self):</p>
        <p>return self.make_dataset(self.train_df)</p>
        <p></p>
        <p>@property</p>
        <p>def val(self):</p>
        <p>return self.make_dataset(self.val_df)</p>
        <p><a id="calibre_link-113"></a><b>260</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>@property</p>
        <p>def test(self):</p>
        <p>return self.make_dataset(self.test_df)</p>
        <p></p>
        <p>@property</p>
        <p>def sample_batch(self):</p>
        <p>result = getattr(self, '_sample_batch', None)</p>
        <p>if result is None:</p>
        <p>result = next(iter(self.train))</p>
        <p>self._sample_batch = result</p>
        <p>return result</p>
        <p>For now, the DataWindow class might seem a bit abstract, but we will soon use it to apply
            baseline models. We will be using this class in all the chapters in this deep learning part of the book,
            so
            you will gradually tame this code and appreciate how easy it is to test different deep learning
            architectures. </p>
        <p> <i><b>13.2</b></i></p>
        <p> <i><b>Applying baseline models</b></i></p>
        <p>With the DataWindow class complete, we are ready to use it. We will apply baseline models as
            single-step, multi-step, and multi-output models. You will see that their implementation is similar and
            incredibly simple when we have the right data windows. </p>
        <p>Recall that a baseline is used as a benchmark to evaluate more complex models. A model is
            performant if it compares favorably to another, so building a baseline is an important step in modeling.
        </p>
        <p> <i><b>13.2.1 Single-step baseline model</b></i></p>
        <p>We'll first implement a single-step model as a baseline. In a single-step model, the input
            is one timestep and the output is the prediction of the next timestep. </p>
        <p>The first step is to generate a window of data. Since we are defining a single-step model,
            the input width is 1, the label width is 1, and the shift is also 1, since the model predicts the next
            timestep. Our target variable is the volume of traffic. </p>
        <p>single_step_window = DataWindow(input_width=1, label_width=1, shift=1, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>For plotting purposes, we'll also define a wider window so we can visualize many predictions
            of our model. Otherwise, we could only visualize one input data point and one output prediction, which
            is
            not very interesting. </p>
        <p>wide_window = DataWindow(input_width=24, label_width=24, shift=1, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>In this situation, the simplest prediction we can make is the last observed value.
            Basically, the prediction is simply the input data point. This is implemented by the class Baseline. As
            you
            can see in the following listing, the Baseline class can also be used for a multi-output model. For now,
            we'll solely focus on a single-step model. </p>
        <p><a id="calibre_link-240"></a> <i><b>13.2</b></i></p>
        <p> <i><b>Applying baseline models</b></i></p>
        <p><b>261</b></p>
        <p>Listing 13.4</p>
        <p>Class to return the input data as a prediction</p>
        <p>class Baseline(Model):</p>
        <p>def __init__(self, label_index=None):</p>
        <p>super().__init__()</p>
        <p>self.label_index = label_index</p>
        <p><b>If no target is specified, we </b></p>
        <p><b>return all columns. This is useful </b></p>
        <p></p>
        <p><b>for multi-output models where </b></p>
        <p>def call(self, inputs):</p>
        <p><b>all columns are to be predicted. </b></p>
        <p>if self.label_index is None: </p>
        <p>return inputs</p>
        <p></p>
        <p>elif isinstance(self.label_index, list): </p>
        <p><b>If we specify a list of targets, it </b></p>
        <p>tensors = []</p>
        <p><b>will return only the specified </b></p>
        <p>for index in self.label_index:</p>
        <p><b>columns. Again, this is used </b></p>
        <p>result = inputs[:, :, index]</p>
        <p><b>for multi-output models. </b></p>
        <p>result = result[:, :, tf.newaxis]</p>
        <p>tensors.append(result)</p>
        <p>return tf.concat(tensors, axis=-1)</p>
        <p><b>Return the input for a </b></p>
        <p></p>
        <p><b>given target variable. </b></p>
        <p>result = inputs[:, :, self.label_index] </p>
        <p>return result[:,:,tf.newaxis]</p>
        <p>With the class defined, we can now initialize the model and compile it to generate
            predictions. To do so, we'll find the index of our target column, traffic_volume, and pass it in to
            Baseline. Note that TensorFlow requires us to provide a loss function and a metric of evaluation. In
            this
            case, and throughout the deep learning chapters, we'll use the mean squared error (MSE) as a loss
            function&mdash;it penalizes large errors, and it generally yields well-fitted models. For the evaluation
            metric, we'll use the mean absolute error (MAE) for its ease of interpretation. </p>
        <p><b>Generate a dictionary with the name and</b></p>
        <p><b>index of each column in the training set. </b></p>
        <p>column_indices = {name: i for i, name in enumerate(train_df.columns)} </p>
        <p>baseline_last = Baseline(label_index=column_indices['traffic_volume']) </p>
        <p><b>Pass the index of the</b></p>
        <p>baseline_last.compile(loss=MeanSquaredError(), </p>
        <p>➥</p>
        <p><b>target column in the</b></p>
        <p>metrics=[MeanAbsoluteError()]) </p>
        <p><b>Baseline class. </b></p>
        <p><b>Compile the model to</b></p>
        <p><b>generate the predictions. </b></p>
        <p>We'll now evaluate the performance of our baseline on both the validation and test sets.
        </p>
        <p>Models built with TensorFlow conveniently come with the evaluate method, which allows us to
            compare the predictions to the actual values and calculate the error metric. </p>
        <p><b>Create a dictionary to hold the MAE </b></p>
        <p><b>Create a dictionary </b></p>
        <p><b>of a model on the validation set. </b></p>
        <p><b>to hold the MAE of </b></p>
        <p><b>a model on the </b></p>
        <p>val_performance = {} </p>
        <p><b>test set. </b></p>
        <p>performance = {} </p>
        <p><b>Store the MAE </b></p>
        <p>val_performance['Baseline - Last'] = </p>
        <p><b>of the baseline on </b></p>
        <p>➥</p>
        <p><b>the validation set. </b></p>
        <p>baseline_last.evaluate(single_step_window.val) </p>
        <p><a id="calibre_link-221"></a><img class="calibre2" src="images/000029.png" alt="Image 108" /></p>
        <p><b>262</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>performance['Baseline - Last'] = </p>
        <p>➥ baseline_last.evaluate(single_step_window.test, verbose=0) </p>
        <p><b>Store the MAE of the</b></p>
        <p><b>baseline on the test set. </b></p>
        <p>Great, we have successfully built a baseline that predicts the last known value and
            evaluated it. We can visualize the predictions using the plot method of the DataWindow class. Remember
            to
            use the wide_window to see more than just two data points. </p>
        <p>wide_window.plot(baseline_last)</p>
        <p>In figure 13.9 the labels are squares and the predictions are crosses. The crosses at each
            timestep are simply the last known value, meaning that we have a baseline that functions as expected.
            Your
            plot may differ from figure 13.9, as the cached sample batch changes every time a data window is
            initialized. </p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.6</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 13.9</p>
        <p>Predictions of our baseline single-step model on three sequences from the sample batch. The
            prediction at each timestep is the last known value, meaning that our baseline works as expected. </p>
        <p>We can optionally print the MAE of our baseline on the test set. </p>
        <p>print(performance['Baseline - Last'][1])</p>
        <p>This returns an MAE of 0.081. More complex models should perform better than the baseline,
            resulting in a smaller MAE. </p>
        <p><a id="calibre_link-114"></a> <i><b>13.2</b></i></p>
        <p> <i><b>Applying baseline models</b></i></p>
        <p><b>263</b></p>
        <p> <i><b>13.2.2 Multi-step baseline models</b></i></p>
        <p>In the previous section, we built a single-step baseline model that simply predicted the
            last known value. For multi-step models, we'll predict more than one timestep into the future. In this
            case,
            we'll forecast the traffic volume for the next 24 hours of data given an input of 24 hours. </p>
        <p>Again, the first step is to generate the appropriate window of data. Because we wish to
            predict 24 timesteps into the future with an input of 24 hours, the input width is 24, the label width
            is
            24, and the shift is also 24. </p>
        <p>multi_window = DataWindow(input_width=24, label_width=24, shift=24, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>With the data window generated, we can now focus on implementing the baseline models. In
            this situation, there are two reasonable baselines:</p>
        <p> Predict the last known value for the next 24 timesteps. </p>
        <p> Predict the last 24 timesteps for the next 24 timesteps. </p>
        <p>With that in mind, let's implement the first baseline, where we'll simply repeat the last
            known value over the next 24 timesteps. </p>
        <p>PREDICTING THE LAST KNOWN VALUE</p>
        <p>To predict the last known value, we'll define a MultiStepLastBaseline class that simply
            takes in the input and repeats the last value of the input sequence over 24 timesteps. This acts as the
            prediction of the model. </p>
        <p>class MultiStepLastBaseline(Model):</p>
        <p><b>Return the last known value</b></p>
        <p>def __init__(self, label_index=None):</p>
        <p><b>of the target column over</b></p>
        <p>super().__init__()</p>
        <p><b>the next 24 timesteps. </b></p>
        <p>self.label_index = label_index</p>
        <p></p>
        <p><b>If no target is specified, return the</b></p>
        <p>def call(self, inputs):</p>
        <p><b>last known value of all columns</b></p>
        <p>if self.label_index is None:</p>
        <p><b>over the next 24 timesteps. </b></p>
        <p>return tf.tile(inputs[:, -1:, :], [1, 24, 1]) </p>
        <p>return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1]) </p>
        <p>Next we'll initialize the class and specify the target column. We'll then repeat the same
            steps as in the previous section, compiling the model and evaluating it on the validation set and test
            set.
        </p>
        <p>ms_baseline_last = </p>
        <p>➥ MultiStepLastBaseline(label_index=column_indices['traffic_volume'])</p>
        <p>ms_baseline_last.compile(loss=MeanSquaredError(), </p>
        <p>➥ metrics=[MeanAbsoluteError()])</p>
        <p>ms_val_performance = {}</p>
        <p>ms_performance = {}</p>
        <p><a id="calibre_link-249"></a><img class="calibre2" src="images/000093.png" alt="Image 109" /></p>
        <p><b>264</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>ms_val_performance['Baseline - Last'] = </p>
        <p>➥ ms_baseline_last.evaluate(multi_window.val)</p>
        <p>ms_performance['Baseline - Last'] = </p>
        <p>➥ ms_baseline_last.evaluate(multi_window.test, verbose=0)</p>
        <p>We can now visualize the predictions using the plot method of DataWindow. The result is
            shown in figure 13.10. </p>
        <p>multi_window.plot(ms_baseline_last)</p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.6</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 13.10</p>
        <p>Predicting the last known value for the next 24 timesteps. We can see that the predictions,
            shown as crosses, correspond to the last value of the input sequence, so our baseline behaves as
            expected.
        </p>
        <p>Again, we can optionally print the baseline's MAE. From figure 13.10, we can expect it to be
            fairly high, since there is a large discrepancy between the labels and the predictions. </p>
        <p>print(ms_performance['Baseline - Last'][1])</p>
        <p>This gives an MAE of 0.347. Now let's see if we can build a better baseline by simply
            repeating the input sequence. </p>
        <p>REPEATING THE INPUT SEQUENCE</p>
        <p>Let's implement a second baseline for multi-step models, which simply returns the input
            sequence. This means that the prediction for the next 24 hours will simply be the last known 24 hours of
            data. This is implemented through the RepeatBaseline class. </p>
        <p><a id="calibre_link-420"></a><img class="calibre2" src="images/000095.png" alt="Image 110" /></p>
        <p> <i><b>13.2</b></i></p>
        <p> <i><b>Applying baseline models</b></i></p>
        <p><b>265</b></p>
        <p>class RepeatBaseline(Model):</p>
        <p>def __init__(self, label_index=None):</p>
        <p>super().__init__()</p>
        <p>self.label_index = label_index</p>
        <p><b>Return the input </b></p>
        <p></p>
        <p><b>sequence for the </b></p>
        <p>def call(self, inputs):</p>
        <p><b>given target column. </b></p>
        <p>return inputs[:, :, self.label_index:] </p>
        <p>Now we can initialize the baseline model and generate predictions. Note that the loss
            function and evaluation metric remain the same. </p>
        <p>ms_baseline_repeat = </p>
        <p>➥ RepeatBaseline(label_index=column_indices['traffic_volume'])</p>
        <p>ms_baseline_repeat.compile(loss=MeanSquaredError(), </p>
        <p>➥ metrics=[MeanAbsoluteError()])</p>
        <p>ms_val_performance['Baseline - Repeat'] = </p>
        <p>➥ ms_baseline_repeat.evaluate(multi_window.val)</p>
        <p>ms_performance['Baseline - Repeat'] = </p>
        <p>➥ ms_baseline_repeat.evaluate(multi_window.test, verbose=0)</p>
        <p>Next we can visualize the predictions. The result is shown in figure 13.11. </p>
        <p>This baseline performs well. This is to be expected, since we identified daily seasonality
            in the previous chapter. This baseline is the equivalent to predicting the last known season. </p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>0.6</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 13.11</p>
        <p>Repeating the input sequence as the predictions. You'll see that the predictions
            (represented as crosses) match exactly the input sequence. You'll also notice that many predictions
            overlap
            the labels, which indicates that this baseline performs quite well. </p>
        <p><a id="calibre_link-115"></a><b>266</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Again, we can print the MAE on the test set to verify that we indeed have a better baseline
            than simply predicting the last known value. </p>
        <p>print(ms_performance['Baseline - Repeat'][1])</p>
        <p>This gives an MAE of 0.341, which is lower than the MAE obtained by predicting the last
            known value. We have therefore successfully built a better baseline. </p>
        <p> <i><b>13.2.3 Multi-output baseline model</b></i></p>
        <p>The final type of model we'll cover is the multi-output model. In this situation, we wish to
            predict the traffic volume and the temperature for the next timestep using a single input data point.
            Essentially, we're applying the single-step model on both the traffic volume and temperature, making it
            a
            multi-output model. </p>
        <p>Again, we'll start off by defining the window of data, but here we'll define two windows:
            one for training and the other for visualization. Since the model takes in one data point and outputs
            one
            prediction, we want to initialize a wide window of data to visualize many predictions over many
            timesteps.
        </p>
        <p>mo_single_step_window = DataWindow(input_width=1, label_width=1, shift=1, </p>
        <p>➥ label_columns=['temp','traffic_volume']) </p>
        <p>mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, </p>
        <p>➥ label_columns=['temp','traffic_volume'])</p>
        <p><b>Notice that we pass in both temp and traffic_volume, as </b></p>
        <p><b>those are our two targets for the multi-output model. </b></p>
        <p>Then we'll use the Baseline class that we defined for the single-step model. Recall that
            this class can output the last known value for a list of targets. </p>
        <p>Listing 13.5</p>
        <p>Class to return the input data as a prediction</p>
        <p>class Baseline(Model):</p>
        <p>def __init__(self, label_index=None):</p>
        <p>super().__init__()</p>
        <p>self.label_index = label_index</p>
        <p><b>If no target is specified, we </b></p>
        <p><b>return all columns. This is useful </b></p>
        <p></p>
        <p><b>for multi-output models where </b></p>
        <p>def call(self, inputs):</p>
        <p><b>all columns are to be predicted. </b></p>
        <p>if self.label_index is None: </p>
        <p>return inputs</p>
        <p></p>
        <p>elif isinstance(self.label_index, list): </p>
        <p><b>If we specify a list of </b></p>
        <p>tensors = []</p>
        <p><b>targets, it will return only </b></p>
        <p>for index in self.label_index:</p>
        <p><b>these specified columns. </b></p>
        <p>result = inputs[:, :, index]</p>
        <p><b>Again, this is used for </b></p>
        <p>result = result[:, :, tf.newaxis]</p>
        <p><b>multi-output models. </b></p>
        <p>tensors.append(result)</p>
        <p>return tf.concat(tensors, axis=-1)</p>
        <p>result = inputs[:, :, self.label_index] </p>
        <p><b>Return the input for a </b></p>
        <p>return result[:,:,tf.newaxis]</p>
        <p><b>given target variable. </b></p>
        <p><a id="calibre_link-421"></a><img class="calibre2" src="images/000006.png" alt="Image 111" /></p>
        <p> <i><b>13.2</b></i></p>
        <p> <i><b>Applying baseline models</b></i></p>
        <p><b>267</b></p>
        <p>In the case of the multi-output model, we must simply pass the indexes of the temp and
            traffic_volume columns to output the last known value for the respective variables as a prediction. </p>
        <p>print(column_indices['traffic_volume']) </p>
        <p><b>Prints out 2</b></p>
        <p>print(column_indices['temp']) </p>
        <p><b>Prints out 0</b></p>
        <p>mo_baseline_last = Baseline(label_index=[0, 2])</p>
        <p>With the baseline initialized with our two target variables, we can now compile the model
            and evaluate it. </p>
        <p>mo_val_performance = {}</p>
        <p>mo_performance = {}</p>
        <p>mo_val_performance['Baseline - Last'] = </p>
        <p>➥ mo_baseline_last.evaluate(mo_wide_window.val)</p>
        <p>mo_performance['Baseline - Last'] = </p>
        <p>➥ mo_baseline_last.evaluate(mo_wide_window.test, verbose=0)</p>
        <p>Finally, we can visualize the predictions against the actual values. By default, our plot
            method will show the traffic volume on the <i>y</i>-axis, allowing us to quickly
            display
            one of our targets, as shown in figure 13.12. </p>
        <p>mo_wide_window.plot(mo_baseline_last)</p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.6</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>0.0</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 13.12</p>
        <p>Predicting the last known value for traffic volume</p>
        <p><a id="calibre_link-116"></a><img class="calibre2" src="images/000015.png" alt="Image 112" /></p>
        <p><b>268</b></p>
        <p>CHAPTER 13</p>
        <p> <i><b>Data windowing and creating baselines for deep
                    learning</b></i></p>
        <p>Figure 13.12 does not show anything surprising, as we already saw these results when we
            built a single-step baseline model. The particularity of the multi-output model is that we also have
            predictions for the temperature. Of course, we can also visualize the predictions for the temperature by
            specifying the target in the plot method. The result is shown in figure 13.13. </p>
        <p>mo_wide_window.plot(model=mo_baseline_last, plot_col='temp')</p>
        <p>0.65</p>
        <p>Inputs</p>
        <p>0.60</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>0.55</p>
        <p>0.50</p>
        <p>temp [scaled]</p>
        <p>0.45</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.75</p>
        <p>0.70</p>
        <p>0.65</p>
        <p>temp [scaled]</p>
        <p>0.60</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.95</p>
        <p>0.90</p>
        <p>0.85</p>
        <p>temp [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 13.13</p>
        <p>Predicting the last known value for the temperature. The predictions (crosses) are equal to
            the previous data point, so our baseline model behaves as expected. </p>
        <p>Again, we can print the MAE of our baseline model. </p>
        <p>print(mo_performance['Baseline - Last'])</p>
        <p>We obtain an MAE of 0.047 on the test set. In the next chapter, we'll start building more
            complex models, and they should result in a lower MAE, as they will be trained to fit the data. </p>
        <p> <i><b>13.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we covered the crucial step of creating data windows, which will allow us
            to quickly build any type of model. We then proceeded to build baseline models for</p>
        <p><a id="calibre_link-117"></a> <i><b>13.4</b></i></p>
        <p> <i><b>Summary</b></i></p>
        <p><b>269</b></p>
        <p>each type of model, so that we have benchmarks we can compare to when we build</p>
        <p>our more complex models in later chapters. </p>
        <p>Of course, building baseline models is not an application of deep learning just yet. </p>
        <p>In the next chapter, we will implement linear models and deep neural networks, and see if
            those models are already more performant than the simple baselines. </p>
        <p> <i><b>13.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>In the previous chapter, as an exercise, we prepared the air pollution dataset for deep
            learning modeling. Now we'll use the training set, validation set, and test set to build baseline models
            and
            evaluate them. </p>
        <p>For each type of model, follow the steps outlined. Recall that the target for the
            single-step and multi-step model is the concentration of NO2, and the targets for the multi-output model
            are
            the concentration of NO2 and temperature. The complete solution is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">tree/master/CH13%26CH14</a>.
        </p>
        <p>1</p>
        <p>For the single-step model</p>
        <p>a</p>
        <p>Build a baseline model that predicts the last known value. </p>
        <p>b</p>
        <p>Plot it. </p>
        <p>c</p>
        <p>Evaluate its performance using the mean absolute error (MAE) and store it</p>
        <p>for comparison in a dictionary. </p>
        <p>2</p>
        <p>For the multi-step model</p>
        <p>a</p>
        <p>Build a baseline that predicts the last known value over a horizon of 24 hours. </p>
        <p>b</p>
        <p>Build a baseline model that repeats the last 24 hours. </p>
        <p>c</p>
        <p>Plot the predictions of both models. </p>
        <p>d</p>
        <p>Evaluate both models using the MAE and store their performance. </p>
        <p>3</p>
        <p>For the multi-output model</p>
        <p>a</p>
        <p>Build a baseline model that predicts the last known value. </p>
        <p>b</p>
        <p>Plot it. </p>
        <p>c</p>
        <p>Evaluate its performance using the MAE and store it for comparison in a</p>
        <p>dictionary. </p>
        <p> <i><b>Summary</b></i></p>
        <p> Data windowing is essential in deep learning to format the data as inputs and</p>
        <p>labels for the model. </p>
        <p> The DataWindow class can easily be used in any situation and can be extended to your
            liking. Make use of it in your own projects. </p>
        <p> Deep learning models require a loss function and an evaluation metric. In our</p>
        <p>case, we chose the mean squared error (MSE) as the loss function, because it</p>
        <p>penalizes large errors and tends to yield better-fit models. The evaluation met-</p>
        <p>ric is the mean absolute error (MAE), chosen for its ease of interpretation. </p>
        <p><a id="calibre_link-17"></a> <i>Baby steps with</i></p>
        <p> <i>deep learning</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Implementing linear models</p>
        <p> Enacting deep neural networks</p>
        <p>In the last chapter, we implemented the DataWindow class, which allows us to quickly create
            windows of data for building single-step models, multi-step models, and multi-output models. With this
            crucial component in place, we then developed the baseline models that will serve as benchmarks for our
            more
            complex models, </p>
        <p>which we'll start building in this chapter. </p>
        <p>Specifically, we'll implement linear models and deep neural networks. A <i>linear</i> <i
                class="calibre3">model</i> is a special case of a neural network, where
            there is no hidden layer. This model simply calculates weights for each input variable in order to
            output a
            prediction for the target. In contrast, a <i>deep neural network</i> has at least one
            hidden layer, allowing us to start modeling nonlinear relationships between the features and the target,
            usually resulting in better forecasts. </p>
        <p>In this chapter, we'll continue the work we started in chapter 13. I recommend</p>
        <p>that you continue coding in the same notebook or Python scripts as in the last chapter, so
            that you can compare the performance of these linear models and deep neural networks to that of the
            baseline
            models from chapter 13. We'll also keep <b>270</b></p>
        <p><a id="calibre_link-118"></a> <i><b>14.1</b></i></p>
        <p> <i><b>Implementing a linear model</b></i></p>
        <p><b>271</b></p>
        <p>working with the same dataset as previously, and our target variable will remain the traffic
            volume for both the single-step and multi-step models. For the multi-output model, we'll keep the
            temperature and traffic volume as our targets. </p>
        <p> <i><b>14.1</b></i></p>
        <p> <i><b>Implementing a linear model</b></i></p>
        <p>A <i>linear model</i> is the simplest architecture we can implement in deep
            learning. In fact, we might argue that it is not deep learning at all, since the model has no hidden
            layer.
        </p>
        <p>Each input feature is simply given a weight, and they are combined to output a prediction
            for the target, just like in a traditional linear regression. </p>
        <p>Let's consider a single-step model as an example. Recall that we have the following features
            in our dataset: temperature, cloud coverage, traffic volume, and day_sin and day_cos, which encode the
            time
            of day as numerical values. A linear model simply takes all the features, calculates a weight for each
            of
            them, and sums them to output a prediction for the next timestep. This process is illustrated in figure
            14.1. </p>
        <p>Cloud coverage t</p>
        <p> <i>w</i> 1</p>
        <p>Temperature t</p>
        <p> <i>w</i> 2</p>
        <p> <i>w</i> 3</p>
        <p>Traffic volume t</p>
        <p>Traffic volume t +1</p>
        <p> <i>w</i> 4</p>
        <p>day_sin t</p>
        <p> <i>w</i> 5</p>
        <p>day_cos t</p>
        <p>Figure 14.1</p>
        <p>An example of a linear model as a single-step model. Each </p>
        <p>feature at time <i><b>t</b></i> is assigned a weight ( <i class="calibre3"><b class="calibre4">w</b></i> 1 to
            <i><b>w</b></i>
            5). They are then summed to calculate an output for the traffic volume at the next timestep, <i
                class="calibre3"><b> t</b></i>+1. </p>
        <p>This is similar to a linear regression. </p>
        <p>The model in figure 14.1 can be mathematically expressed as equation 14.1, where <i>x</i> 1
            is cloud coverage, <i>x</i> 2 is temperature, <i>x</i> 3 is traffic
            volume, <i>x</i> 4 is day_sin, and <i>x</i> 5 is day_cos. </p>
        <p>traffic volume <i>t</i>+1 = <i>w</i> 1 <i>x</i> 1, <i class="calibre3">t</i> + <i>w</i> 2 <i>x</i> 2, <i
                class="calibre3">t</i> + <i>w</i> 3 <i>x</i> 3, <i>t
            </i>+ <i>w</i> 4 <i>x</i> 4, <i>t</i> + <i class="calibre3">w</i> 5 <i>x</i> 5, <i>t</i> Equation 14.1</p>
        <p>We can easily recognize equation 14.1 as being a simple multivariate linear regression. </p>
        <p>During training, the model tries multiple values for <i>w</i> 1 to <i>w</i>
            5 in order to minimize the mean squared error (MSE) between the prediction and
            actual value of the traffic volume at the next timestep. </p>
        <p><a id="calibre_link-119"></a><b>272</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>Now that you understand the concept of a linear model in deep learning, let's implement it
            as a single-step model, multi-step model, and multi-output model. </p>
        <p> <i><b>14.1.1 Implementing a single-step linear
                    model</b></i></p>
        <p>A single-step linear model is one of the simplest models to implement, as it is exactly as
            described in figure 14.1 and equation 14.1. We simply take all the inputs, assign a weight to each, take
            the
            sum, and generate a prediction. Remember that we are using the traffic volume as a target. </p>
        <p>Assuming that you are working in the same notebook or Python script as in the</p>
        <p>last chapter, you should have access to the single_step_window for training and wide_window
            for plotting. Recall also that the performance of the baseline is stored in val_performance and
            performance.
        </p>
        <p>Unlike a baseline model, a linear model actually requires training. Thus, we'll define a
            compile_and_fit function that configures the model for training and then fits the model on the data, as
            shown in the following listing. </p>
        <p>NOTE</p>
        <p>You can consult the source code for this chapter on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">https://</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">26CH14</a>.
        </p>
        <p>Listing 14.1</p>
        <p>Function to configure a deep learning model and fit it on data</p>
        <p><b>The function takes a model and a window of data from the DataWindow </b>
        </p>
        <p><b>The validation loss is</b></p>
        <p><b>class. The patience is the number of epochs after which the model should
            </b></p>
        <p><b>tracked to determine</b></p>
        <p><b>stop training if the validation loss does not improve; max_epochs sets a
            </b></p>
        <p><b>if we should apply</b></p>
        <p><b>maximum number of epochs to train the model. </b></p>
        <p><b>early stopping or not. </b></p>
        <p>def compile_and_fit(model, window, patience=3, max_epochs=50): </p>
        <p>early_stopping = EarlyStopping(monitor='val_loss', </p>
        <p><b>The MSE</b></p>
        <p>patience=patience, </p>
        <p><b>Early stopping </b></p>
        <p><b>is used as</b></p>
        <p>mode='min')</p>
        <p><b>occurs if 3 </b></p>
        <p><b>the loss</b></p>
        <p></p>
        <p><b>consecutive epochs </b></p>
        <p><b>function. </b></p>
        <p>model.compile(loss=MeanSquaredError(), </p>
        <p><b>do not decrease the </b></p>
        <p>optimizer=Adam(), </p>
        <p><b>validation loss, as </b></p>
        <p><b>The model</b></p>
        <p>metrics=[MeanAbsoluteError()]) </p>
        <p><b>set by the patience </b></p>
        <p><b>is fit on the</b></p>
        <p></p>
        <p><b>parameter. </b></p>
        <p><b>training set. </b></p>
        <p>history = model.fit(window.train, </p>
        <p>epochs=max_epochs, </p>
        <p><b>The MAE is used as an </b></p>
        <p>validation_data=window.val, </p>
        <p><b>error metric. This is </b></p>
        <p>callbacks=[early_stopping]) </p>
        <p><b>how we compare the </b></p>
        <p></p>
        <p><b>performance of our </b></p>
        <p>return history</p>
        <p><b>models. A lower MAE </b></p>
        <p><b>early_stopping is passed as a</b></p>
        <p><b>means a better model. </b></p>
        <p><b>The model can train for </b></p>
        <p><b>callback. If the validation</b></p>
        <p><b>at most 50 epochs, as </b></p>
        <p><b>loss does not decrease after</b></p>
        <p><b>We use the validation </b></p>
        <p><b>set by the max_epochs </b></p>
        <p><b>3 consecutive epochs, the</b></p>
        <p><b>set to calculate the </b></p>
        <p><b>parameter. </b></p>
        <p><b>model stops training. This</b></p>
        <p><b>validation loss. </b></p>
        <p><b>avoids overfitting. </b></p>
        <p>This piece of code will be reused throughout the deep learning chapters, so it's important
            to understand what is happening. The compile_and_fit function takes in</p>
        <p><a id="calibre_link-229"></a> <i><b>14.1</b></i></p>
        <p> <i><b>Implementing a linear model</b></i></p>
        <p><b>273</b></p>
        <p>a deep learning model, a window of data from the DataWindow class, the patience parameter,
            and the max_epochs parameter. The patience parameter is used in the early_stopping function, which
            allows us
            to stop the model from training if there are no improvements in the validation loss, as specified by the
            monitor parameter. That way, we avoid useless training time and overfitting. </p>
        <p>Then the model is compiled. In Keras, this simply configures the model to specify the loss
            function to be used, the optimizer, and metrics of evaluation. In our case, we'll use the MSE as the
            loss
            function because the error is squared, meaning that the model is heavily penalized for large differences
            between the predicted and actual values. We'll use the Adam optimizer because it is a fast and efficient
            optimizer. Finally, we'll use the MAE as an evaluation metric to compare the performance of our models
            because we used it to evaluate our baseline models in the previous chapter, and it is easy to interpret.
        </p>
        <p>The model is then fit on the training data for up to 50 epochs, as set by the max_</p>
        <p>epochs parameter. The validation is performed on the validation set, and we pass in
            early_stopping as a callback. That way, Keras will apply early stopping if it sees that the validation
            loss
            has not decreased after 3 consecutive epochs. </p>
        <p>With compile_and_fit in place, we can move on to actually building our linear model. We'll
            use the Sequential model from Keras, as it allows us to stack different layers. Since we are building a
            linear model here, we only have one layer&mdash;a Dense layer, which is the most basic layer in deep
            learning. We'll specify the number of units as 1, since the model must output only one value: the
            prediction
            for traffic volume at the next timestep. </p>
        <p>linear = Sequential([</p>
        <p>Dense(units=1)</p>
        <p>])</p>
        <p>Clearly, Keras makes it very easy to build models. With this step complete, we can then
            train the model using compile_and_fit and store the performance to later compare it to the baseline.
        </p>
        <p>history = compile_and_fit(linear, single_step_window)</p>
        <p>val_performance['Linear'] = linear.evaluate(single_step_window.val)</p>
        <p>performance['Linear'] = linear.evaluate(single_step_window.test, verbose=0)</p>
        <p>Optionally, we can visualize the predictions of our linear model using the plot method of
            the wide_window. The result is shown in figure 14.2. </p>
        <p>wide_window.plot(linear)</p>
        <p>Our model makes fairly good predictions, as we can observe some overlap between the
            forecasts and the actual values. We will wait until the end of the chapter to compare</p>
        <p><a id="calibre_link-120"></a><img class="calibre2" src="images/000078.png" alt="Image 113" /></p>
        <p><b>274</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.6</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>lume [scaled]</p>
        <p>0.2</p>
        <p>c_voffi</p>
        <p>tra</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 14.2</p>
        <p>Predictions of traffic volume using the linear model as a single-step model. The predictions
            (shown as crosses) are fairly accurate, with some predictions overlapping the actual values (shown as
            squares). </p>
        <p>the performance of our models to the baselines. For now, let's move on to implementing the
            multi-step linear and multi-output linear models. </p>
        <p> <i><b>14.1.2 Implementing a multi-step linear
                    model</b></i></p>
        <p>Our single-step linear model is built, and we can now extend it to a multi-step linear
            model. Recall that in the multi-step situation, we wish to predict the next 24 hours of data using an
            input
            window of 24 hours of data. Our target remains the traffic volume. </p>
        <p>This model will greatly resemble the single-step linear model, but this time we'll use 24
            hours of input and output 24 hours of predictions. The multi-step linear model is illustrated in figure
            14.3. As you can see, the model takes in 24 hours of each feature, combines them in a single layer, and
            outputs a tensor containing the forecast for the next 24 hours. </p>
        <p>Implementing the model is easy, as our model only contains a single Dense layer. </p>
        <p>We can optionally initialize the weights to 0, which makes the training procedure slightly
            faster. We then compile and fit the model before storing its evaluation metrics in ms_val_performance
            and
            ms_performance. </p>
        <p><a id="calibre_link-121"></a> <i><b>14.1</b></i></p>
        <p> <i><b>Implementing a linear model</b></i></p>
        <p><b>275</b></p>
        <p>Cloud coverage</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>... t </i>= 22 <i>t </i>= 23</p>
        <p>Temperature</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>... t </i>= 22 <i>t </i>= 23</p>
        <p>Traffic volume</p>
        <p>Traffic volume</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>... t </i>= 22 <i>t </i>= 23</p>
        <p> <i>t </i>= 24 <i>t </i>= 25 <i>... t
            </i>= 46 <i>t </i>= 47</p>
        <p>day_sin</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>... t </i>= 22 <i>t </i>= 23</p>
        <p>day_cos</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>... t </i>= 22 <i>t </i>= 23</p>
        <p>Figure 14.3</p>
        <p>The multi-step linear model. We'll take 24 hours of each feature, combine them in a single
            layer, and immediately output predictions for the next 24 hours. </p>
        <p>ms_linear = Sequential([</p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros) </p>
        <p><b>Initializing the </b></p>
        <p>])</p>
        <p><b>weights to 0 </b></p>
        <p><b>makes training </b></p>
        <p>history = compile_and_fit(ms_linear, multi_window)</p>
        <p><b>slightly faster. </b></p>
        <p>ms_val_performance['Linear'] = ms_linear.evaluate(multi_window.val)</p>
        <p>ms_performance['Linear'] = ms_linear.evaluate(multi_window.test, verbose=0)</p>
        <p>We have just built a multi-step linear model. You might feel underwhelmed, since the code is
            almost identical to the single-step linear model. This is due to our work building the DataWindow class
            and
            properly windowing our data. With that step done, building models becomes extremely easy. </p>
        <p>Next we'll implement a multi-output linear model. </p>
        <p> <i><b>14.1.3 Implementing a multi-output linear
                    model</b></i></p>
        <p>The multi-output linear model will return predictions for the traffic volume and the
            temperature. The input is the present timestep, and the predictions are for the next timestep. </p>
        <p>The model's architecture is shown in figure 14.4. There, you can see that our multi-output
            linear model will take all the features at <i>t</i> = 0, combine them in a single
            layer,
            and output both the temperature and traffic volume at the next timestep. </p>
        <p><a id="calibre_link-122"></a><b>276</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>Cloud coverage <i>t </i>= 0</p>
        <p>Temperature</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1 Traffic volume</p>
        <p>Traffic volume <i>t </i>= 0</p>
        <p> <i>t </i>= 1 Temperature</p>
        <p>day_sin <i>t </i>= 0</p>
        <p>day_cos <i>t </i>= 0</p>
        <p>Figure 14.4</p>
        <p>A multi-output linear model. In this case, the model takes the </p>
        <p>present timestep of all features and produces a forecast for the temperature </p>
        <p>and traffic volume at the next timestep. </p>
        <p>Up to this point, we have only predicted the traffic volume, meaning that we had only one
            target, so we used the layer Dense(units=1). In this case, since we must output a prediction for two
            targets, our layer will be Dense(units=2). As before, we'll train the model and store its performance to
            compare it later to the baseline and deep neural network. </p>
        <p>mo_linear = Sequential([</p>
        <p><b>We set units equal to the number </b></p>
        <p>Dense(units=2) </p>
        <p><b>of targets we are predicting in </b></p>
        <p>])</p>
        <p><b>the output layer. </b></p>
        <p>history = compile_and_fit(mo_linear, mo_single_step_window)</p>
        <p>mo_val_performance['Linear'] = </p>
        <p>➥ mo_linear.evaluate(mo_single_step_window.val)</p>
        <p>mo_performance['Linear'] = mo_linear.evaluate(mo_single_step_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>Again, you can see how easy it is to build a deep learning model in Keras, especially when
            we have the proper data window as input. </p>
        <p>With our single-step, multi-step, and multi-output linear models done, we can now move on to
            implementing a more complex architecture: a deep neural network. </p>
        <p> <i><b>14.2</b></i></p>
        <p> <i><b>Implementing a deep neural network</b></i></p>
        <p>With our three types of linear models implemented, it is time to move on to deep neural
            networks. It has been empirically shown that adding hidden layers in neural networks helps achieve
            better
            results. Furthermore, we'll introduce a nonlinear activation function to capture nonlinear relationships
            in
            the data. </p>
        <p><a id="calibre_link-194"></a> <i><b>14.2</b></i></p>
        <p> <i><b>Implementing a deep neural network</b></i></p>
        <p><b>277</b></p>
        <p>Linear models have no hidden layers; the model had an input layer and an output</p>
        <p>layer. In a deep neural network (DNN), we'll add more layers between the input and output
            layers, called <i>hidden layers</i>. This difference in architecture is highlighted in
            figure 14.5. </p>
        <p><b>Deep neural network</b></p>
        <p><b>Linear model</b></p>
        <p>Prediction</p>
        <p>Prediction</p>
        <p>Input layer</p>
        <p>Output</p>
        <p>layer</p>
        <p>Input layer</p>
        <p>Output</p>
        <p>layer</p>
        <p>Hidden</p>
        <p>Hidden</p>
        <p>layer</p>
        <p>layer</p>
        <p>Figure 14.5</p>
        <p>Comparing a linear model to a deep neural network. In the linear model, the input layer is
            directly connected to an output layer that returns a prediction. Therefore, only a linear relationship
            is
            derived. A deep neural network contains hidden layers. These layers allow it to model nonlinear
            relationships between inputs and predictions, generally resulting in better models. </p>
        <p>The idea behind adding layers to the network is that it gives the model more opportunities
            to learn, which usually results in the model generalizing better on unseen data, thus improving its
            performance. Of course, with added layers, the model necessarily trains for a longer time and is thus
            supposed to learn better. </p>
        <p>Each circle in a hidden layer represents a neuron, and each neuron has an activation
            function. The number of neurons is equal to the number of units that is passed as an argument in the
            Dense
            layer in Keras. Usually we set the number of units, or neurons, as a power of 2, as it is more
            computationally efficient&mdash;calculations in the CPU</p>
        <p>and GPU happen in batch sizes that are also powers of 2. </p>
        <p>Before implementing a DNN, we need to address the <i>activation
                function</i> in each neuron of the hidden layers. The activation function defines the output of each
            neuron based on the input. Therefore, if we wish to model nonlinear relationships, we need to use a
            nonlinear activation function. </p>
        <p><a id="calibre_link-123"></a><b>278</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>Activation function</p>
        <p>The activation function is in each neuron of the neural network and is responsible for
            generating an output from the input data. </p>
        <p>If a linear activation function is used, the model will only model linear relationships.
        </p>
        <p>Therefore, to model nonlinear relationships in the data, we must use a nonlinear activation
            function. Examples of nonlinear activation functions are ReLU, softmax, or tanh. </p>
        <p>In our case, we'll use the Rectified Linear Unit (ReLU) activation function. This nonlinear
            activation function basically returns either the positive part of its input or 0, as defined by equation
            14.2. </p>
        <p>f( <i>x</i>) = <i>x</i>+ = max (0, <i>x</i>)</p>
        <p>Equation 14.2</p>
        <p>This activation function comes with many advantages, such as better gradient propa-gation,
            more efficient computation, and scale-invariance. For all those reasons, it is now the most widely used
            activation function in deep learning, and we'll use it when-ever we have a Dense layer that is a hidden
            layer. </p>
        <p>We are now ready to implement a deep neural network in Keras. </p>
        <p> <i><b>14.2.1 Implementing a deep neural network as a
                    single-step model</b></i></p>
        <p>We are now back to the single-step model, but this time we'll implement a deep neural
            network. The DNN takes in the features at the current timestep to output the prediction for traffic
            volume
            at the next timestep. </p>
        <p>The model still makes use of the Sequential model, as we'll stack Dense layers in order to
            build a deep neural network. In this case, we'll use two hidden layers with 64</p>
        <p>neurons each. As mentioned before, we'll specify the activation function to be ReLU. </p>
        <p>The last layer is the output layer, which in this case only returns one value representing
            the prediction for traffic volume. </p>
        <p><b>First hidden layer with 64 </b></p>
        <p><b>neurons. Specify the activation </b></p>
        <p>dense = Sequential([</p>
        <p><b>function to be ReLU. </b></p>
        <p>Dense(units=64, activation='relu'), </p>
        <p>Dense(units=64, activation='relu'), </p>
        <p>Dense(units=1) </p>
        <p><b>The output layer has only one neuron, </b></p>
        <p>])</p>
        <p><b>as we output only one value. </b></p>
        <p>With the model defined, we can now compile it, train it, and record its performance to
            compare it to the baseline and the linear model. </p>
        <p>history = compile_and_fit(dense, single_step_window)</p>
        <p>val_performance['Dense'] = dense.evaluate(single_step_window.val)</p>
        <p>performance['Dense'] = dense.evaluate(single_step_window.test, verbose=0)</p>
        <p><a id="calibre_link-422"></a><img class="calibre2" src="images/000109.png" alt="Image 114" /></p>
        <p> <i><b>14.2</b></i></p>
        <p> <i><b>Implementing a deep neural network</b></i></p>
        <p><b>279</b></p>
        <p>Of course, we can take a look at the model's predictions using the plot method, as shown in
            figure 14.6. Our deep neural network seems to be making quite accurate predictions. </p>
        <p>0.8</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.6</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>lume [scaled]</p>
        <p>0.2</p>
        <p>c_voffi</p>
        <p>tra</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 14.6</p>
        <p>Predicting the traffic volume using a deep neural network as a single-step model. Here even
            more predictions (shown as crosses) overlap with the actual values (shown as squares), suggesting that
            the
            model is making very accurate predictions. </p>
        <p>Let's compare the MAE of the DNN with the linear model and the baseline that we</p>
        <p>built in chapter 13. The result is shown in figure 14.7. </p>
        <p>mae_val = [v[1] for v in val_performance.values()]</p>
        <p>mae_test = [v[1] for v in performance.values()]</p>
        <p>x = np.arange(len(performance))</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.bar(x - 0.15, mae_val, width=0.25, color='black', edgecolor='black', </p>
        <p>➥ label='Validation')</p>
        <p>ax.bar(x + 0.15, mae_test, width=0.25, color='white', edgecolor='black', </p>
        <p>➥ hatch='/', label='Test')</p>
        <p>ax.set_ylabel('Mean absolute error')</p>
        <p>ax.set_xlabel('Models')</p>
        <p><a id="calibre_link-225"></a><img class="calibre2" src="images/000039.png" alt="Image 115" /></p>
        <p><b>280</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>for index, value in enumerate(mae_val):</p>
        <p>plt.text(x=index - 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p></p>
        <p>for index, value in enumerate(mae_test):</p>
        <p>plt.text(x=index + 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p>plt.ylim(0, 0.1)</p>
        <p>plt.xticks(ticks=x, labels=performance.keys())</p>
        <p>plt.legend(loc='best')</p>
        <p>plt.tight_layout()</p>
        <p>0.10</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.083</p>
        <p>0.081</p>
        <p>0.08</p>
        <p>0.065</p>
        <p>0.061</p>
        <p>0.06</p>
        <p>0.04</p>
        <p>Mean absolute error</p>
        <p>0.036</p>
        <p>0.032</p>
        <p>0.02</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>Models</p>
        <p>Figure 14.7</p>
        <p>The MAE for all of the single-step models so far. The linear model performs better than the
            baseline, which only predicts the last known value. The dense model outperforms both models, since it
            has
            the lowest MAE. </p>
        <p>In figure 14.7 the MAE is highest for the baseline. It decreases with the linear model and
            decreases again with the deep neural network. Thus, both models outperformed the baseline, with the deep
            neural network having the best performance. </p>
        <p><a id="calibre_link-124"></a> <i><b>14.2</b></i></p>
        <p> <i><b>Implementing a deep neural network</b></i></p>
        <p><b>281</b></p>
        <p> <i><b>14.2.2 Implementing a deep neural network as a
                    multi-step model</b></i></p>
        <p>Now let's implement a deep neural network as a multi-step model. In this case, we want to
            predict the next 24 hours of traffic volume based on the last 24 hours of recorded data. </p>
        <p>Again we'll use two hidden layers with 64 neurons each, and we'll use the ReLU</p>
        <p>activation function. Since we have a data window with 24 hours of input, the model will also
            output 24 hours of predictions; the output layer simply has one neuron because we are predicting traffic
            volume only. </p>
        <p>ms_dense = Sequential([</p>
        <p>Dense(64, activation='relu'), </p>
        <p>Dense(64, activation='relu'), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>Then we'll compile, train the model, and save its performance for comparison with the linear
            and baseline models. </p>
        <p>history = compile_and_fit(ms_dense, multi_window)</p>
        <p>ms_val_performance['Dense'] = ms_dense.evaluate(multi_window.val)</p>
        <p>ms_performance['Dense'] = ms_dense.evaluate(multi_window.test, verbose=0)</p>
        <p>Just like that, we have built a multi-step deep neural network model. Let's see which model
            performed best for the multi-step task. The result is shown in figure 14.8. </p>
        <p>ms_mae_val = [v[1] for v in ms_val_performance.values()]</p>
        <p>ms_mae_test = [v[1] for v in ms_performance.values()]</p>
        <p>x = np.arange(len(ms_performance))</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.bar(x - 0.15, ms_mae_val, width=0.25, color='black', edgecolor='black', </p>
        <p>➥ label='Validation')</p>
        <p>ax.bar(x + 0.15, ms_mae_test, width=0.25, color='white', edgecolor='black', </p>
        <p>➥ hatch='/', label='Test')</p>
        <p>ax.set_ylabel('Mean absolute error')</p>
        <p>ax.set_xlabel('Models')</p>
        <p>for index, value in enumerate(ms_mae_val):</p>
        <p>plt.text(x=index - 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p></p>
        <p>for index, value in enumerate(ms_mae_test):</p>
        <p>plt.text(x=index + 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p>plt.ylim(0, 0.4)</p>
        <p>plt.xticks(ticks=x, labels=ms_performance.keys())</p>
        <p><a id="calibre_link-125"></a><img class="calibre2" src="images/000050.png" alt="Image 116" /></p>
        <p><b>282</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>plt.legend(loc='best')</p>
        <p>plt.tight_layout()</p>
        <p>0.40</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.352</p>
        <p>0.347</p>
        <p>0.347</p>
        <p>0.35</p>
        <p>0.341</p>
        <p>0.30</p>
        <p>0.25</p>
        <p>0.20</p>
        <p>Mean absolute error 0.15</p>
        <p>0.10</p>
        <p>0.088</p>
        <p>0.076</p>
        <p>0.075</p>
        <p>0.061</p>
        <p>0.05</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Baseline - Repeat</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>Models</p>
        <p>Figure 14.8</p>
        <p>The MAE for all of the multi-step models so far. The linear model performs better than both
            baselines. The dense model outperforms all models. </p>
        <p>In figure 14.8 you'll see that the linear model and deep neural network both outperform the
            two baselines that we built for the multi-step task in chapter 13. Again, the deep neural network has
            the
            lowest MAE of all, meaning that it is the most performant model for now. </p>
        <p> <i><b>14.2.3 Implementing a deep neural network as a
                    multi-output model</b></i></p>
        <p>Finally, we'll implement a deep neural network as a multi-output model. In this case, we'll
            use the features at the present timestep to forecast both the traffic volume and temperature at the next
            timestep. </p>
        <p>As for the previous DNNs that we implemented, we'll use two hidden layers of 64</p>
        <p>neurons each. This time, because we are forecasting two targets, our output layer has two
            neurons or units. </p>
        <p><a id="calibre_link-223"></a> <i><b>14.2</b></i></p>
        <p> <i><b>Implementing a deep neural network</b></i></p>
        <p><b>283</b></p>
        <p>mo_dense = Sequential([</p>
        <p>Dense(units=64, activation='relu'), </p>
        <p>Dense(units=64, activation='relu'), </p>
        <p>Dense(units=2) </p>
        <p><b>The output layer has two neurons, </b></p>
        <p>])</p>
        <p><b>since we are forecasting two targets. </b></p>
        <p>Next we'll compile and fit the model and store its performance for comparison. </p>
        <p>history = compile_and_fit(mo_dense, mo_single_step_window)</p>
        <p>mo_val_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.val)</p>
        <p>mo_performance['Dense'] = mo_dense.evaluate(mo_single_step_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>Let's see which model performed best at the multi-output task. Note that the reported MAE is
            averaged for both targets. </p>
        <p>mo_mae_val = [v[1] for v in mo_val_performance.values()]</p>
        <p>mo_mae_test = [v[1] for v in mo_performance.values()]</p>
        <p>x = np.arange(len(mo_performance))</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.bar(x - 0.15, mo_mae_val, width=0.25, color='black', edgecolor='black', </p>
        <p>➥ label='Validation')</p>
        <p>ax.bar(x + 0.15, mo_mae_test, width=0.25, color='white', edgecolor='black', </p>
        <p>➥ hatch='/', label='Test')</p>
        <p>ax.set_ylabel('Mean absolute error')</p>
        <p>ax.set_xlabel('Models')</p>
        <p>for index, value in enumerate(mo_mae_val):</p>
        <p>plt.text(x=index - 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p></p>
        <p>for index, value in enumerate(mo_mae_test):</p>
        <p>plt.text(x=index + 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p>plt.ylim(0, 0.06)</p>
        <p>plt.xticks(ticks=x, labels=mo_performance.keys())</p>
        <p>plt.legend(loc='best')</p>
        <p>plt.tight_layout()</p>
        <p>As you can see in figure 14.9, our models outperform the baseline, with the deep learning
            model being the most performant. </p>
        <p></p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-126"></a><img class="calibre2" src="images/000083.png" alt="Image 117" /></p>
        <p><b>284</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p>0.06</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.048</p>
        <p>0.05</p>
        <p>0.047</p>
        <p>0.04</p>
        <p>0.038</p>
        <p>0.036</p>
        <p>0.03</p>
        <p>0.023</p>
        <p>Mean absolute error</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.01</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>Models</p>
        <p>Figure 14.9</p>
        <p>The MAE for all of the multi-output models built so far. Again, the baseline has the highest
            MAE, while the deep neural network achieves the lowest error metric. </p>
        <p> <i><b>14.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we implemented both linear models and deep neural networks to make
            single-step, multi-step, and multi-output predictions. In all cases, the deep neural network
            outperformed
            the other models. This is generally the case, as DNNs can map nonlinear relationships between the
            features
            and the targets, which generally leads to more accurate predictions. </p>
        <p>This chapter only brushed the surface of what deep learning can achieve in time</p>
        <p>series forecasting. In the next chapter, we'll explore a more complex architecture: the <i
                class="calibre3">long short-term memory</i> (LSTM). This architecture is widely used to process
            sequences of data. Since a time series is a sequence of points equally spaced in time, it makes sense to
            apply an LSTM for time series forecasting. We will then test whether the LSTM outperforms the DNN or
            not.
        </p>
        <p><a id="calibre_link-127"></a> <i><b>14.4</b></i></p>
        <p> <i><b>Summary</b></i></p>
        <p><b>285</b></p>
        <p> <i><b>14.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>In the last chapter, as an exercise, you built baseline models to forecast the concentration
            of NO2 and temperature. Now you'll build linear models and deep neural networks. The full solutions to
            these
            exercises are available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">https://github</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14">.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14.
            </a></p>
        <p>1</p>
        <p>For the single-step model:</p>
        <p>a</p>
        <p>Build a linear model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Measure its performance using the mean absolute error (MAE) and store it. </p>
        <p>d</p>
        <p>Build a deep neural network (DNN). </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p>f</p>
        <p>Measure its performance using the MAE and store it. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p>2</p>
        <p>For the multi-step model:</p>
        <p>a</p>
        <p>Build a linear model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Measure its performance using the MAE and store it. </p>
        <p>d</p>
        <p>Build a DNN. </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p>f</p>
        <p>Measure its performance using the MAE and store it. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p>3</p>
        <p>For the multi-output model:</p>
        <p>a</p>
        <p>Build a linear model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Measure its performance using the MAE and store it. </p>
        <p>d</p>
        <p>Build a DNN. </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p>f</p>
        <p>Measure its performance using the MAE and store it. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p>At any point, feel free to run your own experiments with the deep neural networks. </p>
        <p>Add layers, change the number of neurons, and see how those changes impact the performance
            of the model. </p>
        <p> <i><b>Summary</b></i></p>
        <p> A linear model is the simplest architecture in deep learning. It has an input layer and an
            output layer, with no activation function. </p>
        <p> A linear model can only derive linear relationships between the features and the target.
        </p>
        <p><a id="calibre_link-215"></a><b>286</b></p>
        <p>CHAPTER 14</p>
        <p> <i><b>Baby steps with deep learning</b></i></p>
        <p> A deep neural network (DNN) has hidden layers, which are layers between the</p>
        <p>input and output layers. Adding more layers usually improves the performance</p>
        <p>of the model, as it allows it more time to train and learn the data. </p>
        <p> To model nonlinear relationships from the data, you must use a nonlinear acti-</p>
        <p>vation function in the network. Examples of nonlinear activation functions are</p>
        <p>ReLU, softmax, tanh, sigmoid, etc. </p>
        <p> The number of neurons in a hidden layer is usually a power of 2, to make com-</p>
        <p>putation more efficient. </p>
        <p> The Rectified Linear Unit (ReLU) is a popular nonlinear activation function that does not
            vary with scale and allows for efficient model training. </p>
        <p><a id="calibre_link-18"></a> <i>Remembering</i></p>
        <p> <i>the past with LSTM</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the long short-term memory (LSTM) </p>
        <p>architecture</p>
        <p> Implementing an LSTM with Keras</p>
        <p>In the last chapter, we built our first models in deep learning, implementing both linear
            and deep neural network models. In the case of our dataset, we saw that both models outperformed the
            baselines we built in chapter 13, with the deep neural network being the best model for single-step,
            multi-step, and multi-output tasks. </p>
        <p>Now we'll explore a more advanced architecture called <i>long short-term
                memory</i> (LSTM), which is a particular case of a <i>recurrent neural network</i>
            (RNN). This type of neural network is used to process sequences of data, where the order matters. One
            common
            application of RNN and LSTM is in natural language processing. Words</p>
        <p>in a sentence have an order, and changing that order can completely change the</p>
        <p>meaning of a sentence. Thus, we often find this architecture behind text classification and
            text generation algorithms. </p>
        <p>Another situation where the order of data matters is time series. We know that</p>
        <p>time series are sequences of data equally spaced in time, and that their order cannot be
            changed. The data point observed at 9 a.m. must come before the data point <b>287</b>
        </p>
        <p><a id="calibre_link-246"></a><b>288</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>at 10 a.m. and after the data point at 8 a.m. Thus, it makes sense to apply the LSTM</p>
        <p>architecture for forecasting time series. </p>
        <p>In this chapter, we'll first explore the general architecture of a recurrent neural network,
            and then we'll dive deep into the LSTM architecture and examine its unique features and inner workings.
            Then
            we'll implement an LSTM using Keras to produce single-step, multi-step, and multi-output models. We'll
            finally compare the performance of LSTM against all the models we've built, from the baselines to the
            deep
            neural networks. </p>
        <p> <i><b>15.1</b></i></p>
        <p> <i><b>Exploring the recurrent neural network
                    (RNN)</b></i></p>
        <p>A recurrent neural network (RNN) is a deep learning architecture especially adapted to
            processing sequences of data. It denotes a set of networks that share a similar architecture: long
            short-term memory (LSTM) and gated recurrent unit (GRU) are subtypes of RNNs. In this chapter, we'll
            solely
            focus on the LSTM architecture. </p>
        <p>To understand the inner workings of an RNN, we'll start with figure 15.1, which</p>
        <p>shows a compact illustration of an RNN. Just like in a deep neural network (DNN), we have an
            input, denoted as <i>xt</i>, and an output, denoted as <i>yt</i>. Here
            <i>xt</i> is an element of a sequence. When it is fed to the RNN, it computes a hidden
            state, denoted as <i>ht</i>. This hidden state acts as memory. It is computed for each
            element of the sequence and fed back to the RNN as an input. That way, the network effectively uses past
            information computed for previous elements of the sequence to inform the output for the next element of
            the
            sequence. </p>
        <p> <i>yt</i></p>
        <p>RNN</p>
        <p>Figure 15.1</p>
        <p>A compact illustration of an RNN. It </p>
        <p>computes a hidden state <i><b>ht</b></i>, which is looped
            back in </p>
        <p>the network and combined with the next input of the </p>
        <p>sequence. This is how RNNs keep information from </p>
        <p> <i>xt</i></p>
        <p>past elements of a sequence and use them to process </p>
        <p>the next element of a sequence. </p>
        <p>Figure 15.2 shows an expanded illustration of an RNN. You can see how the hidden state is
            first computed at <i>t</i> = 0 and then is updated and passed on as each element of the
            sequence is processed. This is how the RNN effectively replicates the concept of memory and uses past
            information to produce a new output. </p>
        <p><a id="calibre_link-423"></a> <i><b>15.1</b></i></p>
        <p> <i><b>Exploring the recurrent neural network
                    (RNN)</b></i></p>
        <p><b>289</b></p>
        <p> <i>y</i> 0</p>
        <p> <i>y</i> 1</p>
        <p> <i>y</i> 2</p>
        <p> <i>yt</i></p>
        <p>RNN</p>
        <p>RNN</p>
        <p>RNN</p>
        <p>RNN</p>
        <p> <i>h</i> 0</p>
        <p> <i>h</i> 1</p>
        <p> <i>h</i> 2</p>
        <p> <i>x</i> 0</p>
        <p> <i>x</i> 1</p>
        <p> <i>x</i> 2</p>
        <p> <i>xt</i></p>
        <p>Figure 15.2</p>
        <p>Expanded illustration of an RNN. Here you can see how the hidden state is updated and passed
            on to the next element of the sequence as an input. </p>
        <p>Recurrent neural network</p>
        <p>A recurrent neural network (RNN) is especially adapted to processing sequences of data. It
            uses a hidden state that is fed back into the network so it can use past information as an input when
            processing the next element of a sequence. This is how it replicates the concept of memory. </p>
        <p>However, RNNs suffer from short-term memory, meaning that information from an early element
            in the sequence will stop having an impact further into the sequence. </p>
        <p>However, the basic RNNs that we have examined come with a drawback: they suffer</p>
        <p>from short-term memory due to the vanishing gradient. The gradient is simply the function
            that tells the network how to change the weights. If the change in gradient is large, the weights change
            by
            a large magnitude. On the other hand, if the change in gradient is small, the weights do not change
            significantly. The vanishing gradient problem refers to what happens when the change in gradient becomes
            very small, sometimes close to 0. This in turn means that the weights of the network do not get updated,
            and
            the network stops learning. </p>
        <p>In practice, this means the RNN forgets about past information that is far away in the
            sequence. It therefore suffers from a short-term memory. For example, if an RNN is processing 24 hours
            of
            hourly data, the points at hours 9, 10, and 11 might still impact the output at hour 12, but any point
            prior
            to hour 9 might not contrib-ute at all to the network's learning, because the gradient gets very small
            for
            those early data points. </p>
        <p><a id="calibre_link-128"></a><b>290</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>Therefore, we must find a way to retain the importance of past information in our network.
            This brings us to the long short-term memory (LSTM) architecture, which uses the cell state as an
            additional
            way of keeping past information in memory for a long time. </p>
        <p> <i><b>15.2</b></i></p>
        <p> <i><b>Examining the LSTM architecture</b></i></p>
        <p>The <i>long short-term memory</i> (LSTM) architecture adds a cell state to
            the RNN architecture to avoid the vanishing gradient problem, where past information ceases to impact
            the
            learning of the network. This allows the network to keep past information in memory for a longer time.
        </p>
        <p>The LSTM architecture is shown in figure 15.3, and you can see that it is more complex than
            the basic RNN architecture. You'll notice the addition of the cell state, denoted as <i
                class="calibre3">C</i>. This cell state is what allows the network to keep past information in the
            network for a longer time, thus resolving the vanishing gradient problem. Note that this is unique to
            the
            LSTM architecture. We still have an element of a sequence being processed, shown as <i
                class="calibre3">xt</i>, and a hidden state is also computed, denoted as <i>ht</i>.
            In
            this case, both the cell state <i>Ct</i> and the hidden <i>ht</i> are
            passed on to the next element of the sequence, making sure that past information is used as an input for
            the
            next element in the sequence being processed. </p>
        <p> <i>ht</i></p>
        <p> <i>Ct</i>&ndash;1</p>
        <p> <i>Ct</i></p>
        <p>Forget</p>
        <p>gate</p>
        <p>Output</p>
        <p> <i>ht</i></p>
        <p>Input</p>
        <p>gate</p>
        <p>gate</p>
        <p> <i>ht</i>&ndash;1</p>
        <p> <i>xt</i></p>
        <p>Figure 15.3</p>
        <p>The architecture of an LSTM neuron. The cell state is denoted </p>
        <p>as <i><b>C</b></i>, while the input is <i><b>x</b></i> and the hidden state
            is <i><b>h</b></i>. </p>
        <p>You'll also notice the presence of three gates: the forget gate, the input gate, and the
            output gate. Each has its specific function in the LSTM, so let's explore each one in detail. </p>
        <p><a id="calibre_link-129"></a> <i><b>15.2</b></i></p>
        <p> <i><b>Examining the LSTM architecture</b></i></p>
        <p><b>291</b></p>
        <p>Long short-term memory</p>
        <p>Long short-term memory (LSTM) is a deep learning architecture that is a subtype of RNN. LSTM
            addresses the problem of short-term memory by adding the cell state. This allows for past information to
            flow through the network for a longer period of time, meaning that the network still carries information
            from early values in the sequence. </p>
        <p>The LSTM is made up of three gates: </p>
        <p> The <i>forget gate</i> determines what information from past steps is
            still relevant. </p>
        <p> The <i>input gate</i> determines what information from the current step
            is relevant. </p>
        <p> The <i>output gate</i> determines what information is passed on to the
            next element of the sequence or as a result to the output layer. </p>
        <p> <i><b>15.2.1 The forget gate</b></i></p>
        <p>The <i>forget gate</i> is the first gate in an LSTM cell. Its role is to
            determine what information, from both the past values and the current value of the sequence, should be
            forgotten or kept in the network. </p>
        <p> <i>C</i></p>
        <p>To the input gate</p>
        <p> <i>t-1</i></p>
        <p> <i>C' </i></p>
        <p>×</p>
        <p> <i>t-1</i></p>
        <p>Figure 15.4</p>
        <p>The forget gate in an LSTM </p>
        <p>sigmoid</p>
        <p>cell. The present element of a sequence, </p>
        <p> <i><b>xt</b></i>, and past information, <i><b>ht</b></i>&ndash;1, are
            first </p>
        <p>combined. They are duplicated, and one is </p>
        <p>sent to the input gate while the other goes </p>
        <p>through the sigmoid activation function. </p>
        <p> <i>h</i></p>
        <p>To the input gate</p>
        <p> <i>t-1</i></p>
        <p>[ <i>h</i></p>
        <p>The sigmoid outputs a value between 0 and </p>
        <p> <i>t-1</i> + <i>xt</i>]</p>
        <p>1, and if the output is close to 0, this means </p>
        <p>that information must be forgotten. If it </p>
        <p>is close to 1, the information is kept. The </p>
        <p>output is then combined with the past </p>
        <p>cell state using pointwise multiplication, </p>
        <p> <i>xt</i></p>
        <p>generating an updated cell state <i><b>C't</b></i>&ndash;1. </p>
        <p>Looking at figure 15.4, we can see how the different inputs flow through the forget gate.
            First, the past hidden state <i>ht</i>&ndash;1 and the present value of a sequence <i
                class="calibre3">xt</i> are fed into the forget gate. Recall that the past hidden state carries
            information from past values. Then, <i>ht</i>&ndash;1 and <i>xt</i>
            are
            combined and duplicated. One copy goes straight to the input gate, which we'll study in the next
            section.
            The other copy is sent through</p>
        <p><a id="calibre_link-130"></a><img src="images/000147.jpg" alt="Image 118" class="calibre2" /></p>
        <p><b>292</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>a sigmoid activation function, which is expressed as equation 15.1 and is shown in figure
            15.5. </p>
        <p>Equation 15.1</p>
        <p>Figure 15.5</p>
        <p>The sigmoid function </p>
        <p>outputs values between 0 and 1. In the </p>
        <p>context of the forget gate, if the output </p>
        <p>of the sigmoid function is close to 0, the </p>
        <p>output is information that is forgotten. If </p>
        <p>the output is close to 1, it is information </p>
        <p>that must be kept. </p>
        <p>The sigmoid function determines which information to keep or to forget. That output is then
            combined with the previous cell state <i>Ct</i>&ndash;1 using pointwise multiplication.
            This results in an updated cell state that we call <i>C't</i>&ndash;1. </p>
        <p>Once this is done, two things are sent to the input gate: an updated cell state, and a copy
            of the combination of the past hidden state and the current element of the sequence. </p>
        <p> <i><b>15.2.2 The input gate</b></i></p>
        <p>Once information has passed through the forget gate, it proceeds to the input gate. </p>
        <p>This is the step where the network determines which information is relevant from the current
            element of the sequence. The cell state is updated again here, resulting in the final cell state. </p>
        <p>Again, let's zoom in on the input gate using figure 15.6. The combination of the past hidden
            state and the current element of a sequence [ <i>ht</i>&ndash;1 + <i class="calibre3">xt</i>] coming from
            the forget gate is fed into the input gate and it is again
            duplicated. One copy goes out the input gate toward the output gate, which we'll explore in the next
            section. Another copy is sent through the sigmoid activation function to determine if the information
            will
            be kept or forgotten. Another copy is sent through the hyperbolic tangent (tanh) function, which is
            shown in
            figure 15.7. </p>
        <p><a id="calibre_link-424"></a><img src="images/000011.jpg" alt="Image 119" class="calibre2" /></p>
        <p> <i><b>15.2</b></i></p>
        <p> <i><b>Examining the LSTM architecture</b></i></p>
        <p><b>293</b></p>
        <p>T o the output gate</p>
        <p> <i>C' </i></p>
        <p> <i>C</i></p>
        <p> <i>t-1</i></p>
        <p> <i>t</i></p>
        <p>+</p>
        <p>×</p>
        <p>Sigmoid</p>
        <p>Tanh</p>
        <p>T o the output gate</p>
        <p>[ <i>ht</i>&ndash;1 + <i>xt</i>]</p>
        <p>[ <i>ht</i>&ndash;1 + <i>xt</i>]</p>
        <p>[ <i>ht</i>&ndash;1 + <i>xt</i>]</p>
        <p>[ <i>ht</i>&ndash;1 + <i>xt</i>]</p>
        <p>Figure 15.6</p>
        <p>The input gate of an LSTM. The past hidden state and current </p>
        <p>element of the sequence are duplicated again and sent through a sigmoid </p>
        <p>activation function and a hyperbolic tangent (tanh) activation function. Again, the sigmoid
            determines what information is kept or discarded, while the tanh function regulates the network to keep
            it
            computationally efficient. The results of both </p>
        <p>operations are combined using pointwise multiplication, and the result is used to update the
            cell state using pointwise addition, resulting in the final cell state <i><b>Ct</b></i>.
        </p>
        <p>This final cell state is then sent to the output gate. Meanwhile, the same </p>
        <p>combination, [ <i><b>ht</b></i>&ndash;1 + <i><b>xt</b></i>], is sent to the
            output gate too. </p>
        <p>Figure 15.7</p>
        <p>The hyperbolic tangent </p>
        <p>(tanh) function outputs values between </p>
        <p>&ndash;1 and 1. In the context of the LSTM, this </p>
        <p>serves as a way to regulate the network, </p>
        <p>making sure that values do not get very </p>
        <p>large and ensuring that computation </p>
        <p>remains efficient. </p>
        <p><a id="calibre_link-131"></a><b>294</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>The outputs of the sigmoid and tanh functions are combined using pointwise multiplication,
            and the result is combined with the updated cell state coming from the forget gate <i
                class="calibre3">C't</i>&ndash;1 using pointwise addition. This operation generates the final cell
            state
            <i>Ct</i>. </p>
        <p>Therefore, it is in the input gate that we add information from the current element in the
            sequence to the long memory of the network. This newly updated cell state is then sent to the output
            gate.
        </p>
        <p> <i><b>15.2.3 The output gate</b></i></p>
        <p>Information has now passed from the forget gate to the input gate, and now it arrives at the
            output gate. It is in this gate that past information contained in the network's memory, represented by
            the
            cell state <i>Ct</i>, is finally used to process the current element of the sequence.
            This
            is also where the network either outputs a result to the output layer or computes new information to be
            sent
            to the processing of the next element in the sequence. </p>
        <p> <i>C</i></p>
        <p> <i>C</i></p>
        <p> <i>t</i></p>
        <p> <i>t</i></p>
        <p> <i>Ct</i></p>
        <p>Tanh</p>
        <p>Sigmoid</p>
        <p>×</p>
        <p> <i>h</i></p>
        <p> <i>h</i></p>
        <p> <i>t</i></p>
        <p> <i>t</i></p>
        <p>[ <i>h</i></p>
        <p>[</p>
        <p> <i>t-1</i> + <i>xt</i>]</p>
        <p> <i>ht</i>&ndash;1 + <i>xt</i>]</p>
        <p>Figure 15.8</p>
        <p>The output gate of an LSTM. The past hidden state and </p>
        <p>current element of a sequence [ <i><b>ht</b></i>&ndash;1 +
            <i><b>xt</b></i>] are passed through the sigmoid function to determine
            if
            information will be kept or discarded. Then the cell </p>
        <p>state is passed through the tanh function and combined with the output </p>
        <p>of the sigmoid using pointwise multiplication. This is the step where past </p>
        <p>information is used to process the current element of a sequence. We then </p>
        <p>output a new hidden state <i><b>ht</b></i>, which is
            passed to the next LSTM neuron or to the output layer. The cell state is also output. </p>
        <p>In figure 15.8 the past hidden state and current element of a sequence are sent through the
            sigmoid function. In parallel, the cell state goes through the tanh function. </p>
        <p><a id="calibre_link-132"></a> <i><b>15.3</b></i></p>
        <p> <i><b>Implementing the LSTM architecture</b></i></p>
        <p><b>295</b></p>
        <p>The resulting values from the tanh and sigmoid functions are then combined using pointwise
            multiplication, generating an updated hidden state <i>ht</i>. This is the step where
            past
            information, represented by the cell state <i>Ct</i>, is used to process the
            information of
            the present element of the sequence. </p>
        <p>The current hidden state is then sent out of the output gate. This will either be sent to
            the output layer of the network or to the next LSTM neuron treating the next element of the sequence.
            The
            same applies for the cell state <i>Ct</i>. </p>
        <p>In summary, the forget gate determines which information from the past is kept or discarded.
            The input gate determines which information from the current step is kept to update the network's memory
            or
            is discarded. Finally, the output gate uses the information from the past stored in the network's memory
            to
            process the current element of a sequence. </p>
        <p>Having examined the inner workings of the LSTM architecture, we can now imple-</p>
        <p>ment it for our interstate traffic dataset. </p>
        <p> <i><b>15.3</b></i></p>
        <p> <i><b>Implementing the LSTM architecture</b></i></p>
        <p>We'll now implement the LSTM architecture for the interstate traffic dataset we have been
            working with since chapter 12. Recall that the main target of our scenario is the traffic volume. For
            the
            multi-output model, the targets are traffic volume and temperature. </p>
        <p>We'll implement LSTM as a single-step model, a multi-step model, and a multi-</p>
        <p>output model. The single-step model will predict the traffic volume for the next timestep
            only, the multi-step model will predict the traffic volume for the next 24</p>
        <p>hours, and the multi-output model will predict the temperature and traffic volume for the
            next timestep. </p>
        <p>Make sure you have the DataWindow class and the compile_and_fit function (from chapters 13
            and 14) in your notebook or Python script, as we'll use these pieces of code to create windows of data
            and
            train the LSTM model. </p>
        <p>The other prerequisite is to read the training set, the validation set, and the test set, so
            let's do that right now:</p>
        <p>train_df = pd.read_csv('../data/train.csv', index_col=0)</p>
        <p>val_df = pd.read_csv('../data/val.csv', index_col=0)</p>
        <p>test_df = pd.read_csv('../data/test.csv', index_col=0)</p>
        <p>NOTE</p>
        <p>At any point, feel free to consult the source code for this chapter on</p>
        <p>GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15">tree/master/CH15</a>.
        </p>
        <p> <i><b>15.3.1 Implementing an LSTM as a single-step
                    model</b></i></p>
        <p>We'll start by implementing the LSTM architecture as a single-step model. In this case,
            we'll use 24 hours of data as an input to predict the next timestep. That way, there is a</p>
        <p><a id="calibre_link-230"></a><b>296</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>sequence of time that can be processed by the LSTM, allowing us to leverage past information
            to make a future prediction. </p>
        <p>First we need to create a data window to train the model. This will be a wide window, with
            24 hours of data as input. For plotting purposes, the label_width is also 24, so that we can compare the
            predictions to the actual values over 24 timesteps. Note that this is still a single-step model, so over
            24
            hours the model will only predict one timestep at a time, just like a rolling forecast. </p>
        <p>wide_window = DataWindow(input_width=24, label_width=24, shift=1, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>Then we need to define our LSTM model in Keras. Again we'll use the Sequential</p>
        <p>model to allow us to stack different layers in our network. Keras conveniently comes with
            the LSTM layer, which implements an LSTM. We'll set return_sequences to True, as this signals Keras to
            use
            past information from the sequence, in the form of the hidden state and cell state, which we covered
            earlier. Finally, we'll define the output layer, which is simply a Dense layer with one unit because we
            are
            forecasting the traffic volume only. </p>
        <p>lstm_model = Sequential([</p>
        <p><b>Set return_sequences to True to </b></p>
        <p>LSTM(32, return_sequences=True), </p>
        <p><b>make sure that past information </b></p>
        <p>Dense(units=1)</p>
        <p><b>is being used by the network. </b></p>
        <p>])</p>
        <p>It is as simple as that. We can now train the model using the compile_and_fit function and
            store its performance on the validation and test sets. </p>
        <p>history = compile_and_fit(lstm_model, wide_window)</p>
        <p>val_performance = {}</p>
        <p>performance = {}</p>
        <p>val_performance['LSTM'] = lstm_model.evaluate(wide_window.val)</p>
        <p>performance['LSTM'] = lstm_model.evaluate(wide_window.test, verbose=0)</p>
        <p>Optionally, we can visualize the predictions of our model on three sampled sequences using
            the plot method of our data window. The result is shown in figure 15.9. </p>
        <p>wide_window.plot(lstm_model)</p>
        <p>Figure 15.9 shows that we have a performant model generating accurate predictions. </p>
        <p>Of course, this visualization is only three sampled sequences of 24 hours, so let's
            visualize the model's performance on the entire validation and test sets and compare it to the previous
            models we have built so far. </p>
        <p></p>
        <p><a id="calibre_link-133"></a><img class="calibre2" src="images/000002.png" alt="Image 120" /></p>
        <p> <i><b>15.3</b></i></p>
        <p> <i><b>Implementing the LSTM architecture</b></i></p>
        <p><b>297</b></p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>0.2</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 15.9</p>
        <p>Predicting traffic volume using an LSTM as a single-step model. Many predictions (shown as
            crosses) overlap the labels (shown as squares), suggesting we have a performant model with accurate
            predictions. </p>
        <p>Figure 15.10 shows that the LSTM is the winning model, since it has the lowest MAE</p>
        <p>on both the validation and test sets, meaning that it generated the most accurate
            predictions of all the models. </p>
        <p> <i><b>15.3.2 Implementing an LSTM as a multi-step
                    model</b></i></p>
        <p>We'll move on to implementing the LSTM architecture as a multi-step model. In this case, we
            wish to predict traffic volume for next 24 hours, using an input window of 24 hours. </p>
        <p>First, we'll define the time window to feed our model. The input_width and label_width are
            both 24, since we want to input 24 hours of data and evaluate the predictions on 24 hours of data as
            well.
            This time the shift is also 24, specifying that the model must output predictions for the next 24 hours
            in a
            single shot. </p>
        <p>multi_window = DataWindow(input_width=24, label_width=24, shift=24, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>Next, we'll define our model in Keras. From chapter 14, you might recall that the process of
            defining the multi-step model and single-step model was exactly the same. The</p>
        <p><a id="calibre_link-231"></a><img class="calibre2" src="images/000138.jpg" alt="Image 121" /></p>
        <p><b>298</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>0.10</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.083</p>
        <p>0.081</p>
        <p>0.08</p>
        <p>0.068</p>
        <p>0.068</p>
        <p>0.06</p>
        <p>0.04</p>
        <p>Mean absolute error</p>
        <p>0.033</p>
        <p>0.03</p>
        <p>0.029</p>
        <p>0.026</p>
        <p>0.02</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>Models</p>
        <p>Figure 15.10</p>
        <p>Mean absolute error (MAE) of all single-step models built so far. For now, the LSTM is the
            winning model, since it has the lowest MAE on both the validation and test sets. </p>
        <p>same is true here. We still use the Sequential model, along with the LSTM layer and a Dense
            output layer with one unit. </p>
        <p>ms_lstm_model = Sequential([</p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>Once it's defined, we'll train the model and store its evaluation metrics for comparison. By
            now, you should be comfortable with this workflow. </p>
        <p>history = compile_and_fit(ms_lstm_model, multi_window)</p>
        <p>ms_val_performance = {}</p>
        <p>ms_performance = {}</p>
        <p>ms_val_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.val)</p>
        <p>ms_performance['LSTM'] = ms_lstm_model.evaluate(multi_window.test, </p>
        <p>➥ verbose=0)</p>
        <p><a id="calibre_link-134"></a><img class="calibre2" src="images/000065.png" alt="Image 122" /></p>
        <p> <i><b>15.3</b></i></p>
        <p> <i><b>Implementing the LSTM architecture</b></i></p>
        <p><b>299</b></p>
        <p>We can visualize the predictions of the model using the plot method, as shown in figure
            15.11. </p>
        <p>multi_window.plot(ms_lstm_model)</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>0.2</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled]</p>
        <p>0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 15.11</p>
        <p>Predicting the traffic volume over the next 24 hours using a multi-step LSTM model. We can
            see some discrepancies between the predictions and the labels. Of course, this visual inspection is not
            enough to assess the performance of the model. </p>
        <p>In figure 15.11 you'll see that the predictions for the top sequence are very good, as most
            predictions overlap the actual values. However, there are some discrepancies between the output and
            labels
            in the bottom two sequences. Let's compare its MAE to that of the other multi-step models we have built.
        </p>
        <p>As you can see in figure 15.12, the LSTM is our most accurate model so far, as it achieved
            the lowest MAE on both the validation and test sets. </p>
        <p> <i><b>15.3.3 Implementing an LSTM as a multi-output
                    model</b></i></p>
        <p>Finally, we'll implement an LSTM as a multi-output model. Again, we'll use 24 hours of input
            data, so that the network can process a sequence of data points and use past information to produce
            forecasts. The predictions will be for both the traffic volume and temperature at the next timestep.
        </p>
        <p><a id="calibre_link-425"></a><img class="calibre2" src="images/000173.jpg" alt="Image 123" /></p>
        <p><b>300</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>0.40</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.352 0.347</p>
        <p>0.35</p>
        <p>0.347 0.341</p>
        <p>0.30</p>
        <p>0.25</p>
        <p>0.20</p>
        <p>Mean absolute error 0.15</p>
        <p>0.10</p>
        <p>0.088</p>
        <p>0.076</p>
        <p>0.078</p>
        <p>0.07</p>
        <p>0.064</p>
        <p>0.058</p>
        <p>0.05</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Baseline - Repeat</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>Models</p>
        <p>Figure 15.12</p>
        <p>The MAE of all the multi-step models built so far. Again, the LSTM is the winning model,
            since it achieves the lowest MAE on both the validation and test sets. </p>
        <p>In this situation, the data window consists of an input of 24 timesteps and 24 timesteps of
            labels. The shift is 1, as we want to produce forecasts for the next timestep only. </p>
        <p>Thus, our model will be creating rolling forecasts to generate predictions one timestep at a
            time, over 24 timesteps. We'll specify temp and traffic_volume as our target columns. </p>
        <p>mo_wide_window = DataWindow(input_width=24, label_width=24, shift=1, </p>
        <p>➥ label_columns=['temp','traffic_volume'])</p>
        <p>The next step is to define our LSTM model. Just as before, we'll use the Sequential model to
            stack an LSTM layer and a Dense output layer with two units, since we have two targets. </p>
        <p>mo_lstm_model = Sequential([</p>
        <p>LSTM(32, return_sequences=True), </p>
        <p><a id="calibre_link-426"></a><img class="calibre2" src="images/000082.png" alt="Image 124" /></p>
        <p> <i><b>15.3</b></i></p>
        <p> <i><b>Implementing the LSTM architecture</b></i></p>
        <p><b>301</b></p>
        <p>Dense(units = 2) </p>
        <p><b>We have two units because we have </b></p>
        <p>])</p>
        <p><b>two targets: the temperature and </b></p>
        <p><b>the traffic volume. </b></p>
        <p>Then we'll train the model and store its performance metrics for comparison. </p>
        <p>history = compile_and_fit(mo_lstm_model, mo_wide_window)</p>
        <p>mo_val_performance = {}</p>
        <p>mo_performance = {}</p>
        <p>mo_val_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.val)</p>
        <p>mo_performance['LSTM'] = mo_lstm_model.evaluate(mo_wide_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>We can now visualize the prediction for the traffic volume (figure 15.13) and temperature
            (figure 15.14). Both figures show many predictions (shown as crosses) overlapping the labels (shown as
            squares), which means that we have a performant model generating accurate predictions. </p>
        <p>Inputs</p>
        <p>0.6</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 15.13</p>
        <p>Predicting the traffic volume with an LSTM as a multi-output model. Many predictions (shown
            as crosses) overlap the labels (shown as squares), suggesting very accurate predictions for the traffic
            volume. </p>
        <p><a id="calibre_link-135"></a><img class="calibre2" src="images/000040.png" alt="Image 125" /></p>
        <p><b>302</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>0.65</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>0.60</p>
        <p>Predictions</p>
        <p>0.55</p>
        <p>0.50</p>
        <p>temp [scaled]</p>
        <p>0.45</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.90</p>
        <p>0.85</p>
        <p>0.80</p>
        <p>temp [scaled]</p>
        <p>0.75</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.45</p>
        <p>0.40</p>
        <p>0.35</p>
        <p>0.30</p>
        <p>0.25</p>
        <p>temp [scaled]</p>
        <p>0.20</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 15.14</p>
        <p>Predicting the temperature using an LSTM as a multi-output model. Again, we see a lot of
            overlap between the predictions (shown as crosses) and the labels (shown as squares), indicating
            accurate
            predictions. </p>
        <p>Let's compare our LSTM model's performance to the other multi-output models built so far.
            Figure 15.15 again shows the LSTM as the winning model, since it achieves the lowest MAE on the
            validation
            and test sets. Thus, it generated the most accurate predictions so far for both our targets. </p>
        <p> <i><b>15.4</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we examined the long short-term memory (LSTM) architecture. You learned
            that it is a subtype of RNN, and you saw how it uses a cell state to overcome the problem of short-term
            memory that occurs in a basic RNN that uses only the hidden state. </p>
        <p>We also studied the three gates of the LSTM. The forget gate determines which</p>
        <p>information from the past and present must be kept, the input gate determines the relevant
            information from the current element of a sequence, and the output gate uses the information stored in
            memory to generate a prediction. </p>
        <p>We then implemented the LSTM as a single-step model, multi-step model, and multi-output
            model. In all cases, the LSTM was the winning model, as it achieved the lowest MAE of all models built
            so
            far. </p>
        <p><a id="calibre_link-136"></a><img class="calibre2" src="images/000103.jpg" alt="Image 126" /></p>
        <p> <i><b>15.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p><b>303</b></p>
        <p>0.06</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.048</p>
        <p>0.05</p>
        <p>0.047</p>
        <p>0.039</p>
        <p>0.04</p>
        <p>0.036</p>
        <p>0.03</p>
        <p>0.023</p>
        <p>Mean absolute error</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.017</p>
        <p>0.01</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>Models</p>
        <p>Figure 15.15</p>
        <p>The MAE of all the multi-output models built so far. Again, the winning model is the LSTM,
            as it achieved the lowest MAE of all. </p>
        <p>The deep learning architecture that we will explore in the next chapter is the <i
                class="calibre3">convolutional neural network</i> (CNN). You might have come across a CNN,
            especially in
            computer vision, as it is a very popular architecture for analyzing pictures. We will apply it for time
            series forecasting, as CNNs are faster to train than LSTMs, they are robust to noise, and they are good
            feature extractors. </p>
        <p> <i><b>15.5</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>In the last chapter, we built linear models and deep neural networks to forecast the air
            quality. Now we'll try LSTM models and see if there is a gain in performance. The solution to these
            exercises can be found on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15">https://github.com/marcopeix/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH15">TimeSeriesForecastingInPython/tree/master/CH15.
            </a></p>
        <p>1</p>
        <p>For the ingle-step model:</p>
        <p>a</p>
        <p>Build an LSTM model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p><a id="calibre_link-257"></a><b>304</b></p>
        <p>CHAPTER 15</p>
        <p> <i><b>Remembering the past with LSTM</b></i></p>
        <p>c</p>
        <p>Evaluate it using the MAE and store the MAE. </p>
        <p>d</p>
        <p>Is it the most performant model? </p>
        <p>2</p>
        <p>For the multi-step model:</p>
        <p>a</p>
        <p>Build an LSTM model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate it using the MAE and store the MAE. </p>
        <p>d</p>
        <p>Is it the most performant model? </p>
        <p>3</p>
        <p>For the multi-output model:</p>
        <p>a</p>
        <p>Build an LSTM model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate it using the MAE and store the MAE. </p>
        <p>d</p>
        <p>Is it the most performant model? </p>
        <p>At any point, try to experiment with the following ideas:</p>
        <p> Add more LSTM layers. </p>
        <p> Change the number of units in the LSTM layer. </p>
        <p> Set return_sequences to False. </p>
        <p> Experiment with different initializers in the output Dense layer. </p>
        <p> Run as many experiments as you want, and see how they impact the error metric. </p>
        <p> <i><b> Summary</b></i></p>
        <p> A recurrent neural network (RNN) is a deep learning architecture especially adapted to
            processing sequences of data like a time series. </p>
        <p> RNNs use a hidden state to store information in memory. However, this is only</p>
        <p>short-term memory due to the vanishing gradient problem. </p>
        <p> Long short-term memory (LSTM) is a type of RNN that addresses the short-term memory
            problem. It uses a cell state to store information for a longer time, giving the network a <i
                class="calibre3">long</i> memory. </p>
        <p> The LSTM is made of three gates:</p>
        <p>&ndash; The forget gate determines what information from the past and present must</p>
        <p>be kept. </p>
        <p>&ndash; The input gate determines what information from the present must be kept. </p>
        <p>&ndash; The output gate uses information stored in memory to process the current</p>
        <p>element of a sequence. </p>
        <p><a id="calibre_link-19"></a> <i>Filtering a time</i></p>
        <p> <i>series with CNN</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the CNN architecture</p>
        <p> Implementing a CNN with Keras</p>
        <p> Combining a CNN with an LSTM</p>
        <p>In the last chapter, we examined and implemented a long short-term memory (LSTM) network,
            which is a type of recurrent neural network (RNN) that processes sequences of data especially well. Its
            implementation was the top performing architecture for the single-step model, multi-step model, and
            multi-output model. </p>
        <p>Now we're going to explore the <i>convolutional neural network</i> (CNN).
            CNNs are mostly applied in the field of computer vision, and this architecture is behind many algorithms
            for
            image classification and image segmentation. </p>
        <p>Of course, this architecture can also be used for time series analysis. It turns out that
            CNNs are noise resistant and can effectively filter out the noise in a time series with the <i
                class="calibre3">convolution</i> operation. This allows the network to produce a set of robust
            features
            that do not include abnormal values. In addition, CNNs are usually faster to train than LSTMs, as their
            operations can be parallelized. </p>
        <p>In this chapter, we'll first explore the CNN architecture and understand how</p>
        <p>the network filters a time series and creates a unique set of features. Then we'll <b
                class="calibre4">305</b></p>
        <p><a id="calibre_link-137"></a><b>306</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>implement a CNN using Keras to produce forecasts. We'll also combine the CNN</p>
        <p>architecture with the LSTM architecture to see if we can further improve the performance of
            our deep learning models. </p>
        <p> <i><b>16.1</b></i></p>
        <p> <i><b>Examining the convolutional neural network
                    (CNN)</b></i></p>
        <p>A convolutional neural network is a deep learning architecture that makes use of the
            convolution operation. The convolution operation allows the network to create a reduced set of features.
            Therefore, it is a way of regularizing the network, preventing overfitting, and effectively filtering
            the
            inputs. Of course, for this to make sense, you must first understand the convolution operation and how
            it
            impacts the inputs. </p>
        <p>In mathematical terms, a convolution is an operation on two functions that generates a third
            function that expresses how the shape of one function is changed by the other. In a CNN, this operation
            occurs between the inputs and a <i>kernel</i> (also known as a <i class="calibre3">filter</i>). The kernel
            is simply a matrix that is placed on top of the feature
            matrix.
            In figure 16.1, the kernel is slid along the time axis, taking the dot product between the kernel and
            the
            features. This results in a reduced set of features, achieving regularization and the filtering of
            abnormal
            values. </p>
        <p>Feature 1</p>
        <p>Feature 2</p>
        <p>Feature 3</p>
        <p>Figure 16.1</p>
        <p>Visualizing the kernel </p>
        <p>Feature 4</p>
        <p>and the feature map. The kernel is </p>
        <p>the light gray matrix that is applied </p>
        <p>Feature 5</p>
        <p>on top of the feature map. Each row </p>
        <p>corresponds to a feature of the </p>
        <p>dataset, while the length is the </p>
        <p>Time</p>
        <p>time axis. </p>
        <p>To better understand the convolution operation, let's consider a simple example with only
            one feature and one kernel, as shown in figure 16.2. To make things simple, we'll consider only one row
            of
            features. Keep in mind that the horizontal axis remains the time dimension. The kernel is a smaller
            vector
            that is used to perform the convolution operation. Do not worry about the values used inside the kernel
            and
            the feature vector. They are arbitrary values. The values of the kernel are optimized and will change as
            the
            network is trained. </p>
        <p>Feature</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>Kernel</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Figure 16.2</p>
        <p>A simple example of one </p>
        <p>row of features and one kernel. </p>
        <p><a id="calibre_link-189"></a> <i><b>16.1</b></i></p>
        <p> <i><b>Examining the convolutional neural network
                    (CNN)</b></i></p>
        <p><b>307</b></p>
        <p>We can visualize the convolution operation and its result in figure 16.3. At first, the
            kernel is aligned with the beginning of the feature vector and the dot product is taken between the
            kernel
            and the values of the feature vector that are aligned with it. Once this is done, the kernel shifts one
            timestep to the right&mdash;this is also called a <i>stride</i> of one timestep. The
            dot
            product is again taken between the kernel and the feature vector, again only with the values that are
            aligned with the kernel. The kernel again shifts one timestep to the right, and the process is repeated
            until the kernel reaches the end of the feature vector. This happens when the kernel cannot be shifted
            any
            further with all of its values having an aligned feature value. </p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>Step 1</p>
        <p>1 × 2 + 0 × 3 + 1 × 12 = 14</p>
        <p>14</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 2</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>1 × 3 + 0 × 12 + 1 × 0 = 3</p>
        <p>14</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 3</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>1 × 12 + 0 × 0 + 1 × 3 = 15</p>
        <p>14</p>
        <p>3</p>
        <p>15</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 4</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>1 × 0 + 0 × 3 + 1 × 1 = 1</p>
        <p>14</p>
        <p>3</p>
        <p>15</p>
        <p>1</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Figure 16.3</p>
        <p>The full convolution operation. The operation starts with the kernel aligned at the
            beginning of the feature vector in step 1. The dot product is computed as shown by the intermediary
            equation
            of step 1, resulting in the first value in our output vector. In step 2, the kernel shifts one timestep
            to
            the right, and the dot product is taken again, resulting in the second value in the output vector. The
            process is repeated two more times until the kernel reaches the end of the feature vector. </p>
        <p>In figure 16.3 you can see that using a feature vector of length 6 and a kernel of length 3,
            we obtain an output vector of length 4. Thus, in general, the length of the output vector of a
            convolution
            is given by equation 16.1. </p>
        <p>output length = input length &ndash; kernel length + 1</p>
        <p>Equation 16.1</p>
        <p>Note that since the kernel is moving only in one direction (to the right), this is a <i
                class="calibre3">1D</i></p>
        <p> <i>convolution</i>. Luckily, Keras comes with the Conv1D layer, allowing
            us to easily implement</p>
        <p><a id="calibre_link-277"></a><b>308</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>it in Python. This is mostly used for time series forecasting, as the kernel can only move
            in the time dimension. For image processing, you'll often see 2D or 3D convolutions, but that is outside
            of
            the scope of this book. </p>
        <p>A convolution layer reduces the length of the set of features, and performing many
            convolutions will keep reducing the feature space. This can be problematic, as it limits the number of
            layers in the network, and we might lose too much information in the process. A common technique to
            prevent
            that is <i>padding</i>. Padding simply means adding values before and after the feature
            vector to keep the output length equivalent to the input length. Padding values are often zeros. You can
            see
            this in action in figure 16.4, where the output of the convolution is the same length as the input. </p>
        <p>Step 1</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 2</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>14</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 3</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>14</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 4</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>14</p>
        <p>3</p>
        <p>15</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 5</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>14</p>
        <p>3</p>
        <p>15</p>
        <p>1</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Step 6</p>
        <p>0</p>
        <p>2</p>
        <p>3</p>
        <p>12</p>
        <p>0</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>3</p>
        <p>14</p>
        <p>3</p>
        <p>15</p>
        <p>1</p>
        <p>3</p>
        <p>1</p>
        <p>0</p>
        <p>1</p>
        <p>Figure 16.4</p>
        <p>Convolution with padding. Here we padded the original input vector with zeros, as shown by
            the black squares. The output of the convolution thus has a length of 6, just like the original feature
            vector. </p>
        <p><a id="calibre_link-138"></a> <i><b>16.2</b></i></p>
        <p> <i><b>Implementing a CNN</b></i></p>
        <p><b>309</b></p>
        <p>You can thus see how padding keeps the dimension of the output constant, allowing us to
            stack more convolution layers, and allowing the network to process features for a longer time. We use
            zeroes
            for padding because a multiplication by 0 is ignored. Thus, using zeroes as padding values is usually a
            good
            initial option. </p>
        <p>Convolutional neural network (CNN)</p>
        <p>A convolutional neural network (CNN) is a deep learning architecture that uses the
            convolution operation. This allows the network to reduce the feature space, effectively filtering the
            inputs
            and preventing overfitting. </p>
        <p>The convolution is performed with a kernel, which is also trained during model fitting. </p>
        <p>The stride of the kernel determines the number of steps it shifts at each step of the
            convolution. In time series forecasting, only 1D convolution is used. </p>
        <p>To avoid reducing the feature space too quickly, we can use padding, which adds zeros before
            and after the input vector. This keeps the output dimension the same as the original feature vector,
            allowing us to stack more convolution layers, which in turn allows the network to process the features
            for a
            longer time. </p>
        <p>Now that you understand the inner working of a CNN, we can implement it with Keras and see
            if a CNN can produce more accurate predictions than the models we have built so far. </p>
        <p> <i><b>16.2</b></i></p>
        <p> <i><b>Implementing a CNN</b></i></p>
        <p>As in previous chapters, we'll implement the CNN architecture as a single-step model, a
            multi-step model, and a multi-output model. The single-step model will predict the traffic volume for
            the
            next timestep only, the multi-step model will predict the traffic volume for the next 24 hours, and the
            multi-output model will predict the temperature and traffic volume at the next timestep. </p>
        <p>Make sure you have the DataWindow class and the compile_and_fit function (from</p>
        <p>chapters 13 to 15) in your notebook or Python script, as we'll use both pieces of code to
            create windows of data and train the CNN model. </p>
        <p>NOTE</p>
        <p>The source code for this chapter is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH16">https://github</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH16">.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH16.
            </a></p>
        <p>In this chapter, we'll also combine the CNN architecture with the LSTM architecture. It can
            be interesting to see if filtering our time series with a convolution layer and then processing the
            filtered
            sequence with an LSTM will improve the accuracy of our predictions. </p>
        <p>Thus, we'll implement both a CNN only, and the combination of a CNN with an LSTM. </p>
        <p>Of course, the other prerequisite is to read the training set, the validation set, and the
            test set, so let's do that right now. </p>
        <p>train_df = pd.read_csv('../data/train.csv', index_col=0)</p>
        <p>val_df = pd.read_csv('../data/val.csv', index_col=0)</p>
        <p>test_df = pd.read_csv('../data/test.csv', index_col=0)</p>
        <p><a id="calibre_link-139"></a><b>310</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>Finally, we'll use a kernel length of three timesteps in our CNN implementation. This is an
            arbitrary value, and you will have a chance to experiment with various kernel lengths in this chapter's
            exercises and see how they impact the model's performance. </p>
        <p>However, your kernel should have a length greater than 1; otherwise, you are simply
            multiplying the feature space by a scalar, and no filtering will be achieved. </p>
        <p> <i><b>16.2.1 Implementing a CNN as a single-step
                    model</b></i></p>
        <p>We'll start by implementing a CNN as a single-step model. Recall that the single-step model
            outputs a prediction for traffic volume at the next timestep using the last known feature. </p>
        <p>In this case, however, it does not make sense to provide the CNN model with only one
            timestep as an input because we want to run a convolution. We will instead use three input values to
            generate a prediction for the next timestep. That way we'll have a sequence of data on which we can run
            a
            convolution operation. Furthermore, our input sequence must have a length at least equal to the kernel's
            length, which in our case is 3. Recall that we expressed the relationship between the input length,
            kernel
            length, and output length in equation 16.1:</p>
        <p>output length = input length &ndash; kernel length + 1</p>
        <p>In this equation, no length can be equal to 0, since that would mean that no data is being
            processed or output. The condition that no length can be 0 is only satisfied if the input length is
            greater
            than or equal to the kernel length. Therefore, our input sequence must have at least three timesteps.
        </p>
        <p>We can thus define the data window that will be used to train the model. </p>
        <p>KERNEL_WIDTH = 3</p>
        <p>conv_window = DataWindow(input_width=KERNEL_WIDTH, label_width=1, shift=1, </p>
        <p>➥ label_columns=['traffic_volume']) </p>
        <p>For plotting purposes, we would like to see the predictions of the model over a period of 24
            hours. That way, we can evaluate the rolling forecasts of the model 1 timestep at a time, over 24
            timesteps.
            Thus, we need to define another data window with a label_width of 24. The shift remains 1, as the model
            only
            predicts the next timestep. The input length is obtained by rearranging equation 16.1 as equation 16.2.
        </p>
        <p>output length = input length &ndash; kernel length + 1</p>
        <p>input length = output length + kernel length &ndash; 1</p>
        <p>Equation 16.2</p>
        <p>We can now simply compute the required input length to generate predictions over a sequence
            of 24 timesteps. In this case, the input length is 24 + 3 &ndash; 1 = 26. That way, we avoid using
            padding.
            Later, in the exercises, you'll be able to try using padding instead of a longer input sequence to
            accommodate the output length. </p>
        <p><a id="calibre_link-232"></a> <i><b>16.2</b></i></p>
        <p> <i><b>Implementing a CNN</b></i></p>
        <p><b>311</b></p>
        <p>We can now define our data window for plotting the predictions of the model. </p>
        <p>LABEL_WIDTH = 24</p>
        <p><b>From equation </b></p>
        <p><b>1</b></p>
        <p>INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH &ndash; 1 </p>
        <p><b>6.2</b></p>
        <p>wide_conv_window = DataWindow(input_width=INPUT_WIDTH, </p>
        <p>➥ label_width=LABEL_WIDTH, shift=1, label_columns=['traffic_volume'])</p>
        <p>With all the data windows ready, we can define our CNN model. Again, we'll use the
            Sequential model from Keras to stack different layers. Then we'll use the Conv1D</p>
        <p>layer, as we are working with time series, and the kernel only moves in the temporal
            dimension. The filters parameter is equivalent to the units parameter of the Dense layer, and it simply
            represents the number of neurons in the convolutional layer. We'll set the kernel_size to the width of
            our
            kernel, which is 3. We don't need to specify the other dimensions, as Keras will automatically take the
            right shape to accommodate the inputs. Then we'll pass the output of the CNN to a Dense layer. That way,
            the
            model will be learning on a reduced set of features that were previously filtered by the convolutional
            step.
            We'll finally output a prediction with a Dense layer of only one unit, as we are forecasting only the
            traffic volume for the next timestep. </p>
        <p><b>The filters parameter is equivalent to the units </b></p>
        <p><b>parameter of the Dense layer; it defines the number </b></p>
        <p>cnn_model = Sequential([</p>
        <p><b>of neurons in the convolutional layer. </b></p>
        <p>Conv1D(filters=32, </p>
        <p>kernel_size=(KERNEL_WIDTH,), </p>
        <p><b>The width of the kernel is specified, </b></p>
        <p>activation='relu'), </p>
        <p><b>but the other dimensions are left </b></p>
        <p>Dense(units=32, activation='relu'), </p>
        <p><b>out, as Keras automatically adapts </b></p>
        <p>Dense(units=1)</p>
        <p><b>to the shape of the inputs. </b></p>
        <p>])</p>
        <p>Next, we'll compile and fit the model, and we'll store its performance metrics for
            comparison later. </p>
        <p>history = compile_and_fit(cnn_model, conv_window)</p>
        <p>val_performance = {}</p>
        <p>performance = {}</p>
        <p>val_performance['CNN'] = cnn_model.evaluate(conv_window.val)</p>
        <p>performance['CNN'] = cnn_model.evaluate(conv_window.test, verbose=0)</p>
        <p>We can visualize the predictions against the labels using the plot method of our data
            window. The result is shown in figure 16.5. </p>
        <p>wide_conv_window.plot(cnn_model)</p>
        <p>As you can see in figure 16.5, many predictions overlap labels, meaning that we have fairly
            accurate predictions. Of course, we must compare this model's performance metrics to those of the other
            models to properly assess its performance. </p>
        <p><a id="calibre_link-427"></a><img class="calibre2" src="images/000169.png" alt="Image 127" /></p>
        <p><b>312</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>0.2</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>traffic_volume [scaled]</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>5</p>
        <p>10</p>
        <p>15</p>
        <p>20</p>
        <p>25</p>
        <p>Time (h)</p>
        <p>Figure 16.5</p>
        <p>Predicting traffic volume with a CNN as a single-step model. The model takes three values as
            an input, which is why we only see a prediction at the fourth timestep. Again, many predictions (shown
            as
            crosses) overlap labels (shown as squares), meaning that the model is fairly accurate. </p>
        <p>Before doing that, let's combine the CNN and LSTM architectures into a single model. </p>
        <p>You saw in the previous chapter how the LSTM architecture resulted in the best-performing
            models so far. Thus, it is a reasonable hypothesis that filtering our input sequence before feeding it
            to an
            LSTM might improve the performance. </p>
        <p>Thus, we'll follow the Conv1D layer with two LSTM layers. This is an arbitrary choice, so
            make sure you experiment with it later on. There is rarely only one good way of building models, so it
            is
            important to showcase what is possible. </p>
        <p>cnn_lstm_model = Sequential([</p>
        <p>Conv1D(filters=32, </p>
        <p>kernel_size=(KERNEL_WIDTH,), </p>
        <p>activation='relu'), </p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(1)</p>
        <p>]) </p>
        <p>We'll then fit the model and store its evaluation metrics. </p>
        <p></p>
        <p><a id="calibre_link-222"></a><img class="calibre2" src="images/000158.png" alt="Image 128" /></p>
        <p> <i><b>16.2</b></i></p>
        <p> <i><b>Implementing a CNN</b></i></p>
        <p><b>313</b></p>
        <p>history = compile_and_fit(cnn_lstm_model, conv_window)</p>
        <p>val_performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.val)</p>
        <p>performance['CNN + LSTM'] = cnn_lstm_model.evaluate(conv_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>With both models built and evaluated, we can look at the MAE of our newly built models in
            figure 16.6. As you can see, the CNN model did not perform any better than the LSTM, and the combination
            of
            CNN and LSTM resulted in a slightly higher MAE</p>
        <p>than the CNN alone. </p>
        <p>0.10</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.083 0.081</p>
        <p>0.08</p>
        <p>0.068 0.068</p>
        <p>0.06</p>
        <p>Mean absolute error 0.04</p>
        <p>0.036</p>
        <p>0.033</p>
        <p>0.034</p>
        <p>0.033</p>
        <p>0.029</p>
        <p>0.03</p>
        <p>0.03</p>
        <p>0.026</p>
        <p>0.02</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>CNN</p>
        <p>CNN + LSTM</p>
        <p>Models</p>
        <p>Figure 16.6</p>
        <p>The MAE of all the single-step models built so far. You can see that the CNN did not improve
            upon the LSTM performance. Combining the CNN with an LSTM did not help either, and the combination even
            performed slightly worse than the CNN. </p>
        <p>These results might be explained by the length of the input sequence. The model is given
            only an input sequence of three values, which might not be sufficient for the CNN to extract valuable
            features for predictions. While a CNN is better than the baseline model and the linear model, the LSTM
            remains the best-performing single-step model for now. </p>
        <p><a id="calibre_link-140"></a><b>314</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p> <i><b>16.2.2 Implementing a CNN as a multi-step
                    model</b></i></p>
        <p>We'll now move on to the multi-step model. Here we'll use the last known 24 hours to
            forecast the traffic volume over the next 24 hours. </p>
        <p>Again, keep in mind that the convolution reduces the length of the features, but we still
            expect the model to generate 24 predictions in a single shot. Therefore, we'll reuse equation 16.2 and
            feed
            the model an input sequence with a length of 26 to make sure that we get an output of length 24. This,
            of
            course, means that we'll keep the kernel length of 3. We can thus define our data window for the
            multi-step
            model. </p>
        <p>KERNEL_WIDTH = 3</p>
        <p>LABEL_WIDTH = 24</p>
        <p>INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1</p>
        <p>multi_window = DataWindow(input_width=INPUT_WIDTH, label_width=LABEL_WIDTH, </p>
        <p>➥ shift=24, label_columns=['traffic_volume'])</p>
        <p>Next, we'll define the CNN model. Again, we'll use the Sequential model, in which we'll
            stack the Conv1D layer, followed by a Dense layer with 32 neurons, and then a Dense layer with one unit,
            since we are predicting only traffic volume. </p>
        <p>ms_cnn_model = Sequential([</p>
        <p>Conv1D(32, activation='relu', kernel_size=(KERNEL_WIDTH)), </p>
        <p>Dense(units=32, activation='relu'), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>We can then train the model and store its performance metrics for comparison later. </p>
        <p>history = compile_and_fit(ms_cnn_model, multi_window)</p>
        <p>ms_val_performance = {}</p>
        <p>ms_performance = {}</p>
        <p>ms_val_performance['CNN'] = ms_cnn_model.evaluate(multi_window.val)</p>
        <p>ms_performance['CNN'] = ms_cnn_model.evaluate(multi_window.test, verbose=0)</p>
        <p>Optionally, we can visualize the forecasts of the model using multi_window</p>
        <p>.plot(ms_cnn_model). For now, let's skip this and combine the CNN architecture with the LSTM
            architecture as previously. Here we'll simply replace the intermediate Dense layer with an LSTM layer.
            Once
            the model is defined, we can fit it and store its performance metrics. </p>
        <p>ms_cnn_lstm_model = Sequential([</p>
        <p>Conv1D(32, activation='relu', kernel_size=(KERNEL_WIDTH)), </p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>history = compile_and_fit(ms_cnn_lstm_model, multi_window)</p>
        <p><a id="calibre_link-141"></a><img class="calibre2" src="images/000096.png" alt="Image 129" /></p>
        <p> <i><b>16.2</b></i></p>
        <p> <i><b>Implementing a CNN</b></i></p>
        <p><b>315</b></p>
        <p>ms_val_performance['CNN + LSTM'] = </p>
        <p>➥ ms_cnn_lstm_model.evaluate(multi_window.val)</p>
        <p>ms_performance['CNN + LSTM'] = </p>
        <p>➥ ms_cnn_lstm_model.evaluate(multi_window.test, verbose=0)</p>
        <p>With the two new models trained, we can evaluate their performance against all the
            multi-step models built so far. As you can see in figure 16.7, the CNN model did not improve upon the
            LSTM
            model. However, combining both models resulted in the lowest MAE of all the multi-step models, meaning
            that
            it generates the most accurate predictions. The LSTM model is thus dethroned, and we have a new winning
            model. </p>
        <p>0.40</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.352 0.347</p>
        <p>0.347</p>
        <p>0.35</p>
        <p>0.341</p>
        <p>0.30</p>
        <p>0.25</p>
        <p>0.20</p>
        <p>Mean absolute error 0.15</p>
        <p>0.10</p>
        <p>0.088</p>
        <p>0.076</p>
        <p>0.078</p>
        <p>0.078</p>
        <p>0.07</p>
        <p>0.064</p>
        <p>0.069</p>
        <p>0.063</p>
        <p>0.058</p>
        <p>0.055</p>
        <p>0.05</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Baseline - Repeat</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>CNN</p>
        <p>CNN + LSTM</p>
        <p>Models</p>
        <p>Figure 16.7</p>
        <p>The MAE of all multi-step models built so far. The CNN model is worse than the LSTM model,
            since it has a higher MAE. However, combining the CNN with an LSTM resulted in the lowest MAE of all.
        </p>
        <p> <i><b>16.2.3 Implementing a CNN as a multi-output
                    model</b></i></p>
        <p>Finally, we'll implement the CNN architecture as a multi-output model. In this case, we wish
            to forecast the temperature and traffic volume for the next timestep only. </p>
        <p>We have seen that giving an input sequence of length 3 was not sufficient for the CNN model
            to extract meaningful features, so we will use the same input length as for</p>
        <p><a id="calibre_link-428"></a><b>316</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>the multi-step model. This time, however, we are forecasting one timestep at a time over 24
            timesteps. </p>
        <p>We'll define our data window as follows:</p>
        <p>KERNEL_WIDTH = 3</p>
        <p>LABEL_WIDTH = 24</p>
        <p>INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1</p>
        <p>wide_mo_conv_window = DataWindow(input_width=INPUT_WIDTH, label_width=24, </p>
        <p>➥ shift=1, label_columns=['temp', 'traffic_volume'])</p>
        <p>By now you should be comfortable building models with Keras, so defining the CNN</p>
        <p>architecture as a multi-output model should be straightforward. Again, we'll use the
            Sequential model, in which we'll stack a Conv1D layer, followed by a Dense layer, allowing the network
            to
            learn on a set of filtered features. The output layer will have two neurons, since we're forecasting
            both
            the temperature and the traffic volume. </p>
        <p>Next we'll fit the model and store its performance metrics. </p>
        <p>mo_cnn_model = Sequential([</p>
        <p>Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'), </p>
        <p>Dense(units=32, activation='relu'), </p>
        <p>Dense(units=2)</p>
        <p>])</p>
        <p>history = compile_and_fit(mo_cnn_model, wide_mo_conv_window)</p>
        <p>mo_val_performance = {}</p>
        <p>mo_performance = {}</p>
        <p>mo_val_performance['CNN'] = mo_cnn_model.evaluate(wide_mo_conv_window.val)</p>
        <p>mo_performance['CNN'] = mo_cnn_model.evaluate(wide_mo_conv_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>We can also combine the CNN architecture with the LSTM architecture as done previously.
            We'll simply replace the intermediate Dense layer with an LSTM layer, fit the model, and store its
            metrics.
        </p>
        <p>mo_cnn_lstm_model = Sequential([</p>
        <p>Conv1D(filters=32, kernel_size=(KERNEL_WIDTH,), activation='relu'), </p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(units=2)</p>
        <p>])</p>
        <p>history = compile_and_fit(mo_cnn_lstm_model, wide_mo_conv_window)</p>
        <p>mo_val_performance['CNN + LSTM'] = </p>
        <p>➥ mo_cnn_model.evaluate(wide_mo_conv_window.val)</p>
        <p>mo_performance['CNN + LSTM'] = </p>
        <p>➥ mo_cnn_model.evaluate(wide_mo_conv_window.test, verbose=0)</p>
        <p>As usual, we'll compare the performance of the new models with the previous multi-output
            models in figure 16.8. You'll notice that the CNN, and the combination of</p>
        <p><a id="calibre_link-142"></a><img class="calibre2" src="images/000160.png" alt="Image 130" /></p>
        <p> <i><b>16.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>317</b></p>
        <p>0.06</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.05</p>
        <p>0.048 0.047</p>
        <p>0.04</p>
        <p>0.039</p>
        <p>0.036</p>
        <p>0.03</p>
        <p>0.023</p>
        <p>Mean absolute error</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.02</p>
        <p>0.017</p>
        <p>0.017</p>
        <p>0.017</p>
        <p>0.01</p>
        <p>0.00</p>
        <p>Baseline - Last</p>
        <p>Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>CNN</p>
        <p>CNN + LSTM</p>
        <p>Models</p>
        <p>Figure 16.8</p>
        <p>The MAE of all multi-output models built so far. As you can see, the CNN and the combination
            of CNN and LSTM did not result in improvements over the LSTM model. </p>
        <p>CNN and LSTM, did not result in an improvement over the LSTM. In fact, all three models
            achieve the same MAE. </p>
        <p>Explaining this behavior is hard, as deep learning models are black boxes, mean-</p>
        <p>ing that they are hard to interpret. While they can be very performant, the tradeoff lies in
            their explicability. Methods to interpret neural network models do exist, but they are outside of the
            scope
            of this book. If you want to learn more, take a look at Christof Molnar's book, <i
                class="calibre3">Interpretable Machine Learning</i>, <i>Second Edition</i> <a
                href="https://christophm.github.io/interpretable-ml-book/">(https://christophm</a></p>
        <p><a href="https://christophm.github.io/interpretable-ml-book/">.github.io/interpretable-ml-book/). </a>
        </p>
        <p> <i><b>16.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we examined the architecture of the CNN. We observed how the convolution
            operation is used in the network and how it effectively filters the input sequence with the use of a
            kernel.
            We then implemented the CNN architecture and</p>
        <p>combined it with the LSTM architecture to produce two new single-step models, multi-step
            models, and multi-output models. </p>
        <p><a id="calibre_link-143"></a><b>318</b></p>
        <p>CHAPTER 16</p>
        <p> <i><b>Filtering a time series with CNN</b></i></p>
        <p>In the case of the single-step models, using a CNN did not improve the results. In fact, it
            performed worse than the LSTM alone. For the multi-step models, we observed a slight performance boost
            and
            obtained the best-performing multi-step model with the combination of a CNN and an LSTM. In the case of
            the
            multi-output model, the use of a CNN resulted in constant performance, so we have a tie between the CNN,
            the
            LSTM, and the combination of CNN and LSTM. Thus, we can see that a CNN</p>
        <p>does not necessarily result in the best-performing model. In one situation it did, in
            another it did not, and in another there was no difference. </p>
        <p>It is important to consider the CNN architecture as a tool in your toolset when it comes to
            modeling with deep learning. Models will perform differently depending on the dataset and the
            forecasting
            goal. The key lies in windowing your data correctly, as is done by the DataWindow class, and in
            following a
            testing methodology, as we have done by keeping the training set, validation set, and testing set
            constant
            and evaluating all models using the MAE against baseline models. </p>
        <p>The last deep learning architecture that we are going to explore specifically con-cerns the
            multi-step models. Up until now, all multi-step models have output predictions for the next 24 hours in
            a
            single shot. However, it is possible to gradually predict the next 24 hours and feed a past prediction
            back
            into the model to output the next prediction. This is especially done with the LSTM architecture,
            resulting
            in an <i>autoregressive LSTM</i> (ARLSTM). This will be the subject of the next
            chapter.
        </p>
        <p> <i><b>16.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>In the previous chapter's exercises, you built LSTM models. Now you'll experiment with a CNN
            and a combination of CNN and LSTM to see if you can gain in performance. The solutions to these
            exercises
            are available on GitHub:<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH16">
                https://github.com/</a></p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH16">marcopeix/TimeSeriesForecastingInPython/tree/master/CH16</a>.
        </p>
        <p> For the single-step model:</p>
        <p>a</p>
        <p>Build a CNN model. Set the kernel width to 3. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate the model using the mean absolute error (MAE) and store the MAE. </p>
        <p>d</p>
        <p>Build a CNN + LSTM model. </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p>f</p>
        <p>Evaluate the model using the MAE and store the MAE. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p> For the multi-step model:</p>
        <p>a</p>
        <p>Build a CNN model. Set the kernel width to 3. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate the model using the MAE and store the MAE. </p>
        <p>d</p>
        <p>Build a CNN+LSTM model. </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p><a id="calibre_link-307"></a> <i><b>Summary</b></i></p>
        <p><b>319</b></p>
        <p>f</p>
        <p>Evaluate the model using the MAE and store the MAE. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p> Multi-output model:</p>
        <p>a</p>
        <p>Build a CNN model. Set the kernel width to 3. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate the model using the MAE and store the MAE. </p>
        <p>d</p>
        <p>Build a CNN + LSTM model. </p>
        <p>e</p>
        <p>Plot its predictions. </p>
        <p>f</p>
        <p>Evaluate the model using the MAE and store the MAE. </p>
        <p>g</p>
        <p>Which model performs best? </p>
        <p>As always, this is an occasion to experiment. You can explore the following:</p>
        <p> Add more layers. </p>
        <p> Change the number of units. </p>
        <p> Pad the sequence instead of increasing the input length. This is done in the Conv1D layer
            using the parameter padding="same". In that case, your input sequence must have a length of 24. </p>
        <p> Use different layer initializers. </p>
        <p> <i><b>Summary</b></i></p>
        <p> The convolutional neural network (CNN) is a deep learning architecture that</p>
        <p>makes use of the convolution operation. </p>
        <p> The convolution operation is performed between a kernel and the feature space. It is
            simply the dot product between the kernel and the feature vector. </p>
        <p> Running a convolution operation results in an output sequence that is shorter</p>
        <p>than the input sequence. Running many convolutions can therefore decrease</p>
        <p>the output length quickly. Padding can be used to prevent that. </p>
        <p> In time series forecasting, the convolution is performed in one dimension only: the
            temporal dimension. </p>
        <p> The CNN is just another model in your toolbox and may not always be the best-</p>
        <p>performing model. Make sure you window your data correctly with DataWindow, </p>
        <p>and keep your testing methodology valid by keeping each set of data constant, </p>
        <p>building baseline models, and evaluating all models with the same error metric. </p>
        <p><a id="calibre_link-20"></a> <i>Using predictions to</i></p>
        <p> <i>make more predictions</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Examining the autoregressive LSTM (ARLSTM) </p>
        <p>architecture</p>
        <p> Discovering the caveat of the ARLSTM</p>
        <p> Implementing an ARLSTM</p>
        <p>In the last chapter, we examined and built a convolutional neural network (CNN). </p>
        <p>We even combined it with the LSTM architecture to test whether we could outper-</p>
        <p>form the LSTM models. The results were mixed, as the CNN models performed worse as
            single-step models, performed best as multi-step models, and performed</p>
        <p>equally well as multi-output models. </p>
        <p>Now we'll focus entirely on the multi-step models, as all of them output the entire sequence
            of predictions in a single shot. We're going to modify that behavior and gradually output the prediction
            sequence, using past predictions to make new predictions. That way, the model will create rolling
            forecasts,
            but using its own predictions to inform the output. </p>
        <p>This architecture is commonly used with LSTM and is called <i>autoregressive LSTM</i>
        </p>
        <p>(ARLSTM). In this chapter, we'll first explore the general architecture of the ARLSTM model,
            and then we'll build it in Keras to see if we can build a new top-performing multi-step model. </p>
        <p><b>320</b></p>
        <p><a id="calibre_link-144"></a> <i><b>17.1</b></i></p>
        <p> <i><b>Examining the ARLSTM architecture</b></i></p>
        <p><b>321</b></p>
        <p> <i><b>17.1</b></i></p>
        <p> <i><b>Examining the ARLSTM architecture</b></i></p>
        <p>We have built many multi-step models that all output predictions for traffic volume in the
            next 24 hours. Each model has generated the entire prediction sequence in a single shot, meaning that we
            get
            24 values from the model right away. </p>
        <p>For illustration purposes, let's consider a simple model with only an LSTM layer. </p>
        <p>Figure 17.1 shows the general architecture of the multi-step models we have built so far.
            Each of them had inputs coming in, passing through a layer, whether it is LSTM, Dense, or Conv1D, and
            resulting in a sequence of 24 values. This type of architecture forces an output of 24 values. </p>
        <p>LSTM</p>
        <p>Inputs</p>
        <p> <i>t</i></p>
        <p> <i>t</i></p>
        <p>layer</p>
        <p>24</p>
        <p> <i>t</i> 25</p>
        <p>46</p>
        <p> <i>t</i> 47</p>
        <p>Figure 17.1</p>
        <p>Illustrating a single-shot multi-step model with an LSTM layer. </p>
        <p>All multi-step models that we have built have had this general architecture. </p>
        <p>The LSTM layer can easily be replaced by a CNN layer or a dense layer. </p>
        <p>But what if we want a longer sequence? Or a shorter sequence? What if we wish to forecast
            the next 8 hours only, or forecast the next 48 hours? In that case, we must redo our data windows and
            retrain the models, which might represent quite a bit</p>
        <p>of work. </p>
        <p>Instead, we can opt for an autoregressive deep learning model. As you can see</p>
        <p>in figure 17.2, each prediction is sent back into the model, allowing it to generate the
            next prediction. This process is repeated until we obtain a sequence of the desired length. </p>
        <p>LSTM</p>
        <p>Inputs</p>
        <p> <i>t</i></p>
        <p> <i>t</i></p>
        <p>layer</p>
        <p>24</p>
        <p> <i>t</i> 25</p>
        <p>46</p>
        <p> <i>t</i> 47</p>
        <p>Figure 17.2</p>
        <p>An autoregressive LSTM model. This model returns a first </p>
        <p>prediction at <i><b>t</b></i> 24, and it is sent back into
            the model to generate the prediction at <i><b>t</b></i> 25. This
            process
            is repeated until the desired output length is obtained. </p>
        <p>Again, an LSTM layer is shown, but it could be a CNN or a dense layer. </p>
        <p><a id="calibre_link-145"></a><b>322</b></p>
        <p>CHAPTER 17</p>
        <p> <i><b>Using predictions to make more predictions</b></i>
        </p>
        <p>You can see how easy it becomes to generate any sequence length using an autoregressive deep
            learning architecture. This approach has the added advantage of allowing us to forecast time series with
            different scales, such as hours, days, or months, while avoiding having to retrain a new model. This is
            the
            type of architecture built by Google DeepMind to create WaveNet <a
                href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">(https://deepmind.com/blog/article/wavenet-generative-</a>
        </p>
        <p><a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">model-raw-audio), </a>a
            model that generates raw audio sequences. In the context of time series, DeepAR (<a
                href="http://mng.bz/GEoV">http://mng.bz/GEoV) </a>is a methodology that also uses an autoregressive
            recurrent neural network to achieve state-of-the-art results. </p>
        <p>Nevertheless, autoregressive deep learning models come with a major caveat, which is the
            accumulation of error. We have forecast many time series, and we know that there is always some
            discrepancy
            between our predictions and the actual values. That error accumulates as it is fed back into the model,
            meaning that later predictions will have a larger error than earlier predictions. Thus, while the
            autoregressive deep learning architecture seems powerful, it might not be the best solution for a
            particular
            problem. Hence the importance of using a rigorous testing protocol, which is really what we have
            developed
            since chapter 13. </p>
        <p>Still, it is good to have this model in your toolbox of time series forecasting methods. In
            the next section, we'll code an autoregressive LSTM model to produce forecasts for the next 24 hours.
            We'll
            compare its performance to that of our previous multi-step models. </p>
        <p> <i><b>17.2</b></i></p>
        <p> <i><b>Building an autoregressive LSTM model</b></i></p>
        <p>We are now ready to code our own autoregressive deep learning model in Keras. Specifically,
            we'll code an ARLSTM model, since our experiments have shown that the LSTM model achieves the best
            performance of the multi-step models. Thus we'll try to further improve this model by making it
            autoregressive. </p>
        <p>As always, make sure that you have the DataWindow class and the compile_and_fit</p>
        <p>function accessible in your notebook or Python script. They are the same versions that we
            developed in chapter 13. </p>
        <p>NOTE</p>
        <p>At any time, feel free to consult the source code for this chapter on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17">tree/master/CH17.
            </a></p>
        <p>The first step is to read the training, validation, and test sets. </p>
        <p>train_df = pd.read_csv('../data/train.csv', index_col=0)</p>
        <p>val_df = pd.read_csv('../data/val.csv', index_col=0)</p>
        <p>test_df = pd.read_csv('../data/test.csv', index_col=0)</p>
        <p>Next, we'll define our window of data. In this case, we'll reuse the window of data we used
            for the LSTM model. The input and label sequences will each have 24 timesteps. </p>
        <p><a id="calibre_link-210"></a> <i><b>17.2</b></i></p>
        <p> <i><b>Building an autoregressive LSTM model</b></i></p>
        <p><b>323</b></p>
        <p>We'll specify a shift of 24 so that the model outputs 24 predictions. Our target remains the
            traffic volume. </p>
        <p>multi_window = DataWindow(input_width=24, label_width=24, shift=24, </p>
        <p>➥ label_columns=['traffic_volume'])</p>
        <p>Now we'll wrap our model in a class called AutoRegressive, which inherits from the Model
            class in Keras. This is what allows us to access inputs and outputs. That way, we'll be able to specify
            that
            the output should become an input at each prediction step. </p>
        <p>We'll start by defining the __init__ function in our AutoRegressive class. This function
            takes three parameters:</p>
        <p> self&mdash;References the instance of the AutoRegressive class. </p>
        <p> units&mdash;Represents the number of neurons in a layer. </p>
        <p> out_steps&mdash;Represents the length of the prediction sequence. In this case, it</p>
        <p>is 24. </p>
        <p>Then we'll make use of three different Keras layers: the Dense layer, the RNN layer, and the
            LSTMCell layer. The LSTMCell layer is a lower-level layer than the LSTM layer. It allows us to access
            more
            granular information, such as state and predictions, which we can then manipulate to feed an output back
            into the model as an input. As for the RNN layer, this is used to train the LSTMCell layer on the input
            data. Its output is then passed through the Dense layer to generate a prediction. This is the complete
            __init__ function:</p>
        <p><b>The number of neurons in a layer is defined </b></p>
        <p><b>by units, and the length of the prediction </b></p>
        <p>class AutoRegressive(Model):</p>
        <p><b>sequence is defined by out_steps. </b></p>
        <p>def __init__(self, units, out_steps): </p>
        <p>super().__init__()</p>
        <p></p>
        <p><b>The</b></p>
        <p>self.out_steps = out_steps</p>
        <p><b>The LSTMCell layer is a lower-level class </b></p>
        <p><b>that allows us to access more granular </b></p>
        <p></p>
        <p><b>prediction</b></p>
        <p>self.units = units</p>
        <p><b>information, such as state and outputs. </b></p>
        <p><b>comes fr </b> </p>
        <p><b>om</b></p>
        <p>self.lstm_cell = LSTMCell(units) </p>
        <p><b>this </b></p>
        <p></p>
        <p><b>Dense</b></p>
        <p>self.lstm_rnn = RNN(self.lstm_cell, return_state=True) </p>
        <p><b>lay </b> </p>
        <p><b>er. </b></p>
        <p>self.dense = Dense(train_df.shape[1]) </p>
        <p><b>The RNN layer wraps the</b></p>
        <p><b>LSTMCell layer so it is easier</b></p>
        <p><b>to train the LSTM on the data. </b></p>
        <p>With the initialization done, the next step is to define a function that outputs the very
            first prediction. Since this is an autoregressive model, that prediction is then fed back into the model
            as
            an input to generate the next prediction. We must therefore have a method to capture that very first
            forecast before entering the autoregressive loop. </p>
        <p>Thus, we'll define the warmup function, which replicates a single-step LSTM model. </p>
        <p>We'll simply pass the inputs into the lstm_rnn layer, get the prediction from the Dense
            layer, and return both the prediction and the state. </p>
        <p><a id="calibre_link-220"></a><b>324</b></p>
        <p>CHAPTER 17</p>
        <p> <i><b>Using predictions to make more predictions</b></i>
        </p>
        <p><b>Pass the inputs through the LSTM layer. </b></p>
        <p>def warmup(self, inputs):</p>
        <p><b>The output is sent to the Dense layer. </b></p>
        <p>x, *state = self.lstm_rnn(inputs) </p>
        <p>prediction = self.dense(x) </p>
        <p><b>Get a prediction from </b></p>
        <p><b>the Dense layer. </b></p>
        <p>return prediction, state</p>
        <p>Now that we have a way to capture the first prediction, we can define the call function,
            which will run a loop to generate the sequence of predictions with a length of out_steps. Note that the
            function must be named call because it is called implicitly by Keras; naming it differently would result
            in
            an error. </p>
        <p>Since we are using the LSTMCell class, which is a low-level class, we must manually pass in
            the previous state. Once the loop is finished, we stack our predictions and make sure they have the
            right
            output shape using the transpose method. </p>
        <p><b>Initialize an empty list to </b></p>
        <p>def call(self, inputs, training=None):</p>
        <p><b>collect all the predictions. </b></p>
        <p>predictions = [] </p>
        <p>prediction, state = self.warmup(inputs) </p>
        <p><b>The first prediction is obtained </b></p>
        <p><b>from the warmup function. </b></p>
        <p>predictions.append(prediction) </p>
        <p><b>The</b></p>
        <p><b>Place the first prediction in </b></p>
        <p><b>prediction </b> for n in range(1, self.out_steps):</p>
        <p><b>the list of predictions. </b></p>
        <p><b>becomes </b> x = prediction </p>
        <p><b>an input </b> x, state = self.lstm_cell(x, states=state,
            training=training) <b>for the</b></p>
        <p><b>next one. </b></p>
        <p>prediction = self.dense(x) </p>
        <p><b>Generate a new prediction using </b></p>
        <p>predictions.append(prediction)</p>
        <p><b>the previous one as an input. </b></p>
        <p>predictions = tf.stack(predictions) </p>
        <p>predictions = tf.transpose(predictions, [1, 0, 2]) </p>
        <p><b>Use transpose to </b></p>
        <p><b>get the needed </b></p>
        <p>return predictions</p>
        <p><b>shape of (batch, </b></p>
        <p><b>time, features). </b></p>
        <p><b>Stack all the predictions. At this point, we </b></p>
        <p><b>have a shape (time, batch, features). It must </b></p>
        <p><b>be changed to (batch, time, features). </b></p>
        <p>The complete class is shown in the following listing. </p>
        <p>Listing 17.1</p>
        <p>Defining a class to implement an ARLSTM model</p>
        <p>class AutoRegressive(Model):</p>
        <p>def __init__(self, units, out_steps):</p>
        <p>super().__init__()</p>
        <p>self.out_steps = out_steps</p>
        <p>self.units = units</p>
        <p>self.lstm_cell = LSTMCell(units)</p>
        <p>self.lstm_rnn = RNN(self.lstm_cell, return_state=True)</p>
        <p>self.dense = Dense(train_df.shape[1])</p>
        <p></p>
        <p><a id="calibre_link-284"></a> <i><b>17.2</b></i></p>
        <p> <i><b>Building an autoregressive LSTM model</b></i></p>
        <p><b>325</b></p>
        <p>def warmup(self, inputs):</p>
        <p>x, *state = self.lstm_rnn(inputs)</p>
        <p>prediction = self.dense(x)</p>
        <p></p>
        <p>return prediction, state</p>
        <p></p>
        <p>def call(self, inputs, training=None):</p>
        <p>predictions = []</p>
        <p>prediction, state = self.warmup(inputs)</p>
        <p></p>
        <p>predictions.append(prediction)</p>
        <p></p>
        <p>for n in range(1, self.out_steps):</p>
        <p>x = prediction</p>
        <p>x, state = self.lstm_cell(x, states=state, training=training)</p>
        <p></p>
        <p>prediction = self.dense(x)</p>
        <p>predictions.append(prediction)</p>
        <p></p>
        <p>predictions = tf.stack(predictions)</p>
        <p>predictions = tf.transpose(predictions, [1, 0, 2])</p>
        <p></p>
        <p>return predictions</p>
        <p>We have now defined our AutoRegressive class, which implements an autoregressive LSTM model.
            We can use it and train a model on our data. We'll initialize it with 32</p>
        <p>units and an output sequence length of 24 timesteps, since the objective of the multi-step
            model is to forecast the next 24 hours. </p>
        <p>AR_LSTM = AutoRegressive(units=32, out_steps=24)</p>
        <p>Next, we'll compile the model, train it, and store its performance metrics. </p>
        <p>history = compile_and_fit(AR_LSTM, multi_window)</p>
        <p>ms_val_performance = {}</p>
        <p>ms_performance = {}</p>
        <p>ms_val_performance['AR - LSTM'] = AR_LSTM.evaluate(multi_window.val)</p>
        <p>ms_performance['AR - LSTM'] = AR_LSTM.evaluate(multi_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>We can visualize the predictions of our model against the actual values by using the plot
            method from our DataWindow class. </p>
        <p>multi_window.plot(AR_LSTM)</p>
        <p>In figure 17.3 many predictions are very close to the actual values, sometimes even
            overlapping them. This indicates that we have a fairly accurate model. </p>
        <p><a id="calibre_link-188"></a><img class="calibre2" src="images/000059.png" alt="Image 131" /></p>
        <p><b>326</b></p>
        <p>CHAPTER 17</p>
        <p> <i><b>Using predictions to make more predictions</b></i>
        </p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>0.2</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>] 0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled 0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.8</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>traffic_volume [scaled] 0.0</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 17.3</p>
        <p>Forecasting traffic volume for the next 24 hours using an ARLSTM model. Many predictions
            (shown as crosses) overlap the actual values (shown as squares), which means that we have a fairly
            accurate
            model. </p>
        <p>This visual inspection is not sufficient to determine whether we have a new top-performing
            model, so we'll display its MAE against that of all previous multi-step models. The result is shown in
            figure 17.4, which shows that our autoregressive LSTM model achieves an MAE of 0.063 on the validation
            set
            and 0.049 on the test</p>
        <p>set. This is a better score than the CNN, and the CNN + LSTM models, as well as the simple
            LSTM model. Thus, the ARLSTM model becomes the top-performing multi-step model. </p>
        <p>Always keep in mind that the performance of each model depends on the problem</p>
        <p>at stake. The takeaway here is not that the ARLSTM is always the best model, but that it is
            the best-performing model for this situation. For another problem, you might find another champion
            model. If
            you have been completing the exercises since chapter 13, you can already see this happening. Keep in
            mind
            that each model we have built since chapter 13 is meant to be another tool in your toolbox to help you
            maxi-mize the chances of solving a time series forecasting problem. </p>
        <p><a id="calibre_link-146"></a><img class="calibre2" src="images/000041.png" alt="Image 132" /></p>
        <p> <i><b>17.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>327</b></p>
        <p>0.40</p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.352</p>
        <p>0.35</p>
        <p>0.347</p>
        <p>0.347 0.341</p>
        <p>0.30</p>
        <p>0.25</p>
        <p>0.20</p>
        <p>Mean absolute error 0.15</p>
        <p>0.10</p>
        <p>0.088</p>
        <p>0.078</p>
        <p>0.076</p>
        <p>0.078</p>
        <p>0.07</p>
        <p>0.069</p>
        <p>0.064</p>
        <p>0.063</p>
        <p>0.063</p>
        <p>0.058</p>
        <p>0.055</p>
        <p>0.049</p>
        <p>0.05</p>
        <p>0.00</p>
        <p>Baseline - Last Baseline - Repeat Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>CNN</p>
        <p>CNN + LSTM AR - LSTM</p>
        <p>Models</p>
        <p>Figure 17.4</p>
        <p>The MAE of all our multi-step models on the validation and test sets. The ARLSTM model
            achieves a lower MAE than the CNN and the CNN + LSTM models and the simple LSTM model. </p>
        <p> <i><b>17.3</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>This is a rather short chapter, as it builds on concepts that we have already covered, such
            as the LSTM architecture and data windowing. </p>
        <p>The autoregressive LSTM model outperformed the simple LSTM multi-step model</p>
        <p>in our example, and it performed better than a CNN model. Again, this does not mean that an
            ARLSTM model will always outperform a CNN model or a simple LSTM</p>
        <p>model. Each problem is unique, and a different architecture might result in the best
            performance for a different problem. The important thing is that you now have a wide array of models you
            can
            test and adapt to each problem in order to find the best solution possible. </p>
        <p>This brings the deep learning part of the book almost to a conclusion. In the next chapter,
            we'll apply our knowledge of deep learning methods for time series forecasting in a capstone project. As
            before, a problem and dataset will be provided, and we must produce a forecasting model to solve the
            problem. </p>
        <p><a id="calibre_link-147"></a><b>328</b></p>
        <p>CHAPTER 17</p>
        <p> <i><b>Using predictions to make more predictions</b></i>
        </p>
        <p> <i><b>17.4</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>In the exercises since chapter 13, we have built many models to forecast the air quality in
            Beijing using all three types of models (single-step, multi-step, and multi-output). </p>
        <p>Now we'll build one last multi-step model using an ARLSTM model. The solution can be found
            on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH17">tree/master/CH17.
            </a></p>
        <p> For the multi-step model:</p>
        <p>a</p>
        <p>Build an ARLSTM model. </p>
        <p>b</p>
        <p>Plot its predictions. </p>
        <p>c</p>
        <p>Evaluate the model using the mean absolute error (MAE) and store the MAE</p>
        <p>for comparison. </p>
        <p>d</p>
        <p>Is the ARLSTM model the champion model? </p>
        <p>Of course, feel free to experiment further. For example, you can vary the number of units to
            see how it impacts the model's performance. </p>
        <p> <i><b>Summary</b></i></p>
        <p> The autoregressive architecture in deep learning has given birth to state-of-the-art
            models, such as WaveNet and DeepAR. </p>
        <p> An autoregressive deep learning model generates a sequence of predictions, but each
            prediction is fed back into the model as an input. </p>
        <p> A caveat regarding autoregressive deep learning models is that errors accumu-</p>
        <p>late as the length of the sequence increases. Therefore, an early bad prediction can have a
            large effect on a late prediction. </p>
        <p><a id="calibre_link-21"></a> <i>Capstone: Forecasting</i></p>
        <p> <i>the electric power</i></p>
        <p> <i>consumption of a household</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Developing deep learning models to predict a </p>
        <p>household's electric power consumption</p>
        <p> Comparing various multi-step deep learning </p>
        <p>models</p>
        <p> Evaluating the mean absolute error and selecting </p>
        <p>the champion model</p>
        <p>Congratulations on making it this far! In chapters 12 to 17, we dove headfirst into deep
            learning for time series forecasting. You learned that statistical models become inefficient or unusable
            when you have large datasets, which usually means more than 10,000 data points, with many features. We
            must
            then revert to using deep learning models, which can leverage all the available information while
            remaining
            computationally efficient, to produce forecasting models. </p>
        <p>Just as we had to design a new forecasting procedure in chapter 6 when we started modeling
            time series with the ARMA( <i>p</i>, <i>q</i>) model, modeling with
            deep
            learning techniques required us to use yet another modeling procedure: creating win-</p>
        <p>dows of data with the DataWindow class. This class plays a vital role in modeling with deep
            learning, as it allows us to format our data appropriately to create a set of inputs and labels for our
            models, as shown in figure 18.1. </p>
        <p><b>329</b></p>
        <p><a id="calibre_link-148"></a><b>330</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>Input</p>
        <p>Label</p>
        <p> <i>t </i>= 0</p>
        <p> <i>t </i>= 1</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 22</p>
        <p> <i>t </i>= 23</p>
        <p> <i>t </i>= 24</p>
        <p> <i>t </i>= 25</p>
        <p> <i>t </i>= ... </p>
        <p> <i>t </i>= 46</p>
        <p> <i>t </i>= 47</p>
        <p>Data window</p>
        <p>Figure 18.1</p>
        <p>Example of a data window. This data window has 24 timesteps as input and 24 </p>
        <p>timesteps as output. The model will then use 24 hours of input to generate 24 hours of
            predictions. </p>
        <p>The total length of the data window is the sum of the lengths of inputs and labels. In this
            case, we have a total length of 48 timesteps. </p>
        <p>This data windowing step allows us to produce a wide variety of models, from simple linear
            models to deep neural networks, long short-term memory (LSTM) networks, and convolutional neural
            networks
            (CNNs). Furthermore, data windowing can be used for different scenarios, allowing to us create
            single-step
            models where we predict only the next timestep, multi-step models where we predict a sequence of future
            steps, and multi-output models where we predict more than one target variable. </p>
        <p>Having worked with deep learning in the last several chapters, it's time to apply our
            knowledge to a capstone project. In this chapter, we'll walk through the steps of a forecasting project
            using deep learning models. We'll first look at the project and describe the data that we'll use. Then
            we'll
            cover the data wrangling and preprocessing steps. Although those steps do not relate directly to time
            series
            forecasting, they are crucial steps in any machine learning project. We'll then focus on the modeling
            steps,
            where we'll try a set of deep learning models to uncover the best performer. </p>
        <p> <i><b>18.1</b></i></p>
        <p> <i><b>Understanding the capstone project</b></i></p>
        <p>For this project, we'll use a dataset that tracks the electric power consumption of a
            household. The “Individual household electric power consumption” dataset is openly available from the UC
            Irvine Machine Learning Repository: <a
                href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption">https://archive.ics.uci</a>
        </p>
        <p><a
                href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption">.edu/ml/datasets/Individual+household+electric+power+consumption</a>.
        </p>
        <p>Forecasting electric energy consumption is a common task with worldwide applica-</p>
        <p>tions. In developing countries, it can help in planning the construction of power grids. In
            countries where the grid is already developed, forecasting energy consumption ensures that the grid can
            provide enough energy to power all households efficiently. With accurate forecasting models, energy
            companies can better plan the load on the grid, ensuring that they are producing enough energy during
            peak
            times or have sufficient energy reserves to meet the demand. Also, they can avoid producing too much
            electricity, which, if it's not stored, could cause an imbalance in the grid, posing a risk of
            disconnection. Thus, forecasting electric energy consumption is an important problem that has
            consequences
            in our daily lives. </p>
        <p>To develop our forecasting model, we'll use the power consumption dataset men-</p>
        <p>tioned previously, which contains the electric consumption for a house in Sceaux, </p>
        <p><a id="calibre_link-149"></a> <i><b>18.1</b></i></p>
        <p> <i><b>Understanding the capstone project</b></i></p>
        <p><b>331</b></p>
        <p>France, between December 2006 and November 2010. The data spans 47 months and</p>
        <p>was recorded at every minute, meaning that we have more than two million data points. </p>
        <p>The dataset contains a total of nine columns, listed in table 18.1. The main target is the
            global active power, as it represents the real power used in a circuit. This is the component that is
            used
            by the appliances. Reactive power, on the other hand, moves between the source and the load of a
            circuit, so
            it does not produce any useful work. </p>
        <p>Table 18.1</p>
        <p>Description of the columns in the dataset</p>
        <p>Column name</p>
        <p>Description</p>
        <p>Date </p>
        <p>Date in the following format: dd/mm/yyyy</p>
        <p>Time</p>
        <p>Time in the following format: hh:mm:ss</p>
        <p>Global_active_power</p>
        <p>The global active power in kilowatts</p>
        <p>Global_reactive_power</p>
        <p>The global reactive power in kilowatts</p>
        <p>Voltage</p>
        <p>Voltage in volts </p>
        <p>Global_intensity</p>
        <p>The current intensity in amperes</p>
        <p>Sub_metering_1</p>
        <p>Energy consumed in the kitchen by a dishwasher, oven, and microwave in </p>
        <p>watt-hours</p>
        <p>Sub_metering_2</p>
        <p>Energy consumed in the laundry room by a washing machine, tumble-dryer, </p>
        <p>refrigerator, and light in watt-hours</p>
        <p>Sub_metering_3</p>
        <p>Energy consumed by a water heater and air conditioner in watt-hours</p>
        <p>This dataset does not include any weather information, which could potentially be a strong
            predictor of energy consumption. We can safely expect that during hot summer days, the air conditioning
            unit
            will function for longer, thus requiring more electrical power. The same can be expected during cold
            winter
            days, because heating a house requires a large amount of energy. This data is not available here, but in
            a
            professional setting we could request this type of data to augment our dataset and potentially produce
            better models. </p>
        <p>Now that you have a general understanding of the problem and the dataset, let's</p>
        <p>define the objective of this project and the steps we'll take to achieve it. </p>
        <p> <i><b>18.1.1 Objective of this capstone project</b></i>
        </p>
        <p>The objective of this capstone project is to create a model that can forecast the next 24
            hours of global active power. If you feel confident, this objective should be sufficient for you to
            download
            the dataset, work it on your own, and compare your process to the one presented in this chapter. </p>
        <p>Otherwise, here are the steps that need to be done:</p>
        <p>1</p>
        <p>Data wrangling and preprocessing. This step is optional. It is not directly linked to time
            series forecasting, but it is an important step in any machine</p>
        <p><a id="calibre_link-213"></a><b>332</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>learning project. You can safely skip this step and start at step 2 with a clean dataset:
        </p>
        <p>a</p>
        <p>Calculate the number of missing values. </p>
        <p>b</p>
        <p>Impute the missing values. </p>
        <p>c</p>
        <p>Express each variable as a numerical value (all data is originally stored as strings). </p>
        <p>d</p>
        <p>Combine the Date and Time columns into a DateTime object. </p>
        <p>e</p>
        <p>Determine whether the data sampled at every minute is usable for forecast-</p>
        <p>ing. </p>
        <p>f</p>
        <p>Resample the data by hour. </p>
        <p>g</p>
        <p>Remove any incomplete hours. </p>
        <p>2</p>
        <p>Feature engineering:</p>
        <p>a</p>
        <p>Identify any seasonality. </p>
        <p>b</p>
        <p>Encode the time with a sine and cosine transformation. </p>
        <p>c</p>
        <p>Scale the data. </p>
        <p>3</p>
        <p>Split the data:</p>
        <p>a</p>
        <p>Make a 70:20:10 split to create training, validation, and test sets. </p>
        <p>4</p>
        <p>Prepare for deep learning modeling:</p>
        <p>a</p>
        <p>Implement the DataWindow class. </p>
        <p>b</p>
        <p>Define the compile_and_fit function. </p>
        <p>c</p>
        <p>Create a dictionary of column indices and column names. </p>
        <p>5</p>
        <p>Model with deep learning:</p>
        <p>a</p>
        <p>Train at least one baseline model. </p>
        <p>b</p>
        <p>Train a linear model. </p>
        <p>c</p>
        <p>Train a deep neural network. </p>
        <p>d</p>
        <p>Train an LSTM. </p>
        <p>e</p>
        <p>Train a CNN. </p>
        <p>f</p>
        <p>Train a combination of LSTM and CNN. </p>
        <p>g</p>
        <p>Train an autoregressive LSTM. </p>
        <p>h</p>
        <p>Select the best-performing model. </p>
        <p>You now have all the steps required to successfully complete this capstone project. I highly
            recommend that you try it on your own first, as that will reveal what you have mastered and what you
            need to
            review. At any point, you can refer to the following sections for a detailed walkthrough of each step.
        </p>
        <p>The entire solution is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18">https://github.com/marcopeix/</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH18">TimeSeriesForecastingInPython/tree/master/CH18</a>.
            Note that the data files were too large to be included in the repository, so you'll need to download the
            dataset separately. Good luck! </p>
        <p><a id="calibre_link-150"></a> <i><b>18.2</b></i></p>
        <p> <i><b>Data wrangling and preprocessing</b></i></p>
        <p><b>333</b></p>
        <p> <i><b>18.2</b></i></p>
        <p> <i><b>Data wrangling and preprocessing</b></i></p>
        <p>Data wrangling is the process of transforming data into a form that is easily usable for
            modeling. This step usually involves exploring missing data, filling in blank values, and ensuring that
            the
            data has the right type, meaning that numbers are numerical values and not strings. This is a complex
            step,
            and it's probably the most vital one in any machine learning project. Having poor quality data at the
            start
            of a forecasting project is a guarantee that you'll have poor quality forecasts. You can skip this
            section
            of the chapter if you wish to focus solely on time series forecasting, but I highly recommend that you
            go
            through it, as it will really help you become comfortable with the dataset. </p>
        <p>NOTE</p>
        <p>If you have not done so already, you can download the “Individual house-</p>
        <p>hold electric power consumption” dataset from the UC Irvine Machine Learn-</p>
        <p>ing Repository: <a
                href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption">https://archive.ics.uci.edu/ml/datasets/Individual+household</a>
        </p>
        <p><a href="https://archive.ics.uci.edu/ml/datasets/Individual+household+electric+power+consumption">+electric+power+consumption.
            </a></p>
        <p>To perform this data wrangling, you can start by importing libraries that will be useful for
            data manipulation and visualization into a Python script or Jupyter Notebook. </p>
        <p>import datetime</p>
        <p>import numpy as np</p>
        <p>import pandas as pd</p>
        <p>import tensorflow as tf</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>import warnings</p>
        <p>warnings.filterwarnings('ignore')</p>
        <p>Whenever numpy and TensorFlow are used, I like to set a random seed to ensure that the
            results can be reproduced. If you do not set a seed, your results might vary, and if you set a seed
            that's
            different than mine, your results will differ from those shown here. </p>
        <p>tf.random.set_seed(42)</p>
        <p>np.random.seed(42)</p>
        <p>The next step is to read the data file into a DataFrame. We are working with a raw text
            file, but we can still use the read_csv method from pandas. We simply need to specify the separator,
            which
            is a semicolon in this case. </p>
        <p>df = pd.read_csv('../data/household_power_consumption.txt', sep=';') </p>
        <p><b>We can use this method with a .txt file as</b></p>
        <p><b>long as we specify the separator. </b></p>
        <p>We can optionally display the first five rows with df.head() and the last five rows with
            df.tail(). This will show us that our data starts on December 16, 2006, at 5:24 p.m. </p>
        <p>and ends on November 26, 2010, at 9:02 p.m. and that the data was collected at every</p>
        <p><a id="calibre_link-151"></a><img src="images/000099.jpg" alt="Image 133" class="calibre2" /></p>
        <p><b>334</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>minute. We can also display the shape of our data with df.shape, showing us that we have
            2,075,529 rows and 9 columns. </p>
        <p> <i><b>18.2.1 Dealing with missing data</b></i></p>
        <p>Now let's check for missing values. We can do this by chaining the isna() method with the
            sum() method. This returns the sum of missing values for each column of our dataset. </p>
        <p>df.isna().sum()</p>
        <p>From the output shown in figure 18.2, only the Sub_metering_3 column has missing values. In
            fact, about 1.25% of its values are missing, according to the documentation of the data. </p>
        <p>Figure 18.2</p>
        <p>Output of the total number </p>
        <p>of missing values in our dataset. You can </p>
        <p>see that only the Sub_metering_3 column </p>
        <p>has missing values. </p>
        <p>There are two options we can explore for dealing with the missing values. First, we could
            simply delete this column, since no other features have missing values. Second, we could fill in the
            missing
            values with a certain value. This process is called <i>imputing</i>. </p>
        <p>We'll first check whether there are many consecutive missing values. If that is the case, it
            is preferable to get rid of the column, as imputing many consecutive values will likely introduce a
            nonexistent trend in our data. Otherwise, if the missing values are dispersed across time, it is
            reasonable
            to fill them. The following code block outputs the length of the longest sequence of consecutive missing
            values:</p>
        <p>na_groups = </p>
        <p>➥ df['Sub_metering_3'].notna().cumsum()[df['Sub_metering_3'].isna()]</p>
        <p>len_consecutive_na = na_groups.groupby(na_groups).agg(len)</p>
        <p>longest_na_gap = len_consecutive_na.max()</p>
        <p>This outputs a length of 7,226 consecutive minutes of missing data, which is equivalent to
            roughly 5 days. In this case, the gap is definitely too large to fill with missing values, so we'll
            remove
            this column from the dataset. </p>
        <p>df = df.drop(['Sub_metering_3'], axis=1)</p>
        <p>We no longer have any missing data in our dataset, so we can move on to the next step. </p>
        <p><a id="calibre_link-152"></a> <i><b>18.2</b></i></p>
        <p> <i><b>Data wrangling and preprocessing</b></i></p>
        <p><b>335</b></p>
        <p> <i><b>18.2.2 Data conversion</b></i></p>
        <p>Now let's check if our data has the right type. We should be studying numerical data, as our
            dataset is a collection of sensor readings. </p>
        <p>We can output the type of each column using df.dtypes, which shows us that each</p>
        <p>column is of object type. In pandas this means that our data is mostly text, or a mix of
            numeric and non-numeric values. </p>
        <p>We can convert each column to a numerical value with the to_numeric function</p>
        <p>from pandas. This is essential, as our models expect numerical data. Note that we will not
            convert the date and time columns to numerical values&mdash;these will be processed in a later step.
        </p>
        <p>cols_to_convert = df.columns[2:]</p>
        <p>df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, </p>
        <p>➥ errors='coerce')</p>
        <p>We can optionally check the type of each column again using df.dtypes to make sure that the
            values were converted correctly. This will show that every column from Global_active_power to
            Sub_metering_2
            is now a float64 as expected. </p>
        <p> <i><b>18.2.3 Data resampling</b></i></p>
        <p>The next step is to check if data sampled every minute is appropriate for modeling. It is
            possible that data sampled every minute is too noisy to build a performant predictive model. </p>
        <p>To check this, we'll simply plot our target to see what it looks like. The resulting plot is
            shown in figure 18.3. </p>
        <p>fig, ax = plt.subplots(figsize=(13,6))</p>
        <p>ax.plot(df['Global_active_power'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_xlim(0, 2880)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Figure 18.3 shows that the data is very noisy, with large oscillations or flat sequences
            occurring at every minute. This kind of pattern is difficult to forecast using a deep learning model,
            since
            it seems to move at random. Also, we could question the need to forecast electricity consumption by the
            minute, as changes to the grid cannot occur in such short amounts of time. </p>
        <p>Thus, we definitely need to resample our data. In this case, we'll resample by the hour.
            That way, we'll hopefully smooth out the data and uncover a pattern that may be easier to predict with a
            machine learning model. </p>
        <p><a id="calibre_link-211"></a><img class="calibre2" src="images/000108.png" alt="Image 134" /></p>
        <p><b>336</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>10</p>
        <p>8</p>
        <p>6</p>
        <p>4</p>
        <p>Global active power (kW) 2</p>
        <p>0</p>
        <p>0</p>
        <p>500</p>
        <p>1000</p>
        <p>1500</p>
        <p>2000</p>
        <p>2500</p>
        <p>Timesteps (min)</p>
        <p>Figure 18.3</p>
        <p>The first 24 hours of recorded global active power sampled every minute. You can see that
            the data is quite noisy. </p>
        <p>To do this, we'll need a datetime data type. We can combine the Date and Time</p>
        <p>columns to create a new column that holds the same information with a datetime</p>
        <p>data type. </p>
        <p>df.loc[:,'datetime'] = pd.to_datetime(df.Date.astype(str) + ' ' + </p>
        <p>➥ df.Time.astype(str)) </p>
        <p><b>This step will take a long time. Do </b></p>
        <p><b>not worry if it seems like your </b></p>
        <p>df = df.drop(['Date', 'Time'], axis=1)</p>
        <p><b>code is hanging. </b></p>
        <p>Now we can resample our data. In this case, we'll take an hourly sum of each variable. </p>
        <p>That way we'll know the total electrical power consumed by the household every hour. </p>
        <p>hourly_df = df.resample('H', on='datetime').sum()</p>
        <p>Remember that our data started on December 16, 2006, at 5:24 p.m. and ended on</p>
        <p>November 26, 2010, at 9:02 p.m. With the new resampling, we now have a sum of each column
            per hour, which means that we have data that starts on December 16, 2006, at 5 p.m. and ends on November
            26,
            2010, at 9 p.m. However, the first and last rows of data do not have a full 60 minutes in their sum. The
            first row computed the sum from 5:24 p.m. to 5:59 p.m., which is 35 minutes. The last row computed the
            sum
            from 9:00</p>
        <p>p.m. to 9:02 p.m., which is only 2 minutes. Therefore, we'll remove the first and last rows
            of data so that we are working only with sums over full hours. </p>
        <p>hourly_df = hourly_df.drop(hourly_df.tail(1).index)</p>
        <p>hourly_df = hourly_df.drop(hourly_df.head(1).index)</p>
        <p><a id="calibre_link-429"></a><img class="calibre2" src="images/000110.png" alt="Image 135" /></p>
        <p> <i><b>18.2</b></i></p>
        <p> <i><b>Data wrangling and preprocessing</b></i></p>
        <p><b>337</b></p>
        <p>Finally, this process has changed the index. I personally prefer to have the index as
            integers and the dates as a column, so we'll simply reset the index of our DataFrame. </p>
        <p>hourly_df = hourly_df.reset_index()</p>
        <p>We can optionally check the shape of our data using hourly_df.shape, and we would see that
            we now have 34,949 rows of data. This is a drastic drop from the original two million rows.
            Nevertheless, a
            dataset of this size is definitely suitable for deep learning methods. </p>
        <p>Let's plot our target again to see if resampling our data generated a discernible pattern
            that can be forecast. Here we'll plot the first 15 days of global active power sampled hourly:</p>
        <p>fig, ax = plt.subplots(figsize=(13,6))</p>
        <p>ax.plot(hourly_df['Global_active_power'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_xlim(0, 336)</p>
        <p>plt.xticks(np.arange(0, 360, 24), ['2006-12-17', '2006-12-18', </p>
        <p>➥ '2006-12-19', '2006-12-20', '2006-12-21', '2006-12-22', '2006-12-23', </p>
        <p>➥ '2006-12-24', '2006-12-25', '2006-12-26', '2006-12-27', '2006-12-28', </p>
        <p>➥ '2006-12-29', '2006-12-30', '2006-12-31'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>As you can see in figure 18.4, we now have a smoother pattern of global active power. </p>
        <p>Furthermore, we can discern daily seasonality, although it is not as apparent as previous
            examples in this book. </p>
        <p>400</p>
        <p>350</p>
        <p>300</p>
        <p>250</p>
        <p>200</p>
        <p>150</p>
        <p>100</p>
        <p>Global active power (kW)</p>
        <p>50</p>
        <p>0</p>
        <p>2006-12-17</p>
        <p>2006-12-18</p>
        <p>2006-12-20</p>
        <p>2006-12-19</p>
        <p>2006-12-22</p>
        <p>2006-12-21</p>
        <p>2006-12-23</p>
        <p>2006-12-24</p>
        <p>2006-12-25</p>
        <p>2006-12-262006-12-27</p>
        <p>2006-12-282006-12-29</p>
        <p>2006-12-302006-12-31</p>
        <p>Day</p>
        <p>Figure 18.4</p>
        <p>Total global active power sampled every hour. We now have a smoother pattern with daily
            seasonality. This is ready to be forecast with a deep learning model. </p>
        <p><a id="calibre_link-153"></a><b>338</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>With the data wrangling done, we can save our dataset as a CSV file so we have a clean
            version of our data. This will be our starting file for the next section. </p>
        <p>hourly_df.to_csv('../data/clean_household_power_consumption.csv', </p>
        <p>header=True, index=False)</p>
        <p> <i><b>18.3</b></i></p>
        <p> <i><b>Feature engineering</b></i></p>
        <p>At this point, we have a clean dataset with no missing values and a smoothed pattern that
            will be easier to predict using deep learning techniques. Whether you followed along with the last
            section
            or not, you can read a clean version of the data and start working on the feature engineering. </p>
        <p>hourly_df = pd.read_csv('../data/clean_household_power_consumption.csv')</p>
        <p> <i><b>18.3.1 Removing unnecessary columns</b></i></p>
        <p>The first step in feature engineering is to display the basic statistics for each column.
        </p>
        <p>This is especially useful for detecting whether there are any variables that do not vary
            greatly. Such variables should be removed, since if they are almost constant over time, they are not
            predictive of our target. </p>
        <p>We can get a description of each column using the describe method from pandas:</p>
        <p>hourly_df.describe().transpose()</p>
        <p>As you can see in figure 18.5, Sub_metering_1 is likely not a good predictor for our target,
            since its constant value will not explain the variations in global active power. We can safely remove
            this
            column and keep the rest. </p>
        <p>hourly_df = hourly_df.drop(['Sub_metering_1'], axis=1)</p>
        <p><b>count</b></p>
        <p><b>mean</b></p>
        <p><b>std min</b></p>
        <p><b>25%</b></p>
        <p><b>50%</b></p>
        <p><b>75%</b></p>
        <p><b>max</b></p>
        <p><b>Global_active_power </b>34949.0</p>
        <p>64.002817</p>
        <p>54.112103</p>
        <p>0.0</p>
        <p>19.974</p>
        <p>45.868</p>
        <p>93.738</p>
        <p>393.632</p>
        <p><b>Global_reactive_power </b>34949.0</p>
        <p>7.253838</p>
        <p>4.113238</p>
        <p>0.0</p>
        <p>4.558</p>
        <p>6.324</p>
        <p>8.884</p>
        <p>46.460</p>
        <p><b>Voltage </b>34949.0 14121.298311 2155.548246</p>
        <p>0.0 14340.300 14454.060 14559.180 15114.120</p>
        <p><b>Global_intensity </b>34949.0</p>
        <p>271.331557</p>
        <p>226.626113</p>
        <p>0.0</p>
        <p>88.400</p>
        <p>196.600</p>
        <p>391.600</p>
        <p>1703.000</p>
        <p><b>Sub_metering_1 </b>34949.0</p>
        <p>65.785430</p>
        <p>210.107036</p>
        <p>0.0</p>
        <p>0.000</p>
        <p>0.000</p>
        <p>0.000</p>
        <p>2902.000</p>
        <p><b>Sub_metering_2 </b>34949.0</p>
        <p>76.139861</p>
        <p>248.978569</p>
        <p>0.0</p>
        <p>0.000</p>
        <p>19.000</p>
        <p>39.000</p>
        <p>2786.000</p>
        <p>Figure 18.5</p>
        <p>A description of each column in our dataset. You'll notice that Sub_metering_1 has a value
            of 0 for 75% of the time. Because this variable doesn't vary much over time, it can be removed from the
            set
            of features. </p>
        <p><a id="calibre_link-154"></a><img class="calibre2" src="images/000033.png" alt="Image 136" /></p>
        <p> <i><b>18.3</b></i></p>
        <p> <i><b>Feature engineering</b></i></p>
        <p><b>339</b></p>
        <p> <i><b>18.3.2 Identifying the seasonal period</b></i></p>
        <p>With our target being global active power in a household, it is likely that we'll have some
            seasonality. We can expect that at night, less electrical power will be used. Similarly, there may be a
            peak
            in consumption when people come back from work during the week. Thus, it is reasonable to assume that
            there
            will be some seasonality in our target. </p>
        <p>We can plot our target to see if we can visually detect the period. </p>
        <p>fig, ax = plt.subplots(figsize=(13,6))</p>
        <p>ax.plot(hourly_df['Global_active_power'])</p>
        <p>ax.set_xlabel('Time')</p>
        <p>ax.set_xlim(0, 336)</p>
        <p>plt.xticks(np.arange(0, 360, 24), ['2006-12-17', '2006-12-18', </p>
        <p>➥ '2006-12-19', '2006-12-20', '2006-12-21', '2006-12-22', '2006-12-23', </p>
        <p>➥ '2006-12-24', '2006-12-25', '2006-12-26', '2006-12-27', '2006-12-28', </p>
        <p>➥ '2006-12-29', '2006-12-30', '2006-12-31'])</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In figure 18.6 you can see that our target has some cyclical behavior, but the seasonal
            period is hard to determine from the graph. While our hypothesis about daily seasonality makes sense, we
            need to make sure that it is present in our data. One way to do it is with a Fourier transform. </p>
        <p>400</p>
        <p>350</p>
        <p>300</p>
        <p>250</p>
        <p>200</p>
        <p>150</p>
        <p>100</p>
        <p>Global active power (kW)</p>
        <p>50</p>
        <p>0</p>
        <p>2006-12-17</p>
        <p>2006-12-18</p>
        <p>2006-12-20</p>
        <p>2006-12-19</p>
        <p>2006-12-22</p>
        <p>2006-12-21</p>
        <p>2006-12-23</p>
        <p>2006-12-24</p>
        <p>2006-12-25</p>
        <p>2006-12-262006-12-27</p>
        <p>2006-12-282006-12-29</p>
        <p>2006-12-302006-12-31</p>
        <p>Day</p>
        <p>Figure 18.6</p>
        <p>Total global active power in the first 15 days. While there is clear cyclical behavior, the
            seasonal period is hard to determine from the graph only. </p>
        <p><a id="calibre_link-430"></a><b>340</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>Without diving into the details, a Fourier transform basically allows us to visualize the
            frequency and amplitude of a signal. Hence, we can treat our time series as a signal, apply a Fourier
            transform, and find the frequencies with large amplitudes. Those frequencies will determine the seasonal
            period. The great advantage of this method is that it is independent of the seasonal period. It can
            identify
            yearly, weekly, and daily seasonality, or any specific period we wish to test. </p>
        <p>NOTE</p>
        <p>For more information about Fourier transforms, I suggest reading</p>
        <p>Lakshay Akula's “Analyzing seasonality with Fourier transforms using Python</p>
        <p>&amp; SciPy” blog post, which does a great job of gently introducing Fourier transforms for
            analyzing seasonality: <a href="http://mng.bz/7y2Q">http://mng.bz/7y2Q</a>. </p>
        <p>For our situation, let's test for weekly and daily seasonality. </p>
        <p><b>Get the number of </b></p>
        <p><b>Apply a Fourier </b></p>
        <p><b>hours in a week. </b></p>
        <p><b>transform on </b></p>
        <p>fft = tf.signal.rfft(hourly_df['Global_active_power']) </p>
        <p><b>our target. </b></p>
        <p><b>Get the number of frequencies </b></p>
        <p>f_per_dataset = np.arange(0, len(fft)) </p>
        <p><b>from the Fourier transform. </b></p>
        <p>n_sample_h = len(hourly_df['Global_active_power']) </p>
        <p><b>Find out how </b></p>
        <p><b>many hours are </b></p>
        <p>hours_per_week = 24 * 7 </p>
        <p><b>in the dataset. </b></p>
        <p>weeks_per_dataset = n_sample_h / hours_per_week </p>
        <p>f_per_week = f_per_dataset / weeks_per_dataset </p>
        <p><b>Get the number </b></p>
        <p><b>of weeks in the </b></p>
        <p>plt.step(f_per_week, np.abs(fft)) </p>
        <p><b>dataset. </b></p>
        <p>plt.xscale('log')</p>
        <p>plt.xticks([1, 7], ['1/week', '1/day']) </p>
        <p><b>Get the frequency </b></p>
        <p>plt.xlabel('Frequency')</p>
        <p><b>of a week in the </b></p>
        <p>plt.tight_layout()</p>
        <p><b>Label</b></p>
        <p><b>dataset. </b></p>
        <p>plt.show()</p>
        <p><b>the weekly</b></p>
        <p><b>and daily</b></p>
        <p><b>Plot the frequency </b></p>
        <p><b>frequencies. </b></p>
        <p><b>and amplitude. </b></p>
        <p>In figure 18.7 you can see the amplitude of the weekly and daily frequencies. The weekly
            frequency does not show any visible peak, meaning that its amplitude is very small. Therefore, there is
            no
            weekly seasonality. </p>
        <p>Looking at the daily frequency, however, you'll notice a clear peak in the figure. </p>
        <p>This tells us that we indeed have daily seasonality in our data. Thus, we will encode our
            timestamp using a sine and cosine transformation to express the time while keeping its daily seasonal
            information. We did the same thing in chapter 12 when preparing our data for modeling with deep
            learning.
        </p>
        <p>timestamp_s = </p>
        <p>➥ pd.to_datetime(hourly_df.datetime).map(datetime.datetime.timestamp)</p>
        <p>day = 24 * 60 * 60</p>
        <p>hourly_df['day_sin'] = (np.sin(timestamp_s * (2*np.pi/day))).values</p>
        <p>hourly_df['day_cos'] = (np.cos(timestamp_s * (2*np.pi/day))).values</p>
        <p>hourly_df = hourly_df.drop(['datetime'], axis=1)</p>
        <p><a id="calibre_link-155"></a><img class="calibre2" src="images/000162.png" alt="Image 137" /></p>
        <p> <i><b>18.3</b></i></p>
        <p> <i><b>Feature engineering</b></i></p>
        <p><b>341</b></p>
        <p>1e6</p>
        <p>2.0</p>
        <p>1.5</p>
        <p>1.0</p>
        <p>0.5</p>
        <p>0.0</p>
        <p>1/week</p>
        <p>1/day</p>
        <p>Frequency</p>
        <p>Figure 18.7</p>
        <p>Amplitude of the weekly and daily seasonality in our target. </p>
        <p>You can see that the amplitude of the weekly seasonality is close to 0, </p>
        <p>while there is a visible peak for the daily seasonality. Therefore, we </p>
        <p>indeed have daily seasonality for our target. </p>
        <p>Our feature engineering is complete, and the data is ready to be scaled and split into
            training, validation, and test sets. </p>
        <p> <i><b>18.3.3 Splitting and scaling the data</b></i></p>
        <p>The final step is to split the dataset into training, validation, and test sets, and to
            scale the data. Note that we'll first split the data, so that we scale it using only the information
            from
            the training set, thus avoiding information leakage. Scaling the data will decrease training time and
            improve the performance of our models. </p>
        <p>We'll split the data 70:20:10 for the training, validation, and test sets respectively. </p>
        <p>n = len(hourly_df)</p>
        <p># Split 70:20:10 (train:validation:test)</p>
        <p>train_df = hourly_df[0:int(n*0.7)]</p>
        <p>val_df = hourly_df[int(n*0.7):int(n*0.9)]</p>
        <p>test_df = hourly_df[int(n*0.9):] </p>
        <p>Next, we'll fit the scaler to the training set only, and scale each individual set. </p>
        <p>from sklearn.preprocessing import MinMaxScaler</p>
        <p>scaler = MinMaxScaler()</p>
        <p>scaler.fit(train_df)</p>
        <p>train_df[train_df.columns] = scaler.transform(train_df[train_df.columns])</p>
        <p>val_df[val_df.columns] = scaler.transform(val_df[val_df.columns])</p>
        <p>test_df[test_df.columns] = scaler.transform(test_df[test_df.columns])</p>
        <p><a id="calibre_link-156"></a><b>342</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>We can now save each set to be used later for modeling. </p>
        <p>train_df.to_csv('../data/ch18_train.csv', index=False, header=True)</p>
        <p>val_df.to_csv('../data/ch18_val.csv', index=False, header=True)</p>
        <p>test_df.to_csv('../data/ch18_test.csv', index=False, header=True)</p>
        <p>We are now ready to move on to the modeling step. </p>
        <p> <i><b>18.4</b></i></p>
        <p> <i><b>Preparing for modeling with deep learning</b></i>
        </p>
        <p>In the last section, we produced the three sets of data required for training deep learning
            models. Recall that the objective of this project is to predict the global active power consumption in
            the
            next 24 hours. This means that we must build a univariate multi-step model, since we are forecasting
            only
            one target 24 timesteps into the future. </p>
        <p>We will build two baselines, a linear model, a deep neural network model, a long short-term
            memory (LSTM) model, a convolutional neural network (CNN), a combination of CNN and LSTM, and finally an
            autoregressive LSTM. In the end, we will use the mean absolute error (MAE) to determine which model is
            the
            best. The one that achieves the lowest MAE on the test set will be the top-performing model. </p>
        <p>Note that we'll use the MAE as our evaluation metric and the mean squared error</p>
        <p>(MSE) as the loss function, just as we have since chapter 13. </p>
        <p> <i><b>18.4.1 Initial setup</b></i></p>
        <p>Before moving on to modeling, we first need to import the required libraries, as well as
            define our DataWindow class and a function to train our models. </p>
        <p>We'll start off by importing the necessary Python libraries for modeling. </p>
        <p>import numpy as np</p>
        <p>import pandas as pd</p>
        <p>import tensorflow as tf</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>from tensorflow.keras import Model, Sequential</p>
        <p>from tensorflow.keras.optimizers import Adam</p>
        <p>from tensorflow.keras.callbacks import EarlyStopping</p>
        <p>from tensorflow.keras.losses import MeanSquaredError</p>
        <p>from tensorflow.keras.metrics import MeanAbsoluteError</p>
        <p>from tensorflow.keras.layers import Dense, Conv1D, LSTM, Lambda, Reshape, </p>
        <p>➥ RNN, LSTMCell</p>
        <p>import warnings</p>
        <p>warnings.filterwarnings('ignore')</p>
        <p>Make sure you have TensorFlow 2.6 installed, as this is the latest version at the time of
            writing. You can check the version of TensorFlow using print(tf.__version__). </p>
        <p><a id="calibre_link-157"></a> <i><b>18.4</b></i></p>
        <p> <i><b>Preparing for modeling with deep learning</b></i>
        </p>
        <p><b>343</b></p>
        <p>Optionally, you can set parameters for the plots. In this case, I prefer to specify a size
            and remove the grid on the axes. </p>
        <p>plt.rcParams['figure.figsize'] = (10, 7.5)</p>
        <p>plt.rcParams['axes.grid'] = False</p>
        <p>Then you can set a random seed. This ensures constant results when training models. </p>
        <p>Recall that the initialization of deep learning models is random, so training the same model
            twice in a row might result in slightly different performance. Thus, to ensure reproducibility, we set a
            random seed. </p>
        <p>tf.random.set_seed(42)</p>
        <p>np.random.seed(42)</p>
        <p>Next, we need to read the training set, validation set, and test set so they are ready for
            modeling. </p>
        <p>train_df = pd.read_csv('../data/ch18_train.csv')</p>
        <p>val_df = pd.read_csv('../data/ch18_val.csv')</p>
        <p>test_df = pd.read_csv('../data/ch18_test.csv')</p>
        <p>Finally, we'll build a dictionary to store the column names and their corresponding indexes.
            This will be useful later on for building the baseline models and creating windows of data. </p>
        <p>column_indices = {name: i for i, name in enumerate(train_df.columns)}</p>
        <p>We'll now move on to defining the DataWindow class. </p>
        <p> <i><b>18.4.2 Defining the DataWindow class</b></i></p>
        <p>The DataWindow class allows us to quickly create windows of data for training deep learning
            models. Each window of data contains a set of inputs and a set of labels. The model is then trained to
            produce predictions as close as possible to the labels using the inputs. </p>
        <p>An entire section of chapter 13 was dedicated to implementing the DataWindow class step by
            step, and we have been using it ever since, so we will go straight to its implementation. The only
            change
            here will be the name of the default column to plot when we visualize the predictions against the
            labels.
        </p>
        <p>Listing 18.1</p>
        <p>Implementation of class to create windows of data</p>
        <p>class DataWindow():</p>
        <p>def __init__(self, input_width, label_width, shift, </p>
        <p>train_df=train_df, val_df=val_df, test_df=test_df, </p>
        <p>label_columns=None):</p>
        <p></p>
        <p>self.train_df = train_df</p>
        <p>self.val_df = val_df</p>
        <p>self.test_df = test_df</p>
        <p><a id="calibre_link-431"></a><b>344</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>self.label_columns = label_columns</p>
        <p>if label_columns is not None:</p>
        <p>self.label_columns_indices = {name: i for i, name in </p>
        <p>➥ enumerate(label_columns)}</p>
        <p>self.column_indices = {name: i for i, name in </p>
        <p>➥ enumerate(train_df.columns)}</p>
        <p></p>
        <p>self.input_width = input_width</p>
        <p>self.label_width = label_width</p>
        <p>self.shift = shift</p>
        <p></p>
        <p>self.total_window_size = input_width + shift</p>
        <p></p>
        <p>self.input_slice = slice(0, input_width)</p>
        <p>self.input_indices = </p>
        <p>➥ np.arange(self.total_window_size)[self.input_slice]</p>
        <p></p>
        <p>self.label_start = self.total_window_size - self.label_width</p>
        <p>self.labels_slice = slice(self.label_start, None)</p>
        <p>self.label_indices = </p>
        <p>➥ np.arange(self.total_window_size)[self.labels_slice]</p>
        <p></p>
        <p>def split_to_inputs_labels(self, features):</p>
        <p>inputs = features[:, self.input_slice, :]</p>
        <p>labels = features[:, self.labels_slice, :]</p>
        <p>if self.label_columns is not None:</p>
        <p>labels = tf.stack(</p>
        <p>[labels[:,:,self.column_indices[name]] for name in </p>
        <p>➥ self.label_columns], </p>
        <p>axis=-1</p>
        <p>)</p>
        <p>inputs.set_shape([None, self.input_width, None])</p>
        <p>labels.set_shape([None, self.label_width, None])</p>
        <p></p>
        <p>return inputs, labels</p>
        <p></p>
        <p>def plot(self, model=None, plot_col='Global_active_power', </p>
        <p>➥ max_subplots=3): </p>
        <p><b>Set the default name </b></p>
        <p>inputs, labels = self.sample_batch</p>
        <p><b>of our target to be the </b></p>
        <p></p>
        <p><b>global active power. </b></p>
        <p>plt.figure(figsize=(12, 8))</p>
        <p>plot_col_index = self.column_indices[plot_col]</p>
        <p>max_n = min(max_subplots, len(inputs))</p>
        <p></p>
        <p>for n in range(max_n):</p>
        <p>plt.subplot(3, 1, n+1)</p>
        <p>plt.ylabel(f'{plot_col} [scaled]')</p>
        <p>plt.plot(self.input_indices, inputs[n, :, plot_col_index], </p>
        <p>label='Inputs', marker='.', zorder=-10)</p>
        <p>if self.label_columns:</p>
        <p>label_col_index = self.label_columns_indices.get(plot_col, </p>
        <p>➥ None)</p>
        <p>else:</p>
        <p>label_col_index = plot_col_index</p>
        <p><a id="calibre_link-238"></a> <i><b>18.4</b></i></p>
        <p> <i><b>Preparing for modeling with deep learning</b></i>
        </p>
        <p><b>345</b></p>
        <p>if label_col_index is None:</p>
        <p>continue</p>
        <p>plt.scatter(self.label_indices, labels[n, :, label_col_index], </p>
        <p>edgecolors='k', marker='s', label='Labels', </p>
        <p>➥ c='green', s=64)</p>
        <p>if model is not None:</p>
        <p>predictions = model(inputs)</p>
        <p>plt.scatter(self.label_indices, predictions[n, :, </p>
        <p>➥ label_col_index], </p>
        <p>marker='X', edgecolors='k', label='Predictions', </p>
        <p>c='red', s=64)</p>
        <p>if n == 0:</p>
        <p>plt.legend()</p>
        <p>plt.xlabel('Time (h)')</p>
        <p></p>
        <p>def make_dataset(self, data):</p>
        <p>data = np.array(data, dtype=np.float32)</p>
        <p>ds = tf.keras.preprocessing.timeseries_dataset_from_array(</p>
        <p>data=data, </p>
        <p>targets=None, </p>
        <p>sequence_length=self.total_window_size, </p>
        <p>sequence_stride=1, </p>
        <p>shuffle=True, </p>
        <p>batch_size=32</p>
        <p>)</p>
        <p></p>
        <p>ds = ds.map(self.split_to_inputs_labels)</p>
        <p>return ds</p>
        <p></p>
        <p>@property</p>
        <p>def train(self):</p>
        <p>return self.make_dataset(self.train_df)</p>
        <p></p>
        <p>@property</p>
        <p>def val(self):</p>
        <p>return self.make_dataset(self.val_df)</p>
        <p></p>
        <p>@property</p>
        <p>def test(self):</p>
        <p>return self.make_dataset(self.test_df)</p>
        <p></p>
        <p>@property</p>
        <p>def sample_batch(self):</p>
        <p>result = getattr(self, '_sample_batch', None)</p>
        <p>if result is None:</p>
        <p>result = next(iter(self.train))</p>
        <p>self._sample_batch = result</p>
        <p>return result</p>
        <p>With the DataWindow class defined, we only need a function to compile and train the
            different models we'll develop. </p>
        <p><a id="calibre_link-158"></a><b>346</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p> <i><b>18.4.3 Utility function to train our models</b></i>
        </p>
        <p>Our final step before launching our experiments is to build a function that automates the
            training process. This is the compile_and_fit function that we have been using since chapter 13. </p>
        <p>Recall that this function takes in a model and a window of data. Then it implements early
            stopping, meaning that the model will stop training if the validation loss does not change for three
            consecutive epochs. This is also the function in which we specify the loss function to be the MSE and
            the
            evaluation metric to be the MAE. </p>
        <p>def compile_and_fit(model, window, patience=3, max_epochs=50):</p>
        <p>early_stopping = EarlyStopping(monitor='val_loss', </p>
        <p>patience=patience, </p>
        <p>mode='min')</p>
        <p></p>
        <p>model.compile(loss=MeanSquaredError(), </p>
        <p>optimizer=Adam(), </p>
        <p>metrics=[MeanAbsoluteError()])</p>
        <p></p>
        <p>history = model.fit(window.train, </p>
        <p>epochs=max_epochs, </p>
        <p>validation_data=window.val, </p>
        <p>callbacks=[early_stopping])</p>
        <p></p>
        <p>return history</p>
        <p>At this point, we have everything we need to start developing models to forecast the next 24
            hours of global active power. </p>
        <p> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p>The training, validation, and test sets are ready, as well as the DataWindow class and the
            function that will train our models. Everything is set for us to start building deep learning models.
        </p>
        <p>We'll first implement two baselines, and then we'll train models with increasing complexity:
            a linear model, a deep neural network, an LSTM, a CNN, a CNN and LSTM model, and an autoregressive LSTM.
            Once all the models are trained, we'll select the best model by comparing the MAE on the test set. The
            model
            with the lowest MAE will be the one that we recommend. </p>
        <p> <i><b>18.5.1 Baseline models</b></i></p>
        <p>Every forecasting project must start with a baseline model. Baselines serve as a benchmark
            for our more sophisticated models, as they can only be better in comparison to a certain benchmark.
            Building
            baseline models also allows us to assess whether the added complexity of a model really generates a
            significant benefit. It is possible that a complex model does not perform much better than a baseline,
            in
            which case implementing a complex model is hard to justify. In this case, we'll build two baseline
            models:
            one that repeats the last known value and another that repeats the last 24 hours of data. </p>
        <p><a id="calibre_link-285"></a> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>347</b></p>
        <p>We'll start by creating the window of data that will be used. Recall that the objective is
            to forecast the next 24 hours of global active power. Thus, the length of our label sequence is 24
            timesteps, and the shift will also be 24 timesteps. We'll also use an input length of 24. </p>
        <p>multi_window = DataWindow(input_width=24, label_width=24, shift=24, </p>
        <p>➥ label_columns=['Global_active_power'])</p>
        <p>Next, we'll implement a class that will repeat the last known value of the input sequence as
            a prediction for the next 24 hours. </p>
        <p>class MultiStepLastBaseline(Model):</p>
        <p>def __init__(self, label_index=None):</p>
        <p>super().__init__()</p>
        <p>self.label_index = label_index</p>
        <p></p>
        <p>def call(self, inputs):</p>
        <p>if self.label_index is None:</p>
        <p>return tf.tile(inputs[:, -1:, :], [1, 24, 1])</p>
        <p>return tf.tile(inputs[:, -1:, self.label_index:], [1, 24, 1])</p>
        <p>We can now generate predictions using this baseline and store its performance in a
            dictionary. This dictionary will store the performance of each model so that we can compare them at the
            end.
            Note that we will not display the MAE of each model as we build them. We will compare the evaluation
            metrics
            once all the models are trained. </p>
        <p>baseline_last = </p>
        <p>➥ MultiStepLastBaseline(label_index=column_indices['Global_active_power'])</p>
        <p>baseline_last.compile(loss=MeanSquaredError(), </p>
        <p>➥ metrics=[MeanAbsoluteError()])</p>
        <p>val_performance = {}</p>
        <p>performance = {}</p>
        <p>val_performance['Baseline - Last'] = </p>
        <p>➥ baseline_last.evaluate(multi_window.val)</p>
        <p>performance['Baseline - Last'] = baseline_last.evaluate(multi_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>We can visualize the predictions using the plot method of the DataWindow class, as shown in
            figure 18.8. It will display three plots in the figure, as specified in the DataWindow class. </p>
        <p>multi_window.plot(baseline_last)</p>
        <p>In figure 18.8 we have a working baseline&mdash;the forecasts correspond to a flat line with
            the same value as the last input. You may get a slightly different plot, since the cached sample batch
            used
            to create the plots may not be the same. However, the model's metrics will be identical to what is shown
            here, as long as the random seeds are equal. </p>
        <p><a id="calibre_link-432"></a><img class="calibre2" src="images/000161.png" alt="Image 138" /></p>
        <p><b>348</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>[scaled] 0.3</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>led]a 0.4</p>
        <p>[sc</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>0.0</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>[scaled]</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_power</p>
        <p>0</p>
        <p>G</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 18.8</p>
        <p>Predictions from the baseline model, which simply repeats the last known input value Next,
            let's implement a baseline model that repeats the input sequence. Remember that we identified daily
            seasonality in our target, so this is equivalent to forecasting the last known season. </p>
        <p>class RepeatBaseline(Model):</p>
        <p>def __init__(self, label_index=None):</p>
        <p>super().__init__()</p>
        <p>self.label_index = label_index</p>
        <p></p>
        <p>def call(self, inputs):</p>
        <p>return inputs[:, :, self.label_index:]</p>
        <p>Once it's defined, we can generate predictions and store the baseline's performance for
            comparison. We can also visualize the generated predictions, as shown in figure 18.9. </p>
        <p>baseline_repeat = </p>
        <p>➥ RepeatBaseline(label_index=column_indices['Global_active_power'])</p>
        <p>baseline_repeat.compile(loss=MeanSquaredError(), </p>
        <p>➥ metrics=[MeanAbsoluteError()])</p>
        <p>val_performance['Baseline - Repeat'] = </p>
        <p>➥ baseline_repeat.evaluate(multi_window.val)</p>
        <p>performance['Baseline - Repeat'] = </p>
        <p>➥ baseline_repeat.evaluate(multi_window.test, verbose=0)</p>
        <p><a id="calibre_link-159"></a><img class="calibre2" src="images/000153.png" alt="Image 139" /></p>
        <p> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>349</b></p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>[scaled] 0.3</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>led]a 0.4</p>
        <p>[sc</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>0.0</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>[scaled]</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_power</p>
        <p>0</p>
        <p>G</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 18.9</p>
        <p>Predicting the last season as a baseline</p>
        <p>In figure 18.9 you'll see that the predictions are equal to the input sequence, which is the
            expected behavior for this baseline model. Feel free to print out the MAE for each model as you build
            them.
            I'll display them at the end of the chapter in a bar chart to determine which model should be selected.
        </p>
        <p>With the baseline models in place, we can move on to the slightly more complex</p>
        <p>linear model. </p>
        <p> <i><b>18.5.2 Linear model</b></i></p>
        <p>One of the simplest models we can build is a linear model. This model consists of only an
            input layer and an output layer. Thus, only a sequence of weights is computed to generate predictions
            that
            are as close as possible to the labels. </p>
        <p>In this case, we'll build a model with one Dense output layer that has only one neuron,
            since we are predicting only one target. We'll then train the model and store its performance. </p>
        <p>label_index = column_indices['Global_active_power']</p>
        <p>num_features = train_df.shape[1]</p>
        <p>linear = Sequential([</p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros)</p>
        <p>])</p>
        <p><a id="calibre_link-160"></a><img class="calibre2" src="images/000036.png" alt="Image 140" /></p>
        <p><b>350</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>history = compile_and_fit(linear, multi_window)</p>
        <p>val_performance['Linear'] = linear.evaluate(multi_window.val)</p>
        <p>performance['Linear'] = linear.evaluate(multi_window.test, verbose=0)</p>
        <p>As always, we can visualize the predictions using the plot method, as shown in figure 18.10.
        </p>
        <p>multi_window.plot(linear)</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>[scaled]</p>
        <p>Labels</p>
        <p>0.3</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>led]ac 0.4</p>
        <p>[s</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>0.0</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>[scaled]</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_powerG</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 18.10</p>
        <p>Predictions generated from a linear model</p>
        <p>Now let's add hidden layers and implement a deep neural network. </p>
        <p> <i><b>18.5.3 Deep neural network</b></i></p>
        <p>The previous linear model did not have any hidden layers; it was simply an input layer and
            an output layer. Now we'll add hidden layers, which will help us model nonlinear relationships in the
            data.
        </p>
        <p>Here we'll stack two Dense layers with 64 neurons and use ReLU as the activation function.
            Then we'll train the model and store its performance for comparison. </p>
        <p>dense = Sequential([</p>
        <p>Dense(64, activation='relu'), </p>
        <p><a id="calibre_link-161"></a> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>351</b></p>
        <p>Dense(64, activation='relu'), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>history = compile_and_fit(dense, multi_window)</p>
        <p>val_performance['Dense'] = dense.evaluate(multi_window.val)</p>
        <p>performance['Dense'] = dense.evaluate(multi_window.test, verbose=0)</p>
        <p>You can optionally visualize the predictions with multi_window.plot(dense). </p>
        <p>The next model we'll implement is the long short-term memory model. </p>
        <p> <i><b>18.5.4 Long short-term memory (LSTM) model</b></i>
        </p>
        <p>The main advantage of the long short-term memory (LSTM) model is that it keeps information
            from the past in memory. This makes it especially suitable for treating sequences of data, like time
            series.
            It allows us to combine information from the present and the past to produce a prediction. </p>
        <p>We'll feed the input sequence through an LSTM layer before sending it to the output layer,
            which remains a Dense layer with one neuron. We'll then train the model and store its performance in the
            dictionary for comparison at the end. </p>
        <p>lstm_model = Sequential([</p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>history = compile_and_fit(lstm_model, multi_window)</p>
        <p>val_performance['LSTM'] = lstm_model.evaluate(multi_window.val)</p>
        <p>performance['LSTM'] = lstm_model.evaluate(multi_window.test, verbose=0)</p>
        <p>We can visualize the predictions from the LSTM&mdash;they are shown in figure 18.11. </p>
        <p>multi_window.plot(lstm_model)</p>
        <p>Now let's implement a convolutional neural network. </p>
        <p> <i><b>18.5.5 Convolutional neural network (CNN)</b></i>
        </p>
        <p>A convolutional neural network (CNN) uses the convolution function to reduce the feature
            space. This effectively filters our time series and performs feature selection. </p>
        <p>Furthermore, a CNN is faster to train than an LSTM since the operations are parallelized,
            whereas the LSTM must treat one element of the sequence at a time. </p>
        <p>Because the convolution operation reduces the feature space, we must provide a</p>
        <p>slightly longer input sequence to make sure that the output sequence contains 24</p>
        <p>timesteps. How much longer it needs to be depends on the length of the kernel that</p>
        <p><a id="calibre_link-433"></a><img class="calibre2" src="images/000084.png" alt="Image 141" /></p>
        <p><b>352</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>Labels</p>
        <p>[scaled] 0.3</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>led]a 0.4</p>
        <p>[sc</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>0.0</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>[scaled]</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_power</p>
        <p>0</p>
        <p>G</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 18.11</p>
        <p>Predictions generated from the LSTM model</p>
        <p>performs the convolution operation. In this case, we'll use a kernel length of 3. This is an
            arbitrary choice, so feel free to experiment with different values, although your results might differ
            from
            what is shown here. Given that we need 24 labels, we can calculate the input sequence using equation
            18.1.
        </p>
        <p>input length = label length + kernel length &ndash; 1</p>
        <p>Equation 18.1</p>
        <p>This forces us to define a window of data specifically for the CNN model. Note that since we
            are defining a new window of data, the sample batch used for plotting will differ from the one used so
            far.
        </p>
        <p>We now have all the necessary information to define a window of data for the CNN</p>
        <p>model. </p>
        <p>KERNEL_WIDTH = 3</p>
        <p>LABEL_WIDTH = 24</p>
        <p>INPUT_WIDTH = LABEL_WIDTH + KERNEL_WIDTH - 1</p>
        <p>cnn_multi_window = DataWindow(input_width=INPUT_WIDTH, </p>
        <p>➥ label_width=LABEL_WIDTH, shift=24, </p>
        <p>➥ label_columns=['Global_active_power'])</p>
        <p><a id="calibre_link-228"></a><img class="calibre2" src="images/000087.png" alt="Image 142" /></p>
        <p> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>353</b></p>
        <p>Next, we'll send the input through a Conv1D layer, which filters the input sequence. </p>
        <p>Then it is fed to a Dense layer with 32 neurons for learning before going to the output
            layer. As always, we'll train the model and store its performance for comparison. </p>
        <p>cnn_model = Sequential([</p>
        <p>Conv1D(32, activation='relu', kernel_size=(KERNEL_WIDTH)), </p>
        <p>Dense(units=32, activation='relu'), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>history = compile_and_fit(cnn_model, cnn_multi_window)</p>
        <p>val_performance['CNN'] = cnn_model.evaluate(cnn_multi_window.val)</p>
        <p>performance['CNN'] = cnn_model.evaluate(cnn_multi_window.test, verbose=0)</p>
        <p>We can now visualize the predictions. </p>
        <p>cnn_multi_window.plot(cnn_model)</p>
        <p>You will notice in figure 18.12 that the input sequence differs from our previous methods
            because working with a CNN involves windowing the data again to account for the convolution kernel
            length.
            The training, validation, and test sets remain unchanged, so it is still valid to compare all the
            models'
            performance. </p>
        <p>Now let's combine the CNN model with the LSTM model. </p>
        <p>Inputs</p>
        <p>0.3</p>
        <p>[scaled]</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>led]ac 0.8</p>
        <p>[s</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_power 0.0</p>
        <p>G</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>0.4</p>
        <p>[scaled]</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>lobal_active_powerG</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>Time (h)</p>
        <p>Figure 18.12</p>
        <p>Predictions generated by the CNN model</p>
        <p><a id="calibre_link-162"></a><img class="calibre2" src="images/000157.png" alt="Image 143" /></p>
        <p><b>354</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p> <i><b>18.5.6 Combining a CNN with an LSTM</b></i></p>
        <p>We know that LSTM is good at treating sequences of data, while CNN can filter a sequence of
            data. Therefore, it is interesting to test whether filtering a sequence before feeding it to an LSTM can
            result in a better-performing model. </p>
        <p>We'll feed the input sequence to a Conv1D layer, but use an LSTM layer for learning this
            time. Then we'll send the information to the output layer. Again, we'll train the model and store its
            performance. </p>
        <p>cnn_lstm_model = Sequential([</p>
        <p>Conv1D(32, activation='relu', kernel_size=(KERNEL_WIDTH)), </p>
        <p>LSTM(32, return_sequences=True), </p>
        <p>Dense(1, kernel_initializer=tf.initializers.zeros), </p>
        <p>])</p>
        <p>history = compile_and_fit(cnn_lstm_model, cnn_multi_window)</p>
        <p>val_performance['CNN + LSTM'] = </p>
        <p>➥ cnn_lstm_model.evaluate(cnn_multi_window.val)</p>
        <p>performance['CNN + LSTM'] = cnn_lstm_model.evaluate(cnn_multi_window.test, </p>
        <p>➥ verbose=0)</p>
        <p>The predictions are visualized in figure 18.13. </p>
        <p>cnn_multi_window.plot(cnn_lstm_model)</p>
        <p>Inputs</p>
        <p>0.3</p>
        <p>[scaled]</p>
        <p>Labels</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>led]ac 0.8</p>
        <p>[s</p>
        <p>0.6</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_power 0.0</p>
        <p>G</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>0.4</p>
        <p>[scaled]</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>lobal_active_powerG</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>50</p>
        <p>Time (h)</p>
        <p>Figure 18.13</p>
        <p>Predictions from a CNN combined with an LSTM model</p>
        <p><a id="calibre_link-163"></a> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>355</b></p>
        <p>Finally, let's implement an autoregressive LSTM model. </p>
        <p> <i><b>18.5.7 The autoregressive LSTM model</b></i></p>
        <p>The final model that we'll implement is an autoregressive LSTM (ARLSTM) model. </p>
        <p>Instead of generating the entire output sequence in a single shot, the autoregressive model
            will generate one prediction at a time and use that prediction as an input to generate the next one.
            This
            kind of architecture is present in state-of-the-art forecasting models, but it comes with a caveat. If
            the
            model generates a very bad first prediction, this mistake will be carried on to the next predictions,
            which
            will magnify the errors. </p>
        <p>Nevertheless, it is worth testing this model to see if it works well in our situation. </p>
        <p>The first step is defining the class that implements the ARLSTM model. This is the same
            class that we used in chapter 17. </p>
        <p>Listing 18.2</p>
        <p>Class to implement an ARLSTM model</p>
        <p>class AutoRegressive(Model):</p>
        <p>def __init__(self, units, out_steps):</p>
        <p>super().__init__()</p>
        <p>self.out_steps = out_steps</p>
        <p>self.units = units</p>
        <p>self.lstm_cell = LSTMCell(units)</p>
        <p>self.lstm_rnn = RNN(self.lstm_cell, return_state=True)</p>
        <p>self.dense = Dense(train_df.shape[1])</p>
        <p></p>
        <p>def warmup(self, inputs):</p>
        <p>x, *state = self.lstm_rnn(inputs)</p>
        <p>prediction = self.dense(x)</p>
        <p></p>
        <p>return prediction, state</p>
        <p></p>
        <p>def call(self, inputs, training=None):</p>
        <p>predictions = []</p>
        <p>prediction, state = self.warmup(inputs)</p>
        <p></p>
        <p>predictions.append(prediction)</p>
        <p></p>
        <p>for n in range(1, self.out_steps):</p>
        <p>x = prediction</p>
        <p>x, state = self.lstm_cell(x, states=state, training=training)</p>
        <p></p>
        <p>prediction = self.dense(x)</p>
        <p>predictions.append(prediction)</p>
        <p></p>
        <p>predictions = tf.stack(predictions)</p>
        <p>predictions = tf.transpose(predictions, [1, 0, 2])</p>
        <p></p>
        <p>return predictions</p>
        <p>We can then use this class to initialize our model. We'll train the model on the multi_</p>
        <p>window and store its performance for comparison. </p>
        <p><a id="calibre_link-164"></a><img class="calibre2" src="images/000075.png" alt="Image 144" /></p>
        <p><b>356</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>AR_LSTM = AutoRegressive(units=32, out_steps=24)</p>
        <p>history = compile_and_fit(AR_LSTM, multi_window)</p>
        <p>val_performance['AR - LSTM'] = AR_LSTM.evaluate(multi_window.val)</p>
        <p>performance['AR - LSTM'] = AR_LSTM.evaluate(multi_window.test, verbose=0)</p>
        <p>We can then visualize the predictions of the autoregressive LSTM model, as shown in figure
            18.14. </p>
        <p>multi_window.plot(AR_LSTM)</p>
        <p>0.4</p>
        <p>Inputs</p>
        <p>[scaled]</p>
        <p>Labels</p>
        <p>0.3</p>
        <p>Predictions</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>led]ac 0.4</p>
        <p>[s</p>
        <p>0.3</p>
        <p>0.2</p>
        <p>0.1</p>
        <p>0.0</p>
        <p>Global_active_power</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>0.6</p>
        <p>[scaled]</p>
        <p>0.4</p>
        <p>0.2</p>
        <p>lobal_active_powerG</p>
        <p>0</p>
        <p>10</p>
        <p>20</p>
        <p>30</p>
        <p>40</p>
        <p>Time (h)</p>
        <p>Figure 18.14</p>
        <p>Predictions from the ARLSTM model</p>
        <p>Now that we have built a wide variety of models, let's select the best one based on its MAE
            on the test set. </p>
        <p> <i><b>18.5.8 Selecting the best model</b></i></p>
        <p>We have built many models for this project, from a linear model to an ARLSTM</p>
        <p>model. Now let's visualize the MAE of each model to determine the champion. </p>
        <p>We'll plot the MAE on both the validation and test sets. The result is shown in figure
            18.15. </p>
        <p><a id="calibre_link-434"></a><img class="calibre2" src="images/000026.jpg" alt="Image 145" /></p>
        <p> <i><b>18.5</b></i></p>
        <p> <i><b>Modeling with deep learning</b></i></p>
        <p><b>357</b></p>
        <p>Validation</p>
        <p>Test</p>
        <p>0.302</p>
        <p>0.296</p>
        <p>0.30</p>
        <p>0.286</p>
        <p>0.284</p>
        <p>0.25</p>
        <p>0.20</p>
        <p>0.15</p>
        <p>Mean absolute error</p>
        <p>0.10</p>
        <p>0.0910.088 0.086</p>
        <p>0.085</p>
        <p>0.085</p>
        <p>0.084</p>
        <p>0.083</p>
        <p>0.081</p>
        <p>0.083</p>
        <p>0.081</p>
        <p>0.078</p>
        <p>0.074</p>
        <p>0.05</p>
        <p>0.00</p>
        <p>Baseline - Last Baseline - Repeat Linear</p>
        <p>Dense</p>
        <p>LSTM</p>
        <p>CNN</p>
        <p>CNN + LSTM AR - LSTM</p>
        <p>Models</p>
        <p>Figure 18.15</p>
        <p>Comparing the MAE of all models tested. The ARLSTM model achieved the lowest MAE on the test
            set. </p>
        <p>mae_val = [v[1] for v in val_performance.values()]</p>
        <p>mae_test = [v[1] for v in performance.values()]</p>
        <p>x = np.arange(len(performance))</p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.bar(x - 0.15, mae_val, width=0.25, color='black', edgecolor='black', </p>
        <p>➥ label='Validation')</p>
        <p>ax.bar(x + 0.15, mae_test, width=0.25, color='white', edgecolor='black', </p>
        <p>➥ hatch='/', label='Test')</p>
        <p>ax.set_ylabel('Mean absolute error')</p>
        <p>ax.set_xlabel('Models')</p>
        <p>for index, value in enumerate(mae_val):</p>
        <p>plt.text(x=index - 0.15, y=value+0.005, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p></p>
        <p>for index, value in enumerate(mae_test):</p>
        <p>plt.text(x=index + 0.15, y=value+0.0025, s=str(round(value, 3)), </p>
        <p>➥ ha='center')</p>
        <p><a id="calibre_link-165"></a><b>358</b></p>
        <p>CHAPTER 18</p>
        <p> <i><b>Capstone: Forecasting the electric power
                    consumption of a household</b></i></p>
        <p>plt.ylim(0, 0.33)</p>
        <p>plt.xticks(ticks=x, labels=performance.keys())</p>
        <p>plt.legend(loc='best')</p>
        <p>plt.tight_layout()</p>
        <p>Figure 18.15 shows that all the models performed much better than the baselines. </p>
        <p>Furthermore, our champion is the ARLSTM model, since it achieved a MAE of 0.074</p>
        <p>on the test set, which is the lowest MAE of all. Thus, we would recommend using this model
            to forecast the global active power over the next 24 hours. </p>
        <p> <i><b>18.6</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>Congratulations on completing this capstone project! I hope that you were successful in
            completing it on your own and that you feel confident in your knowledge of forecasting time series using
            deep learning models. </p>
        <p>I highly encourage you to make this project your own. You can turn this project</p>
        <p>into a multivariate forecasting problem by forecasting more than one target. You could also
            change the forecast horizon. In short, make changes and play around with the models and the data and see
            what you can achieve on your own. </p>
        <p>In the next chapter, we'll start the final part of this book, where we'll automate the
            forecasting process. There are many libraries that can generate accurate predictions with minimal steps,
            and
            they are often used in the industry, making this an essential tool for time series forecasting. We'll
            look
            at a widely used library called Prophet. </p>
        <p><a id="calibre_link-22"></a> <i>Part 4</i></p>
        <p> <i>Automating</i></p>
        <p> <i>forecasting at scale</i></p>
        <p>We have so far been building our models by hand. This has given us granu-</p>
        <p>lar control over what is happening, but it can also be a lengthy process. Thus, it's time to
            explore some tools for automatic time series forecasting. These tools are widely used in the industry,
            as
            they are easy to use and enable quick experimentation. They also implement state-of-the-art models,
            making
            them easily accessi-</p>
        <p>ble to any data scientist. </p>
        <p>Here we'll explore the ecosystem of automatic forecasting tools and focus on</p>
        <p>Prophet, as it is one of the most popular libraries for automatic forecasting, and more
            recent libraries model their syntax on that of Prophet. This means that if</p>
        <p>you know how to work with Prophet, it is easy to work with another tool. </p>
        <p>As in previous parts, we'll conclude with a capstone project. </p>
        <p><a id="calibre_link-435"></a> </p>
        <p><a id="calibre_link-23"></a> <i>Automating time series</i></p>
        <p> <i>forecasting with Prophet</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Assessing different libraries for automated </p>
        <p>forecasting</p>
        <p> Exploring the functionality of Prophet</p>
        <p> Forecasting with Prophet</p>
        <p>Throughout this book, we have built models involving many manual steps. For dec-</p>
        <p>linations of the SARIMAX models, for example, we had to develop a function to select the
            best model according to the Akaike information criterion (AIC) and a function to perform rolling
            forecasts.
            In the deep learning portion of the book, we had to build a class to create windows of data, as well as
            define all the deep learning models, although this was greatly facilitated by the use of Keras. </p>
        <p>While manually building and tweaking our models allows for great flexibility and total
            control over our forecasting techniques, it is also useful to automate most of the forecasting process,
            making it easier to forecast time series and accelerating experiments. Therefore, it is important to
            understand the automation tools, as they are a fast way to obtain predictions, and they often facilitate
            the
            use of state-of-the-art models. </p>
        <p>In this chapter, we'll first look at the various libraries that automate the process of time
            series forecasting. Then we'll focus specifically on the Prophet library, which <b>361</b>
        </p>
        <p><a id="calibre_link-166"></a><b>362</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>is arguably the most well-known and widely used forecasting library. We'll explore its
            functionality using a real-life dataset. Finally, we'll conclude this chapter with a forecasting project
            so
            we can see Prophet in action. </p>
        <p> <i><b>19.1</b></i></p>
        <p> <i><b>Overview of the automated forecasting
                    libraries</b></i></p>
        <p>The data science community and companies have developed many libraries to auto-</p>
        <p>mate the forecasting process and make it easier. Some of the most popular libraries and
            their websites are listed here: </p>
        <p> <i>Pmdarima</i><a
                href="http://alkaline-ml.com/pmdarima/modules/classes.html">&mdash;http://alkaline-ml.com/pmdarima/modules/classes.html</a>
        </p>
        <p> <i>Prophet</i><a href="https://facebook.github.io/prophet">&mdash;https://facebook.github.io/prophet</a>
        </p>
        <p> <i>NeuralProphet</i>&mdash;<a
                href="https://neuralprophet.com/html/index.html">https://neuralprophet.com/html/index.html</a></p>
        <p> <i>PyTorch Forecasting</i><a
                href="https://pytorch-forecasting.readthedocs.io/en/stable">&mdash;https://pytorch-forecasting.readthedocs.io/en/stable</a>
        </p>
        <p>This is by no means an exhaustive list, and I wish to remain impartial in their use. As a
            data scientist, you have the knowledge and capacity to assess whether a particular library is suitable
            for
            your needs in a particular context. </p>
        <p>The pmdarima library is the Python implementation of the popular auto.arima</p>
        <p>library in R. Pmdarima is essentially a wrapper that generalizes many of the statistical
            models we have used, such as the ARMA, ARIMA, and SARIMA models. The main advantage of this library is
            that
            it provides an easy-to-use interface that automatically uses all the tools we've discussed for
            forecasting
            with statistical models, such as the augmented Dickey-Fuller (ADF) test to test for stationarity and
            selecting the orders <i>p</i>, <i>q</i>, <i>P</i>,
            and <i>Q</i> to minimize the AIC. It also comes with toy datasets, making it great for
            first-time learners to test different models on simple time series. This package is built and maintained
            by
            the community, but, most importantly, it is still being actively maintained at the time of writing. </p>
        <p>Prophet is an open source package from Meta Open Source, meaning that it is built and
            maintained by Meta. This library was built specifically for business forecasting at scale. It arose from
            the
            internal need at Facebook to produce accurate forecasts quickly, and the library was then made freely
            available. Prophet is arguably the best-known forecasting library in the industry, as it can fit
            nonlinear
            trends and combine the effect of multiple seasonalities. The remainder of this chapter and the next one
            will
            focus entirely on this library, and we'll explore it in greater detail in the next section. </p>
        <p>NeuralProphet builds on the Prophet library to automate the use of hybrid models for time
            series forecasting. This is a rather new project that is still in its beta phase at the time of writing.
            The
            library was built with the collaboration of people from different universities and Facebook. This
            package
            introduces a combination of classical models, such as ARIMA, and neural networks, to produce accurate
            forecasts. It uses PyTorch on the backend, meaning that experienced users can easily extend the
            library's
            functionality. Most importantly, it uses an API similar to Prophet's, so once you learn how to work with
            Prophet, you can seamlessly transition to working with NeuralProphet. To learn more, you can read their
            paper, “NeuralProphet: Explainable</p>
        <p><a id="calibre_link-167"></a> <i><b>19.2</b></i></p>
        <p> <i><b>Exploring Prophet</b></i></p>
        <p><b>363</b></p>
        <p>Forecasting at Scale” (<a href="https://arxiv.org/abs/2111.15397">https://arxiv.org/abs/2111.15397). </a>
            It
            provides greater
            detail on NeuralProphet's internal functions and performance benchmarks while still being an accessible
            article. </p>
        <p>Finally, PyTorch Forecasting facilitates the use of state-of-the-art deep learning models
            for time series forecasting. It, of course, uses PyTorch, and it provides a simple interface to
            implement
            models such as DeepAR, N-Beats, LSTM, and more. This package is built by the community and, at the time
            of
            writing, is being actively maintained. </p>
        <p>NOTE</p>
        <p>For more information about DeepAR, see David Salinas, Valentin</p>
        <p>Flunkert, Jan Gasthaus, Tim Januschowski, “DeepAR: Probabilistic forecasting</p>
        <p>with autoregressive recurrent networks,” <i>International Journal of
                Forecasting</i> 36:3 (2020), <a href="http://mng.bz/z4Kr">http://mng.bz/z4Kr. </a>For information
            about
            N-Beats, see Boris N. Oreshkin, Dmitri Carpov, Nicolas Chapados, Yoshua Bengio, “N-BEATS:</p>
        <p>Neural basis expansion analysis for interpretable time series forecasting,” </p>
        <p>arXiv:1905.10437 (2019), <a href="https://arxiv.org/abs/1905.10437">
                https://arxiv.org/abs/1905.10437</a>. </p>
        <p>This gives you a brief overview of the automatic forecasting ecosystem. Note that this list
            is not exhaustive, as there are many more libraries for automated time series forecasting. </p>
        <p>You do not need to learn how to use each of the libraries I've presented. This is meant to
            be an overview of the different tools available. Each time series forecasting problem can require a
            different set of tools, but knowing how to use one of the libraries usually makes it easier to use a new
            one. Thus, we'll focus on the Prophet library for the rest of this book. </p>
        <p>As I mentioned, Prophet is a well-known and widely used library in the industry, and anyone
            doing time series forecasting will likely come across Prophet. In the next section, we'll explore the
            package in greater detail and learn about its advantages, limitations, and functionality before using it
            for
            forecasting. </p>
        <p> <i><b>19.2</b></i></p>
        <p> <i><b>Exploring Prophet</b></i></p>
        <p>Prophet is an open source library created by Meta that implements a forecasting procedure
            taking into account nonlinear trends with multiple seasonal periods, such as yearly, monthly, weekly,
            and
            daily. The package is available for use with Python. It allows you forecast rapidly with minimal manual
            work. More advanced users, such as ourselves, can fine tune the model to ensure that we get the best
            results
            possible. </p>
        <p>Under the hood, Prophet implements a general additive model where each time</p>
        <p>series <i>y</i>( <i>t</i>) is modeled as the linear
            combination of a trend <i>g</i>( <i>t</i>), a seasonal component <i class="calibre3">s</i>( <i>t</i>),
            holiday effects <i>h</i>( <i class="calibre3">t</i>), and an error term ϵ <i>t</i>, which is normally
            distributed.
            Mathematically, this is expressed as equation 19.1. </p>
        <p> <i> y</i>( <i>t</i>) = <i>g</i>( <i class="calibre3">t</i>) + <i>s</i>( <i>t</i>) + <i
                class="calibre3">h</i>( <i>t</i>) + ϵ <i>t</i> Equation 19.1</p>
        <p>The trend component models the non-periodic long-term changes in the time series. </p>
        <p>The seasonal component models the periodic change, whether it is yearly, monthly, </p>
        <p><a id="calibre_link-436"></a><b>364</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>weekly, or daily. The holiday effect occurs irregularly and potentially on more than one
            day. Finally, the error term represents any change in value that cannot be explained by the previous
            three
            components. </p>
        <p>Notice that this model does not take into account the time dependence of the data, unlike
            the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>) model, where
            future values are dependent on past values. Thus, this process is closer to fitting a curve to the data,
            rather than finding the underlying process. Although there is some loss of predictive information using
            this
            method, it comes with the advantage that it is very flexible, since it can accommodate multiple seasonal
            periods and changing trends. Also, it is robust to outliers and missing data, which is a clear advantage
            in
            a business context. </p>
        <p>The inclusion of multiple seasonal periods was motivated by the observation that human
            behavior produced multi-period seasonal time series. For example, the five-day work week can produce a
            pattern that repeats every week, while school break can produce a pattern that repeats every year. Thus,
            to
            take multiple seasonal periods into account, Prophet uses the Fourier series to model multiple periodic
            effects. Specifically, the seasonal component <i>s</i>( <i>t</i>) is
            expressed as equation 19.2, where <i>P</i> is the length of the seasonal period in
            days,
            and <i>N</i> is the number of terms in the Fourier series. </p>
        <p>Equation 19.2</p>
        <p>In equation 19.2, if we have a yearly seasonality, <i>P</i> = 365.25, as
            there are 365.25 days in a year. For a weekly seasonality, <i>P</i> = 7. <i class="calibre3">N</i> is simply
            the number of parameters we wish to use to estimate the seasonal
            component. This has the added benefit that the seasonal component's sensitivity can be tweaked depending
            on
            how many parameters <i>N</i> are estimated to model the seasonality. We'll look at this
            in
            section 19.4 when we explore the different functions of Prophet. By default, Prophet uses 10 terms to
            model
            the yearly seasonality and 3 terms to model the weekly seasonality. </p>
        <p>Finally, this model allows us to consider the effect of holidays. Holidays are irregular
            events that can have a clear impact on a time series. For example, events such as Black Friday in the
            United
            States can dramatically increase the attendance in stores or the sales on an ecommerce website.
            Similarly,
            Valentine's Day is probably a strong indicator of an increase in sales of chocolates and flowers.
            Therefore,
            to model the impact of holidays in a time series, Prophet lets us define a list of holidays for a
            specific
            country. Holiday effects are then incorporated in the model, assuming that they are all independent. If
            a
            data point falls on a holiday date, a parameter <i>Ki</i> is calculated to represent
            the
            change in the time series at that point in time. The larger the change, the greater the holiday effect.
        </p>
        <p>NOTE</p>
        <p>For more information on the inner workings of Prophet, I highly sug-</p>
        <p>gest that you read the official paper, Sean J. Taylor and Benjamin Letham, </p>
        <p>“Forecasting at Scale,” <i>PeerJ Preprints</i> 5:e3190v2 (2017), <a
                href="https://peerj.com/preprints/3190/">https://peerj.com/</a></p>
        <p><a href="https://peerj.com/preprints/3190/">preprints/3190/</a>. It contains a more detailed
            explanation of the library, including mathematical expressions and test results, while remaining
            accessible.
        </p>
        <p><a id="calibre_link-168"></a> <i><b>19.3</b></i></p>
        <p> <i><b>Basic forecasting with Prophet</b></i></p>
        <p><b>365</b></p>
        <p>The flexibility of Prophet can make it an attractive choice for rapid and accurate
            forecasting. However, it must not be considered to be a one-size-fits-all solution. The documentation
            itself
            specifies that Prophet works best with time series that have a strong seasonal effect with several
            seasons
            of historical data. Therefore, there may be situations where Prophet is not the ideal choice, but that's
            okay, since you have a variety of statistical and deep learning models in your tool belt to produce
            forecasts. </p>
        <p>Let's now dive deeper into Prophet and explore its functionality. </p>
        <p> <i><b>19.3</b></i></p>
        <p> <i><b>Basic forecasting with Prophet</b></i></p>
        <p>To accompany our exploration of Prophet's functionality, we'll use a dataset containing the
            historical daily minimum temperature recorded in Melbourne, Australia, between 1981 and 1990. Besides
            predicting the weather, this dataset can also help us identify long-term climate trends and determine if
            the
            daily minimum temperature is, for example, increasing over time. Our forecast horizon will be 1 year or
            365
            days. We thus wish to build a model that forecasts the next year of daily minimum temperatures. </p>
        <p>NOTE</p>
        <p>At any time, feel free to consult the source code for this chapter on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH19">https://github.com/marcopeix/TimeSeriesForecastingInPython/</a>
        </p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH19">tree/master/CH19.
            </a></p>
        <p>Prophet is as easy to install as any other Python package. It can then be imported in a
            Jupyter Notebook or Python script with the same syntax as when you use pandas or numpy. </p>
        <p>import numpy as np</p>
        <p>import pandas as pd</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>from fbprophet import Prophet</p>
        <p>A note on installing Prophet on Windows</p>
        <p>If you are using a Windows machine, it is highly recommended that you use Anaconda to
            perform any data science task. Trying to install Prophet through Anaconda the first time might result in
            an
            error. This is because a compiler must be installed in order for the package to function correctly on
            Windows. </p>
        <p>If you are using Anaconda, you can run the following commands in your Anaconda</p>
        <p>prompt to install Prophet successfully:</p>
        <p>conda install libpython m2w64-toolchain -c msys2</p>
        <p>conda install numpy cython matplotlib scipy pandas -c conda-forge</p>
        <p>conda install -c conda-forge pystan</p>
        <p>conda install -c conda-forge fbprophet</p>
        <p>The next step is, of course, to read the CSV file. </p>
        <p>df = pd.read_csv('../data/daily_min_temp.csv') </p>
        <p><a id="calibre_link-437"></a><img src="images/000104.jpg" alt="Image 146" class="calibre2" /></p>
        <p><b>366</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>We can now plot our time series. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['Temp'])</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Minimum temperature (deg C)')</p>
        <p>plt.xticks(np.arange(0, 3649, 365), np.arange(1981, 1991, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>The result is shown in figure 19.1. You'll see a clear yearly seasonality, which is
            expected, as temperature is generally higher during summer and lower during winter. </p>
        <p>We thus have a fairly large dataset with 10 seasons of data, which is a perfect scenario for
            using Prophet, as the library performs best when there is a strong seasonal effect with many historical
            seasonal periods. </p>
        <p>Figure 19.1</p>
        <p>Daily minimum temperature recorded in Melbourne from 1981 to 1991. There is a yearly
            seasonality, as expected, since it is hotter in the summer and colder in the winter. </p>
        <p><a id="calibre_link-244"></a> <i><b>19.3</b></i></p>
        <p> <i><b>Basic forecasting with Prophet</b></i></p>
        <p><b>367</b></p>
        <p>We can now move on to forecasting with Prophet. You will see how quickly you can obtain
            accurate forecasts using Prophet with very few manual steps. </p>
        <p>The first step is to rename our columns. Prophet expects to have a DataFrame with two
            columns: a date column named ds and a value column named y. The date column must have a format accepted
            by
            pandas&mdash;usually YYYY-MM-DD or YYYY-MM-DD</p>
        <p>HH:MM:SS. The y column contains the values to be forecast, and those values must be numeric,
            whether float or integer. In our case, the dataset has only two columns that are already in the correct
            format, so we only need to rename them. </p>
        <p>df.columns = ['ds', 'y']</p>
        <p>Next, we'll split our data into train and test sets. We'll keep the last 365 days for the
            test set, as this represents a full year. We'll then take the first 9 years of data for training. </p>
        <p>train = df[:-365]</p>
        <p>test = df[-365:] </p>
        <p>Prophet follows the sklearn API, where a model is initialized by creating an instance of the
            Prophet class, the model is trained using the fit method, and predictions are generated using the
            predict
            method. Therefore, we'll first initialize a Prophet model by creating an instance of the Prophet class.
            Note
            that throughout this chapter, we'll code using Prophet's naming convention. </p>
        <p>m = Prophet() </p>
        <p>Once it's initialized, we'll then fit the model on the train set. </p>
        <p>m.fit(train); </p>
        <p>We now have a model that is ready to produce forecasts, with only two lines of code. </p>
        <p>The next step is to create a DataFrame to hold the predictions from Prophet. We'll use the
            make_future_dataframe method and specify the number of periods, which is the number of days in our
            forecast
            horizon. In this case, we want 365 days of forecast, so that they can be compared to the actual values
            observed in the test set. </p>
        <p>future = m.make_future_dataframe(periods=365)</p>
        <p>All that's left to do is to generate the forecast using the predict method. </p>
        <p>forecast = m.predict(future)</p>
        <p>Take some time to appreciate the fact that we trained a model and obtained predictions using
            only four lines of code. One of the main benefits of automated forecasting libraries is that we can
            experiment quickly and fine-tune the models later to tailor them to the task at hand. </p>
        <p><a id="calibre_link-259"></a><b>368</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>However, our work is not done, since we wish to evaluate the model and measure</p>
        <p>its performance. The forecast DataFrame holds many columns with a lot of information, as
            shown in figure 19.2. </p>
        <p>ds</p>
        <p>trend</p>
        <p>yhat_lower trend_lower trend_upper</p>
        <p>additive_terms additive_terms_lower additive_terms_upper</p>
        <p>weekly</p>
        <p><b>3656 </b>1990-12-27 11.406616</p>
        <p>11.234023</p>
        <p>11.317184</p>
        <p>11.505689</p>
        <p>3.043416</p>
        <p>3.043416</p>
        <p>3.043416</p>
        <p>-0.026441</p>
        <p><b>3656 </b>1990-12-28 11.406528</p>
        <p>11.168300</p>
        <p>11.316559</p>
        <p>11.505902</p>
        <p>3.120759</p>
        <p>3.120759</p>
        <p>3.120759</p>
        <p>-0.009965</p>
        <p><b>3647 </b>1990-12-29 11.406441</p>
        <p>11.235604</p>
        <p>11.315997</p>
        <p>11.506198</p>
        <p>3.144845</p>
        <p>3.144845</p>
        <p>3.144845</p>
        <p>-0.048854</p>
        <p><b>3648 </b>1990-12-30 11.406353</p>
        <p>11.122686</p>
        <p>11.315449</p>
        <p>11.506547</p>
        <p>3.069314</p>
        <p>3.069314</p>
        <p>3.069314</p>
        <p>-0.188713</p>
        <p><b>3649 </b>1990-12-31 11.406265</p>
        <p>11.540265</p>
        <p>11.314879</p>
        <p>11.506889</p>
        <p>3.366551</p>
        <p>3.366551</p>
        <p>3.366551</p>
        <p>0.043655</p>
        <p>Figure 19.2</p>
        <p>The <b>forecast</b> <b>DataFrame</b> containing the
            different components of the prediction. Note that if you add trend with additive_terms, you get the
            prediction yhat, which is hidden in the figure because the <b>DataFrame</b> has too
            many
            columns. Note also that additive_terms is the sum of weekly and yearly, indicating that we have both
            weekly
            and yearly seasonality. </p>
        <p>We are only interested in these four columns: ds, yhat, yhat_lower, and yhat_upper. </p>
        <p>The ds column simply has the datestamp of the forecast. The yhat column contains the value
            of the forecast. You can see how Prophet uses y for the actual value and yhat for the predicted value as
            a
            naming convention. Then, yhat_lower and yhat_upper represent the lower and upper bounds of the 80%
            confidence interval of the forecast. </p>
        <p>This means that there is an 80% chance that the forecast will fall between yhat_lower and
            yhat_upper, with yhat being the value that we expect to obtain. </p>
        <p>We can now join test and forecast together, to create a single DataFrame holding both the
            actual and predicted values. </p>
        <p>test[['yhat', 'yhat_lower', 'yhat_upper']] = forecast[['yhat', </p>
        <p>➥ 'yhat_lower', 'yhat_upper']]</p>
        <p>Before evaluating our mode, let's implement a baseline, as our model can only be better in
            relation to a certain benchmark. Here, let's apply the last season naive forecasting method, meaning
            that
            the last year of the training set is repeated as the forecast for next year. </p>
        <p>test['baseline'] = train['y'][-365:].values</p>
        <p>Everything is set up to easily evaluate our model. We'll use the mean absolute error (MAE)
            for its ease of interpretation. Note that the mean absolute percentage error (MAPE) is not suitable in
            this
            situation, because we have values that are close to 0, in which case the MAPE gets inflated. </p>
        <p>from sklearn.metrics import mean_absolute_error</p>
        <p>prophet_mae = mean_absolute_error(test['y'], test['yhat'])</p>
        <p>baseline_mae = mean_absolute_error(test['y'], test['baseline'])</p>
        <p><a id="calibre_link-209"></a> <i><b>19.3</b></i></p>
        <p> <i><b>Basic forecasting with Prophet</b></i></p>
        <p><b>369</b></p>
        <p>This returns a baseline MAE of 2.87, while the MAE achieved by the Prophet model is 1.94.
            Therefore, we achieve a lower MAE using Prophet, meaning that it is indeed better than the baseline.
            This
            means that, on average, our model predicts the daily minimum temperature with a difference of 1.94
            degrees
            Celsius, either above or below the observed value. </p>
        <p>We can optionally plot the forecasts, as well as the confidence interval from Prophet. The
            result is shown in figure 19.3. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(train['y'])</p>
        <p>ax.plot(test['y'], 'b-', label='Actual')</p>
        <p>ax.plot(test['yhat'], color='darkorange', ls='--', lw=3, label='Predictions')</p>
        <p>ax.plot(test['baseline'], 'k:', label='Baseline')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Minimum temperature (deg C)')</p>
        <p>ax.axvspan(3285, 3649, color='#808080', alpha=0.1)</p>
        <p>ax.legend(loc='best')</p>
        <p>plt.xticks(</p>
        <p>[3224, 3254, 3285, 3316, 3344, 3375, 3405, 3436, 3466, 3497, 3528, </p>
        <p>➥ 3558, 3589, 3619], </p>
        <p>['Nov', 'Dec', 'Jan 1990', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', </p>
        <p>➥ 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])</p>
        <p>plt.fill_between(x=test.index, y1=test['yhat_lower'], y2=test['yhat_upper'], </p>
        <p>➥ color='lightblue')</p>
        <p>plt.xlim(3200, 3649)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>You'll see that the Prophet forecast looks more like a curve-fitting procedure, since its
            forecast, shown as a dashed line in figure 19.3, is a smooth curve that seems to filter the noisier
            fluctuations in the data. </p>
        <p>Using Prophet allowed us to generate accurate forecasts with very few lines of code.
            However, we have only scratched the surface in terms of Prophet's functionality. </p>
        <p>This is only the basic workflow of using Prophet. In the next section, we'll explore more
            advanced Prophet functions, such as visualization techniques and fine-tuning procedures, as well as
            cross-validation and evaluation methods. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-169"></a><img class="calibre2" src="images/000074.png" alt="Image 147" /></p>
        <p><b>370</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Actual</p>
        <p>Predictions</p>
        <p>25</p>
        <p>Baseline</p>
        <p>20</p>
        <p>15</p>
        <p>10</p>
        <p>Minimum temperature (deg C)</p>
        <p>5</p>
        <p>0</p>
        <p>Nov</p>
        <p>Dec</p>
        <p>Feb</p>
        <p>Apr</p>
        <p>May</p>
        <p>Jun</p>
        <p>Jul</p>
        <p>Aug</p>
        <p>Sep</p>
        <p>Oct</p>
        <p>Nov</p>
        <p>Dec</p>
        <p>1990</p>
        <p>Mar</p>
        <p>Jan</p>
        <p>Date</p>
        <p>Figure 19.3</p>
        <p>Forecasting the daily minimum temperature for the year 1990. We can see that the forecast
            from Prophet, shown as a dashed line, is smoother than the baseline, clearly demonstrating the
            curve-fitting
            property of Prophet. </p>
        <p> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p>We'll now explore Prophet's more advanced functionality. These advanced functions can be
            separated into three categories: visualization, performance diagnosis, and hyperparameter tuning. We'll
            work
            with the same dataset as in the previous section, and I highly recommend that you work in the same
            Jupyter
            Notebook or Python script as before. </p>
        <p> <i><b>19.4.1 Visualization capabilities</b></i></p>
        <p>Prophet comes with many methods that allow us to quickly visualize a model's predictions or
            its different components. </p>
        <p>First of all, we can quickly generate a plot of our forecasts simply by using the plot
            method. The result is shown in figure 19.4. </p>
        <p>fig1 = m.plot(forecast)</p>
        <p><a id="calibre_link-282"></a><img src="images/000076.jpg" alt="Image 148" class="calibre2" /></p>
        <p> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p><b>371</b></p>
        <p>Figure 19.4</p>
        <p>Plotting our predictions using Prophet. The black dots represent the training data, while
            the solid continuous line represents the model's predictions. The shaded band surrounding the line
            represents an 80% </p>
        <p>confidence interval. </p>
        <p>We can also display the different components used in our model with the plot_</p>
        <p>components method. </p>
        <p>fig2 = m.plot_components(forecast)</p>
        <p>The resulting plot is shown in figure 19.5. The top plot shows the trend component, as well
            as the uncertainty in the trend for the forecast period. Looking closely, you'll see that the trend
            changes
            over time, with there being six different trends. We'll explore that in more detail later. </p>
        <p>The two bottom plots in figure 19.5 show two different seasonal components: one</p>
        <p>with a weekly period and the other with a yearly period. The yearly seasonality makes sense,
            as the summer months (December to February, since Australia is in the south-ern hemisphere) see hotter
            temperatures than the winter months (June to August). </p>
        <p>However, the weekly seasonal component is rather odd. While it may help the model produce a
            better forecast, I doubt there is a meteorological phenomenon that can explain weekly seasonality in
            daily
            minimum temperatures. Thus, this component likely helps the model achieve a better fit and a better
            forecast, but it is hard to explain its presence. </p>
        <p><a id="calibre_link-288"></a><img src="images/000071.jpg" alt="Image 149" class="calibre2" /></p>
        <p><b>372</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Figure 19.5</p>
        <p>Displaying the components of our model. Here our model uses a trend component and two
            different seasonal components&mdash;one with a weekly period and the other with a yearly period. </p>
        <p>Alternatively, Prophet allows us to plot only the seasonal component. Specifically, we can
            plot the weekly seasonality using the plot_weekly method or the yearly seasonality with the plot_yearly
            method. The result for the latter is shown in figure 19.6. </p>
        <p>from fbprophet.plot import plot_yearly, plot_weekly</p>
        <p>fig4 = plot_yearly(m)</p>
        <p><a id="calibre_link-438"></a><img src="images/000069.jpg" alt="Image 150" class="calibre2" /></p>
        <p> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p><b>373</b></p>
        <p>Figure 19.6</p>
        <p>Plotting the yearly seasonal component of our data. This is equivalent to the third plot in
            figure 19.5. </p>
        <p>You'll recognize the yearly seasonal component of our data, as it is the same plot as the
            third plot in figure 19.5. However, this method allows us to visualize how changing the number of terms
            to
            estimate the seasonal component can impact our model. </p>
        <p>Recall that Prophet uses 10 terms in the Fourier series to estimate the yearly seasonality.
            Now let's visualize the seasonal component if 20 terms are used for the estima-tion. </p>
        <p>m2 = Prophet(yearly_seasonality=20).fit(train)</p>
        <p>fig6 = plot_yearly(m2)</p>
        <p>In figure 19.7 the yearly seasonal component shows more fluctuation than in figure 19.6,
            meaning that it is more sensitive. Tuning this parameter can lead to overfitting if too many terms are
            used,
            or to underfitting if we reduce the number of terms in the Fourier series. This parameter is rarely
            changed,
            but it is interesting to see that Prophet comes with this fine-tuning functionality. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-170"></a><img src="images/000141.jpg" alt="Image 151" class="calibre2" /></p>
        <p><b>374</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Figure 19.7</p>
        <p>Using 20 terms to estimate the yearly seasonal component in our data. Compared to figure
            19.6, this view of the seasonal component is more sensitive, since it shows more variation across time.
            This
            can potentially lead to overfitting. </p>
        <p>Finally, we saw in figure 19.5 that the trend changed over time, and we could identify six
            unique trends. Prophet can identify these trend <i>changepoints</i>. We can visualize
            them
            using the add_changepoints_to_plot method. </p>
        <p>from fbprophet.plot import add_changepoints_to_plot</p>
        <p>fig3 = m.plot(forecast)</p>
        <p>a = add_changepoints_to_plot(fig3.gca(), m, forecast)</p>
        <p>The result is shown in figure 19.8. Notice that Prophet identifies points in time where the
            trend changes. </p>
        <p>We've explored the most important visualization capabilities of Prophet, so let's move on to
            using cross-validation to diagnose our model in more detail. </p>
        <p> <i><b>19.4.2 Cross-validation and performance
                    metrics</b></i></p>
        <p>Prophet comes with an important cross-validation capability, allowing us to forecast over
            multiple periods in our dataset to ensure that we have a stable model. This is similar to a rolling
            forecast
            procedure. </p>
        <p><a id="calibre_link-234"></a><img src="images/000080.jpg" alt="Image 152" class="calibre2" /></p>
        <p> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p><b>375</b></p>
        <p>Figure 19.8</p>
        <p>Showing trend changepoints in our model. Each point where the trend changes is identified by
            a vertical dashed line. Notice that there are six vertical dashed lines, matching the six different
            trend
            slopes in the top plot of figure 19.5. </p>
        <p>Recall that with time series, the order of the data must remain the same. Therefore,
            cross-validation is performed by training the model on a subset of the training data and forecasting on
            a
            certain horizon. Figure 19.9 shows how we start by defining a subset within the training set and use it
            to
            fit the model and generate predictions. Then we add more data to the initial subset, and predict for
            another
            period of time. This process is repeated until the entire training set is used. </p>
        <p>You'll notice the resemblance to rolling forecasts, but this time we are using this
            technique for cross-validation, to ensure that we have a stable model. A stable model is one with an
            evaluation metric that is fairly constant over each forecast period, keeping the horizon constant. In
            other
            words, the performance of our model should be constant, whether it must forecast 365 days starting in
            January or starting in July. </p>
        <p>Prophet's cross_validation function requires a Prophet model that has been fit</p>
        <p>to training data. Then we must specify an initial length for the training set in the
            cross-validation process, denoted as initial. The next parameter is the length of time separating each
            cutoff date, denoted as period. Finally, we must specify the horizon of the forecast, denoted as
            horizon.
            These three parameters must have units that are compatible with the pandas.Timedelta class <a
                href="https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html">(https://pandas.pydata.org/docs/refer-</a>
        </p>
        <p><a
                href="https://pandas.pydata.org/docs/reference/api/pandas.Timedelta.html">ence/api/pandas.Timedelta.html</a>).
            In other words, the largest unit is days, and the</p>
        <p><a id="calibre_link-439"></a><b>376</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Initial</p>
        <p>Horizon</p>
        <p>Step 1</p>
        <p>Training</p>
        <p>Cutoff</p>
        <p>Horizon</p>
        <p>Step 2</p>
        <p>Training</p>
        <p>Cutoff</p>
        <p>Horizon</p>
        <p>Step 3</p>
        <p>Training</p>
        <p>Cutoff</p>
        <p>Horizon</p>
        <p>Step <i>n</i></p>
        <p>Training</p>
        <p>Cutoff</p>
        <p>Figure 19.9</p>
        <p>Illustrating the cross-validation procedure in Prophet. The entire rectangle </p>
        <p>represents the training set, and an initial subset of the set is identified to fit the
            model. At a certain cutoff date, the model produces a forecast over a set horizon. In the next step,
            more
            data is added to the training subset, and the model makes predictions over another period of time. </p>
        <p>The process is then repeated until the horizon exceeds the length of the training set. </p>
        <p>smallest unit is nanoseconds. Anything in between, such as hours, minutes, seconds, or
            milliseconds, will work as well. </p>
        <p>By default, Prophet uses horizon to determine the length of initial and period. </p>
        <p>It sets initial to three times the length of horizon and period to half the length of
            horizon. Of course, we can tweak this behavior to meet our needs. </p>
        <p>Let's start with an initial training period of 730 days, which represents two years of data.
            The horizon will be 365 days, and each cutoff date will be separated by 180 days, which is roughly half
            a
            year. Given our training set size, our cross-validation procedure</p>
        <p><a id="calibre_link-279"></a> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p><b>377</b></p>
        <p>has 13 steps. The output of the procedure is a DataFrame with the datestamp, the forecast,
            its upper and lower bounds, the actual value, and the cutoff date, as shown in figure 19.10. </p>
        <p>from fbprophet.diagnostics import cross_validation</p>
        <p>df_cv = cross_validation(m, initial='730 days', period='180 days', </p>
        <p>➥ horizon='365 days') </p>
        <p><b>The initial training set has 2 years of data. Each </b></p>
        <p><b>cutoff date is separated by 180 days, or half a year. </b></p>
        <p>df_cv.head()</p>
        <p><b>The forecast horizon is 365 days, which is a year. </b></p>
        <p><b>ds</b></p>
        <p><b>yhat</b></p>
        <p><b>yhat_lower</b></p>
        <p><b>yhat_upper</b></p>
        <p><b>y</b></p>
        <p><b>cutoff</b></p>
        <p><b>0</b></p>
        <p>1983-02-02</p>
        <p>15.156298</p>
        <p>11.393460</p>
        <p>18.821358</p>
        <p>17.3</p>
        <p>1983-02-01</p>
        <p><b>1</b></p>
        <p>1983-02-03</p>
        <p>14.818082</p>
        <p>11.443539</p>
        <p>18.180941</p>
        <p>13.0</p>
        <p>1983-02-01</p>
        <p><b>2</b></p>
        <p>1983-02-04</p>
        <p>15.212860</p>
        <p>11.629483</p>
        <p>18.580597</p>
        <p>16.0</p>
        <p>1983-02-01</p>
        <p><b>3</b></p>
        <p>1983-02-05</p>
        <p>15.203778</p>
        <p>11.808610</p>
        <p>18.677870</p>
        <p>14.9</p>
        <p>1983-02-01</p>
        <p><b>4</b></p>
        <p>1983-02-06</p>
        <p>15.250535</p>
        <p>11.780555</p>
        <p>18.771718</p>
        <p>16.2</p>
        <p>1983-02-01</p>
        <p>Figure 19.10</p>
        <p>The first five rows of our cross-validation <b>DataFrame</b>. We can </p>
        <p>see the predictions, the upper and lower bounds, as well as the cutoff date. </p>
        <p>With cross-validation done, we can use the performance_metrics function to evaluate the
            performance of the model over the multiple forecasting periods. We pass in the output of
            cross-validation,
            which is df_cv, and we set the rolling_window parameter. </p>
        <p>This parameter determines the portion of data over which we want to compute the error
            metric. Setting it to 0 means that each evaluation metric is computed for each forecast point. Setting
            it to
            1 averages the evaluation metrics over the entire horizon. </p>
        <p>Here, let's set it to 0. </p>
        <p>from fbprophet.diagnostics import performance_metrics</p>
        <p>df_perf = performance_metrics(df_cv, rolling_window=0)</p>
        <p>df_perf.head()</p>
        <p>The output of this procedure is shown in figure 19.11. The MAPE is not included, since
            Prophet automatically detected that we have values close to 0, which makes the MAPE an unsuitable
            evaluation
            metric. </p>
        <p>Finally, we can visualize the evolution of an evaluation metric over the horizon. </p>
        <p>This allows us to determine whether the error increases as the model predicts further in
            time or if it remains relatively stable. Again, we'll use the MAE, as this is how we first evaluated our
            model. </p>
        <p>from fbprophet.plot import plot_cross_validation_metric</p>
        <p>fig7 = plot_cross_validation_metric(df_cv, metric='mae')</p>
        <p><a id="calibre_link-291"></a><img src="images/000008.jpg" alt="Image 153" class="calibre2" /></p>
        <p><b>378</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p><b>horizon</b></p>
        <p><b>mse</b></p>
        <p><b>rmse</b></p>
        <p><b>mae</b></p>
        <p><b>mdape</b></p>
        <p><b>coverage</b></p>
        <p>Figure 19.11</p>
        <p>The first five </p>
        <p><b>0</b></p>
        <p>1 days</p>
        <p>6.350924</p>
        <p>2.520104</p>
        <p>2.070329</p>
        <p>0.147237</p>
        <p>0.846154</p>
        <p>rows of the evaluation </p>
        <p><b>DataFrame</b>. We can see </p>
        <p><b>1</b></p>
        <p>2 days</p>
        <p>4.685452</p>
        <p>2.164590</p>
        <p>1.745606</p>
        <p>1.139852</p>
        <p>0.846154</p>
        <p>different performance metrics </p>
        <p><b>2</b></p>
        <p>3 days</p>
        <p>10.049956</p>
        <p>3.170167</p>
        <p>2.661797</p>
        <p>0.147149</p>
        <p>0.769231</p>
        <p>over different horizons, </p>
        <p>allowing us to visualize how </p>
        <p><b>3</b></p>
        <p>4 days</p>
        <p>8.686183</p>
        <p>2.947233</p>
        <p>2.377724</p>
        <p>0.195119</p>
        <p>0.769231</p>
        <p>the performance varies </p>
        <p><b>4</b></p>
        <p>5 days</p>
        <p>8.250061</p>
        <p>2.872292</p>
        <p>2.569552</p>
        <p>0.196067</p>
        <p>0.692308</p>
        <p>according to the horizon. </p>
        <p>The result is shown in figure 19.12. Ideally, we will see a fairly flat line, like in figure
            19.12, as it means that the error in our predictions does not increase as the model predicts further in
            time. If the error increases, we should revise the forecast horizon or make sure that we are comfortable
            with an increasing error. </p>
        <p>Figure 19.12</p>
        <p>Evolution of the MAE over the forecast horizon. Each dot represents the absolute error for
            one of the 13 forecast periods, while the solid line averages them over time. The line is fairly flat,
            meaning that we have a stable model where the error does not increase as it predicts further in time.
        </p>
        <p>Now that you've seen Prophet's cross-validation capability, we'll look at hyperparameter
            tuning. Combining the two will result in a robust way of finding an optimal model for our problem. </p>
        <p><a id="calibre_link-171"></a> <i><b>19.4</b></i></p>
        <p> <i><b>Exploring Prophet's advanced functionality</b></i>
        </p>
        <p><b>379</b></p>
        <p> <i><b>19.4.3 Hyperparameter tuning</b></i></p>
        <p>We can combine hyperparameter tuning and cross-validation in Prophet to design a robust
            process that automatically identifies the best parameter combination to fit our data. </p>
        <p>Prophet comes with many parameters that can be fine-tuned by more advanced</p>
        <p>users in order to produce better forecasts. Four parameters are usually tuned:
            changepoint_prior_scale, seasonality_prior_scale, holidays_prior_scale, and</p>
        <p>seasonality_mode. Other parameters can technically be changed, but they are often redundant
            forms of the preceding parameters:</p>
        <p> changepoint_prior_scale&mdash;The changepoint_prior_scale parameter is said</p>
        <p>to be the most impactful parameter in Prophet. It determines the flexibility of</p>
        <p>the trend, and particularly how much the trend changes at the trend change-</p>
        <p>points. If the parameter is too small, the trend will underfit, and the variance observed in
            the data will be treated as noise. If it is set too high, the trend will overfit to noisy fluctuations.
            Using the range [0.001, 0.01, 0.1, 0.5] is enough to have a well-fitted model. </p>
        <p> seasonality_prior_scale&mdash;The seasonality_prior_scale parameter sets the</p>
        <p>flexibility of the seasonality. A large value allows the seasonal component to fit smaller
            fluctuations, while a small value will result in a smoother seasonal component. Using the range [0.01,
            0.1,
            1.0, 10.0] generally works well to find a good model. </p>
        <p> holidays_prior_scale&mdash;The holidays_prior_scale parameter sets the flexi-</p>
        <p>bility of the holiday effects and works just like seasonality_prior_scale. It can be tuned
            using the same range, [0.01, 0.1, 1.0, 10.0]. </p>
        <p> seasonality_mode&mdash;The seasonality_mode parameter can be either additive</p>
        <p>or multiplicative. By default, it is additive, but it can be set to multiplicative if you
            see that the seasonal fluctuation gets larger over time. This can be observed by plotting the time
            series,
            but when in doubt, you can include it in the hyperparameter tuning process. Our current dataset of
            historical daily minimum temperature is a great example of additive seasonality, as the yearly
            fluctuations
            do not increase over time. An example of multiplicative seasonality is shown in</p>
        <p>figure 19.13. </p>
        <p>Let's combine hyperparameter tuning and cross-validation to find the best model parameters
            for forecasting the daily minimum temperature. We'll use only changepoint_prior_scale and
            seasonality_prior_scale in this example, since we do not</p>
        <p>have any holiday effects and our seasonal component is additive. </p>
        <p>We'll first define the range of values to try for each parameter and generate a list of
            unique combinations of parameters. Then, for each unique combination of parameters, we'll train a model
            and
            perform cross-validation. We will then evaluate the model using a rolling_window of 1 to speed up the
            process and average the evaluation metric over the entire forecasting period. We'll finally store the
            parameter combinations</p>
        <p><a id="calibre_link-440"></a><img src="images/000021.jpg" alt="Image 154" class="calibre2" /></p>
        <p><b>380</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Figure 19.13</p>
        <p>Example of multiplicative seasonality. This is taken from the capstone </p>
        <p>project in chapter 11, where we predicted the monthly volume of antidiabetic drug
            prescriptions in Australia. We not only saw a yearly seasonality, but we also noticed that the
            fluctuations
            get larger as we move through time. </p>
        <p>and their associated MAE to find the best parameter combination. The combination with the
            lowest MAE will be deemed to be the best. We'll use the MAE because we have been using it since the
            beginning of this project. </p>
        <p>from itertools import product</p>
        <p>param_grid = {</p>
        <p>'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5], </p>
        <p>'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0]</p>
        <p>}</p>
        <p>all_params = [dict(zip(param_grid.keys(), v)) for v in </p>
        <p>➥ product(*param_grid.values())] </p>
        <p><b>Create a list of unique </b></p>
        <p><b>parameter combinations. </b></p>
        <p>maes = []</p>
        <p><b>For each unique combination, </b></p>
        <p><b>Fit a</b></p>
        <p>for params in all_params: </p>
        <p><b>do the next three steps. </b></p>
        <p><b>model. </b></p>
        <p>m = Prophet(**params).fit(train) </p>
        <p>df_cv = cross_validation(m, initial='730 days', period='180 days', </p>
        <p>➥ horizon='365 days', parallel='processes') </p>
        <p><b>Perform cross-</b></p>
        <p>df_p = performance_metrics(df_cv, rolling_window=1) </p>
        <p><b>validation. We </b></p>
        <p>maes.append(df_p['mae'].values[0])</p>
        <p><b>can speed up the </b></p>
        <p><b>Evaluate the model</b></p>
        <p></p>
        <p><b>process by using </b></p>
        <p>tuning_results = pd.DataFrame(all_params) </p>
        <p><b>with a rolling_window</b></p>
        <p><b>parallelization. </b></p>
        <p><b>of 1. This averages the</b></p>
        <p>tuning_results['mae'] = maes</p>
        <p><b>performance over the</b></p>
        <p><b>Organize the results in a DataFrame. </b></p>
        <p><b>entire forecast horizon. </b></p>
        <p><a id="calibre_link-172"></a> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>381</b></p>
        <p>The parameters achieving the lowest MAE can now be found:</p>
        <p>best_params = all_params[np.argmin(maes)]</p>
        <p>In this case, both changepoint_prior_scale and seasonality_prior_scale should</p>
        <p>be set to 0.01. </p>
        <p>This concludes our exploration of Prophet's advanced functionality. We have mostly worked
            with them in discovery mode, so let's solidify what you've learned by designing and implementing a
            forecast
            that uses Prophet's more advanced functions, such as cross-validation and hyperparameter tuning, to
            automate
            the forecasting process. </p>
        <p> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process </b></i>
        </p>
        <p> <i><b>with Prophet</b></i></p>
        <p>Having explored Prophet's advanced functionality, we'll now design a robust and automated
            forecasting process with Prophet. This step-by-step system will allow us to automatically find the best
            model that Prophet can build for a particular problem. </p>
        <p>Keep in mind that finding the best Prophet model does not mean that Prophet is</p>
        <p>the optimal solution to all problems. This process will simply identify the best possible
            outcome when using Prophet. It is recommended that you test various models, using either deep learning
            or
            statistical techniques, along with a baseline model, of course, to ensure that you find the best
            possible
            solution to your forecasting problem. </p>
        <p>Figure 19.14 illustrates the forecasting process with Prophet to ensure that we obtain the
            optimal Prophet model. We'll first ensure that the columns are named and formatted correctly for
            Prophet.
            Then we'll combine cross-validation and hyperparameter tuning to obtain the best parameter combination,
            fit
            the model, and evaluate it on a test set. It is a fairly straightforward process, which is to be
            expected.
            Prophet does much of the heavy lifting for us, allowing us to quickly experiment and come up with a
            model.
        </p>
        <p>Let's apply this procedure to yet another forecasting project. It involves monthly data,
            which Prophet handles in a particular way. Furthermore, we'll work with data that is potentially
            affected by
            holiday effects, giving us the opportunity to work with a function of Prophet that we have not explored
            yet.
        </p>
        <p> <i><b>19.5.1 Forecasting project: Predicting the
                    popularity of “chocolate” </b></i></p>
        <p> <i><b>searches on Google</b></i></p>
        <p>For this project, we'll try to predict the popularity of the search term “chocolate” on
            Google. Predicting the popularity of search terms can help marketing teams better optimize their bidding
            for
            a particular keyword, which of course impacts the cost-per-click on an ad, ultimately affecting the
            entire
            return on investment of a marketing campaign. It can also give insight into consumer behavior. For
            example,
            if we know that next month is likely to see a surge in people searching for chocolate, it can make sense
            for
            a chocolate shop to offer discounts and ensure that they have enough supply to meet the demand. </p>
        <p><a id="calibre_link-441"></a><b>382</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Rename date column to "ds" and</p>
        <p>value column to "y" </p>
        <p>Format date column as</p>
        <p>YYYY-MM-DD or YYYY-MM-DD</p>
        <p>HH:MM:SS</p>
        <p>Find optimal parameters using</p>
        <p>cross-validation and</p>
        <p>hyperparameter tuning</p>
        <p>Fit model with optimal parameters</p>
        <p>Figure 19.14</p>
        <p>Forecasting process using Prophet. </p>
        <p>First, we'll ensure that the dataset has the right column </p>
        <p>names for Prophet and that the date is expressed </p>
        <p>correctly as a datestamp or a timestamp. Then, we'll </p>
        <p>combine hyperparameter tuning with cross-validation to </p>
        <p>Evaluate the model</p>
        <p>obtain the optimal parameters for our model. We'll </p>
        <p>finally fit the model using the optimal parameters and </p>
        <p>evaluate it on a test set. </p>
        <p>The data for this project comes directly from Google Trends <a
                href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=chocolate">(https://trends.google</a>
        </p>
        <p><a href="https://trends.google.com/trends/explore?date=all&amp;geo=US&amp;q=chocolate">.com/trends/explore?date=all&amp;geo=US&amp;q=chocolate),
            </a>and it shows the monthly popularity of the keyword “chocolate” in the United States, from 2004 to
            today.
            Note that this chapter was written before the end of 2021, so visiting the link now will not result in
            the
            exact same dataset. I have included the dataset I used as a CSV file on GitHub to ensure that you can
            recreate the work presented here. </p>
        <p>We'll kick off this project by reading the data. </p>
        <p>df = pd.read_csv('../data/monthly_chocolate_search_usa.csv')</p>
        <p>The dataset contains 215 rows of data from January 2014 to December 2021. The dataset also
            has two columns: one with the year and month, and one with the measured popularity of “chocolate”
            searches.
            We can plot the evolution of the keyword searches over time&mdash;the result is shown in figure 19.15.
            The
            plot shows strongly seasonal data</p>
        <p><a id="calibre_link-442"></a><img class="calibre2" src="images/000045.png" alt="Image 155" /></p>
        <p> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>383</b></p>
        <p>100</p>
        <p>90</p>
        <p>” </p>
        <p>80</p>
        <p>“chocolate</p>
        <p>70</p>
        <p>60</p>
        <p>50</p>
        <p>40</p>
        <p>Proportion of searches using the keyword</p>
        <p>30</p>
        <p>1</p>
        <p>2004</p>
        <p>2005 2006 2007 2008 2009 2010 201</p>
        <p>2012 2013 2014 2015 2016 2017 2018 2019 2020 2021</p>
        <p>Date</p>
        <p>Figure 19.15</p>
        <p>Popularity of the keyword “chocolate” in Google searches in the United States from January
            2004 to December 2021. The values are expressed as a proportion relative to the period where the search
            term
            was the most popular, which occurs in December 2020 and has a value of 100. Therefore, a value of 50 for
            a
            particular month means that the keyword “chocolate” was searched for half as often, relative to the
            month of
            December 2020. </p>
        <p>with repeated peaks every year. We can also see a clear trend, as the data increases over
            time. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['chocolate'])</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Proportion of searches using the keyword "chocolate"')</p>
        <p>plt.xticks(np.arange(0, 215, 12), np.arange(2004, 2022, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>There are two elements that make this dataset very interesting to model with Prophet. </p>
        <p>First, it is likely that we have holiday effects in action. For example, Christmas is a</p>
        <p><a id="calibre_link-443"></a><b>384</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>holiday in the United States, and it is quite common to offer chocolate for Christmas. </p>
        <p>The next element is that we have monthly data. While Prophet can be used to model monthly
            data, some tweaking must be done to ensure that we get good results. Out of the box, Prophet can work
            with
            daily and sub-daily data, but monthly data requires a bit of extra work. </p>
        <p>Following our forecasting process with Prophet, shown earlier in figure 19.14, we'll start
            by renaming our columns following Prophet's naming convention. Recall that Prophet expects the date
            column
            to be named ds, while the value column must be named y. </p>
        <p>df.columns = ['ds', 'y'] </p>
        <p>We can now move on to verifying that the date is correctly formatted. In this case, we only
            have the year and the month, which does not respect the YYYY-MM-DD format</p>
        <p>expected by Prophet for a datestamp. We'll therefore add a day to our date column. </p>
        <p>In this case, we have monthly data, which can only be obtained at the end of the month, so
            we'll add the last day of the month to the datestamp. </p>
        <p>from pandas.tseries.offsets import MonthEnd</p>
        <p>df['ds'] = pd.to_datetime(df['ds']) + MonthEnd(1)</p>
        <p>Before we dive into hyperparameter tuning, we'll first split our data into train and test
            sets, so we can perform hyperparameter tuning on the training set only and avoid data leakage. In this
            case,
            we'll keep the last twelve months for the test set. </p>
        <p>train = df[:-12]</p>
        <p>test = df[-12:]</p>
        <p>We'll now move on to the next step, where we'll combine hyperparameter tuning and
            cross-validation to find the optimal parameter combination for our model. Just as we did before, we'll
            define a range of values for each parameter we wish to tune and build a list containing each unique
            combination of values. </p>
        <p>param_grid = {</p>
        <p>'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5], </p>
        <p>'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0]</p>
        <p>}</p>
        <p>params = [dict(zip(param_grid.keys(), v)) for v in </p>
        <p>product(*param_grid.values())]</p>
        <p>NOTE</p>
        <p>We will not optimize for holidays_prior_scale to save on time here, </p>
        <p>but feel free to add it as a tunable parameter with the following range of val-</p>
        <p>ues: [0.01, 0.1, 1.0, 10.0]. </p>
        <p><a id="calibre_link-198"></a> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>385</b></p>
        <p>Next, we'll create a list to hold the evaluation metric that we'll use to decide on the set
            of optimal parameters. We'll use the MSE, because it penalizes large errors during the fitting process.
        </p>
        <p>mses = []</p>
        <p>Now, because we are working with monthly data, we must define our own cutoff dates. </p>
        <p>Recall that the cutoff dates define the training and testing periods during
            cross-validation, as shown in figure 19.16. Therefore, when working with monthly data, we must define
            our
            own list of cutoff dates to specify the initial training period and forecasting period for each step
            during
            the cross-validation process. This is a workaround that allows us to work with monthly data using
            Prophet.
        </p>
        <p>Initial</p>
        <p>Horizon</p>
        <p>Training</p>
        <p>Cutoff</p>
        <p>Figure 19.16</p>
        <p>The cutoff date sets a boundary between the training period and the </p>
        <p>forecast horizon during cross-validation. By defining a list of cutoff dates, we can specify
            the initial training period and forecast period for each step during cross-validation. </p>
        <p>Here we'll set the initial training period to be the first 5 years of data. Therefore, our
            first cutoff date will be 2009-01-31. The last cutoff date can be set as the last row of the training
            set,
            and we'll separate each cutoff date by 12 months, so that we have a model that forecasts a full year.
        </p>
        <p>cutoffs = pd.date_range(start='2009-01-31', end='2020-01-31', freq='12M') </p>
        <p><b>The first cutoff date is 2009-01-31, giving us 5 years of initial
                training data on the first</b> <b>step of cross-validation. Each cutoff is
                separated by
                12 months until the end of the</b> <b>training set, resulting in a forecast horizon
                of
                1 year. </b></p>
        <p>With this step done, we can test each parameter combination using cross-validation and store
            their MSEs in a DataFrame. Note that we'll add the holidays effect with the simple add_country_holidays
            method, and we'll specify the country, which is the United States in this case. </p>
        <p>for param in params:</p>
        <p><b>Add the dates of </b></p>
        <p><b>the holidays in the </b></p>
        <p>m = Prophet(**param)</p>
        <p><b>United States. </b></p>
        <p>m.add_country_holidays(country_name='US') </p>
        <p>m.fit(train)</p>
        <p></p>
        <p>df_cv = cross_validation(model=m, horizon='365 days', cutoffs=cutoffs)</p>
        <p>df_p = performance_metrics(df_cv, rolling_window=1)</p>
        <p>mses.append(df_p['mse'].values[0])</p>
        <p></p>
        <p><a id="calibre_link-444"></a><b>386</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>tuning_results = pd.DataFrame(params)</p>
        <p>tuning_results['mse'] = mses</p>
        <p>The full code for hyperparameter tuning is shown in the following listing. </p>
        <p>Listing 19.1</p>
        <p>Hyperparameter tuning in Prophet with monthly data</p>
        <p>param_grid = {</p>
        <p>'changepoint_prior_scale': [0.001, 0.01, 0.1, 0.5], </p>
        <p>'seasonality_prior_scale': [0.01, 0.1, 1.0, 10.0]</p>
        <p>}</p>
        <p>params = [dict(zip(param_grid.keys(), v)) for v in </p>
        <p>➥ product(*param_grid.values())]</p>
        <p>mses = []</p>
        <p>cutoffs = pd.date_range(start='2009-01-31', end='2020-01-31', freq='12M')</p>
        <p>for param in params:</p>
        <p>m = Prophet(**param)</p>
        <p>m.add_country_holidays(country_name='US')</p>
        <p>m.fit(train)</p>
        <p></p>
        <p>df_cv = cross_validation(model=m, horizon='365 days', cutoffs=cutoffs)</p>
        <p>df_p = performance_metrics(df_cv, rolling_window=1)</p>
        <p>mses.append(df_p['mse'].values[0])</p>
        <p></p>
        <p>tuning_results = pd.DataFrame(params)</p>
        <p>tuning_results['mse'] = mses</p>
        <p>Once this process is over, we can extract the optimal parameter combination. </p>
        <p>best_params = params[np.argmin(mses)]</p>
        <p>The result is that changepoint_prior_scale must be set to 0.01, and seasonality_</p>
        <p>prior_scale must be set to 0.01. </p>
        <p>Now that we have the optimal values for each parameter, we can fit the model on</p>
        <p>the entire training set to evaluate it later on the test set. </p>
        <p>m = Prophet(**best_params)</p>
        <p>m.add_country_holidays(country_name='US')</p>
        <p>m.fit(train); </p>
        <p>The next step is to obtain the forecast of our model for the same period as the test set and
            merge them with the test set for easier evaluation and plotting. </p>
        <p>future = m.make_future_dataframe(periods=12, freq='M')</p>
        <p>forecast = m.predict(future)</p>
        <p><a id="calibre_link-445"></a> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>387</b></p>
        <p>test[['yhat', 'yhat_lower', 'yhat_upper']] = forecast[['yhat', </p>
        <p>➥ 'yhat_lower', 'yhat_upper']]</p>
        <p>Before evaluating our model, we must have a benchmark, so we'll use the last season as a
            baseline model. </p>
        <p>test['baseline'] = train['y'][-12:].values</p>
        <p>We are now ready to evaluate our model from Prophet. We'll use the MAE for its ease of
            interpretation. </p>
        <p>prophet_mae = mean_absolute_error(test['y'], test['yhat'])</p>
        <p>baseline_mae = mean_absolute_error(test['y'], test['baseline'])</p>
        <p>Prophet achieves an MAE of 7.42, while our baseline gets an MAE of 10.92. Since the MAE of
            Prophet is lower, the model is better than the baseline. </p>
        <p>We can optionally plot the forecasts, as shown in figure 19.17. Note that this plot also
            shows the confidence interval of the Prophet model. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(train['y'])</p>
        <p>ax.plot(test['y'], 'b-', label='Actual')</p>
        <p>ax.plot(test['baseline'], 'k:', label='Baseline')</p>
        <p>ax.plot(test['yhat'], color='darkorange', ls='--', lw=3, label='Predictions')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Proportion of searches using the keyword "chocolate"')</p>
        <p>ax.axvspan(204, 215, color='#808080', alpha=0.1)</p>
        <p>ax.legend(loc='best')</p>
        <p>plt.xticks(np.arange(0, 215, 12), np.arange(2004, 2022, 1))</p>
        <p>plt.fill_between(x=test.index, y1=test['yhat_lower'], </p>
        <p>➥ y2=test['yhat_upper'], color='lightblue') </p>
        <p><b>Plot 80% confidence </b></p>
        <p>plt.xlim(180, 215)</p>
        <p><b>interval of the </b></p>
        <p><b>Prophet model. </b></p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>In figure 19.17 it is clear that the forecast from Prophet, shown as a dashed line, is
            closer to the actual values than the forecast from the baseline model, shown as a dotted line. This
            translates to a lower MAE for Prophet. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-292"></a><img class="calibre2" src="images/000025.png" alt="Image 156" /></p>
        <p><b>388</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>Actual</p>
        <p>100</p>
        <p>Predictions</p>
        <p>Baseline</p>
        <p>90</p>
        <p>” </p>
        <p>80</p>
        <p>“chocolate</p>
        <p>70</p>
        <p>60</p>
        <p>50</p>
        <p>40</p>
        <p>Proportion of searches using the keyword</p>
        <p>30</p>
        <p>2019</p>
        <p>2020</p>
        <p>2021</p>
        <p>Date</p>
        <p>Figure 19.17</p>
        <p>Forecasting the popularity of “chocolate” searches on Google in the United States. The
            forecast from Prophet, shown as a dashed line, is much closer to the actual values than the baseline
            model,
            shown as a dotted line. </p>
        <p>We can further discover how Prophet modeled our data by plotting the components</p>
        <p>of the model, as shown in figure 19.18. </p>
        <p>prophet_components_fig = m.plot_components(forecast)</p>
        <p>In figure 19.18 you'll see that the trend component in the first plot increases over time,
            just as we noted when we first plotted our data. The second plot shows the holiday effects, which is
            interesting because there are troughs in the negative. This means that Prophet used the list of holidays
            to
            determine when “chocolate” searches were likely to decrease. This counters our first intuition when we
            thought that holidays might determine when chocolate would be more popular. Finally, the third plot
            shows
            the yearly seasonality, with peaks occurring toward the end and beginning of the year, which corresponds
            to
            Christmas, New Year, and Valentine's Day. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-173"></a><img src="images/000043.jpg" alt="Image 157" class="calibre2" /></p>
        <p> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>389</b></p>
        <p>Figure 19.18</p>
        <p>Components of the Prophet model. The trend component increases over time, as expected. We
            can also see the holidays component, which shows signals in the negatives. This is interesting, because
            it
            means that Prophet used holidays to determine when “chocolate” was not a popular search term. Finally,
            we
            have the yearly seasonal component, with peaks in January. </p>
        <p> <i><b>19.5.2 Experiment: Can SARIMA do better? </b></i>
        </p>
        <p>In the previous section, we used Prophet to forecast the popularity of searches on Google
            involving the keyword “chocolate” in the United States. Our model achieved a better performance than our
            baseline, but it would be interesting to see how a SARIMA model compares to Prophet in this situation.
            This
            section is optional, but it is a great</p>
        <p><a id="calibre_link-275"></a><b>390</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>occasion to revisit our modeling skills using statistical models, and it ultimately is a fun
            experiment. </p>
        <p>Let's start by importing the libraries that we need. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from itertools import product</p>
        <p>from typing import Union</p>
        <p>Next, we'll check whether the data is stationary using the augmented Dickey-Fuller (ADF)
            test. </p>
        <p>ad_fuller_result = adfuller(df['y'])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>We get an ADF statistic of &ndash;2.03 and a p-value of 0.27. Since the p-value is greater
            than 0.05, we fail to reject the null hypothesis and conclude that our series is not stationary. </p>
        <p>Let's difference our time series and test for stationarity again. </p>
        <p>y_diff = np.diff(df['y'], n=1)</p>
        <p>ad_fuller_result = adfuller(y_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>We now obtain an ADF statistic of &ndash;7.03 and a p-value that is much smaller than 0.05,
            so we reject the null hypothesis and conclude that our series is now stationary. </p>
        <p>Since we differenced only once and did not take a seasonal difference, we set <i>d</i> =
            1
        </p>
        <p>and <i>D</i> = 0. Also, since we have monthly data, the frequency is <i class="calibre3">m</i> = 12. As you
            can see, having seasonal data does not mean that we have to take
            a
            seasonal difference to make it stationary. </p>
        <p>Now we'll use the optimize_SARIMAX function, as shown in listing 19.2, to find the values of
            <i>p</i>, <i>q</i>, <i>P</i>, and <i class="calibre3">Q</i> that minimize the Akaike information criterion
            (AIC). Note that although the
            function has SARIMAX in its name, we can use it to optimize any declination of the SARIMAX mode. In this
            case, we'll optimize a SARIMA model simply by setting the exogenous variables to None. </p>
        <p>Listing 19.2</p>
        <p>Function to minimize the AIC of a SARIMAX model</p>
        <p>def optimize_SARIMAX(endog: Union[pd.Series, list], </p>
        <p>➥ exog: Union[pd.Series, list], </p>
        <p>➥ order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:</p>
        <p></p>
        <p>results = []</p>
        <p><a id="calibre_link-446"></a> <i><b>19.5</b></i></p>
        <p> <i><b>Implementing a robust forecasting process with
                    Prophet</b></i></p>
        <p><b>391</b></p>
        <p>for order in tqdm_notebook(order_list):</p>
        <p>try: </p>
        <p>model = SARIMAX(</p>
        <p>endog, </p>
        <p>exog, </p>
        <p>order=(order[0], d, order[1]), </p>
        <p>seasonal_order=(order[2], D, order[3], s), </p>
        <p>simple_differencing=False).fit(disp=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p>aic = model.aic</p>
        <p>results.append([order, model.aic])</p>
        <p></p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['(p,q,P,Q)', 'AIC']</p>
        <p></p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df</p>
        <p>To find the optimal parameters, we'll first define a range of values for each and create a
            list of unique combinations. We can then pass that list to the optimize_SARIMAX</p>
        <p>function. </p>
        <p>ps = range(0, 4, 1)</p>
        <p>qs = range(0, 4, 1)</p>
        <p>Ps = range(0, 4, 1)</p>
        <p>Qs = range(0, 4, 1)</p>
        <p>order_list = list(product(ps, qs, Ps, Qs))</p>
        <p>d = 1</p>
        <p>D = 0</p>
        <p>s = 12</p>
        <p>SARIMA_result_df = optimize_SARIMAX(train['y'], None, order_list, d, D, s)</p>
        <p>SARIMA_result_df</p>
        <p>The resulting DataFrame, shown in figure 19.19, is interesting. The lowest AIC is 143.51 and
            the second-lowest AIC is 1,127.75. The difference is very large, which hints that something is wrong
            with
            the first <i>p</i>, <i>d</i>, <i>P</i>, <i class="calibre3">Q</i> values. </p>
        <p><b>(p,q,P,Q)</b></p>
        <p><b>AIC</b></p>
        <p><b>0</b></p>
        <p>(1,0,1,3)</p>
        <p>143.508936</p>
        <p><b>1</b></p>
        <p>(1,1,1,1)</p>
        <p>1127.746591</p>
        <p>Figure 19.19</p>
        <p>Ordering the parameters ( <i><b>p</b></i>, <i><b>d</b></i>, <i><b class="calibre4">P</b></i>, <i
                class="calibre3"><b>Q</b></i>) in <b class="calibre4">2</b></p>
        <p>(1,1,2,1)</p>
        <p>1129.725199</p>
        <p>ascending order of AIC. We can see a large difference </p>
        <p>between the first two entries of the <b>DataFrame</b>. This </p>
        <p><b>3</b></p>
        <p>(1,1,1,2)</p>
        <p>1129.725695</p>
        <p>indicates that something is wrong with the first set of </p>
        <p><b>4</b></p>
        <p>(0,2,1,1)</p>
        <p>1130.167369</p>
        <p>parameters, and we should choose the second set. </p>
        <p><a id="calibre_link-447"></a><img src="images/000106.jpg" alt="Image 158" class="calibre2" /></p>
        <p><b>392</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p>We'll thus use the second set of values, which sets the values <i>p</i>, <i class="calibre3">q</i>, <i>P</i>,
            and <i>Q</i> to 1, resulting in
            a
            SARIMA(1,1,1)(1,0,1)12 model. We can fit a model on the training set using those values and study its
            residuals, which are shown in figure 19.20. </p>
        <p>SARIMA_model = SARIMAX(train['y'], order=(1,1,1), </p>
        <p>➥ seasonal_order=(1,0,1,12), simple_differencing=False)</p>
        <p>SARIMA_model_fit = SARIMA_model.fit(disp=False)</p>
        <p>SARIMA_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>Figure 19.20</p>
        <p>Residuals of the SARIMA(1,1,1)(1,0,1)12 model. At the top left, you can see that the
            residuals are random, with no trend. At the top right, the distribution is close to a normal
            distribution,
            but there is some deviation on the right. This is further supported by the Q-Q plot at the bottom left,
            where we see a fairly straight line that lies on <i><b>y</b></i> = <i class="calibre3"><b
                    class="calibre4">x</b></i>, but there is a clear departure at the end. Finally,
            the
            correlogram at the bottom right shows no significant coefficients after lag 0, just like white noise.
        </p>
        <p>At this point, it is hard to determine whether the residuals are close enough to white
            noise, so we'll use the Ljung-Box test to determine if the residuals are independent and uncorrelated.
        </p>
        <p><a id="calibre_link-174"></a> <i><b>19.6</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>393</b></p>
        <p>residuals = SARIMA_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>The returned p-values are all greater than 0.05, except the first one, which stands at
            0.044. Since all other nine p-values are greater than 0.05, we'll assume we can reject the null
            hypothesis
            and conclude that this is as close as we can get our residuals to white noise. </p>
        <p>Next, let's generate the predictions from the SARIMA model over the period of</p>
        <p>the test set. </p>
        <p>SARIMA_pred = SARIMA_model_fit.get_prediction(204, 215).predicted_mean</p>
        <p>test['SARIMA_pred'] = SARIMA_pred</p>
        <p>Finally, we'll measure the MAE of the SARIMA model. Remember that our Prophet</p>
        <p>model had an MAE of 7.42, and the baseline achieved an MAE of 10.92. </p>
        <p>SARIMA_mae = mean_absolute_error(test['y'], test['SARIMA_pred'])</p>
        <p>Here, SARIMA achieves an MAE of 10.09. It is better than the baseline, but it does not
            perform better than Prophet in this case. </p>
        <p> <i><b>19.6</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>In this chapter, we explored the use of the Prophet library for automatic time series
            forecasting. Prophet uses a general additive model that combines a trend component, a seasonal
            component,
            and holiday effects. </p>
        <p>The main advantage of this library is that it allows us to quickly experiment and generate
            predictions. Many functions are available for visualizing and understanding our models, and more
            advanced
            functions are also available, allowing us to perform cross-validation and hyperparameter tuning. </p>
        <p>While Prophet is widely used in the industry, it must not be considered a one-size-fits-all
            solution. Prophet works particularly well with strongly seasonal data that has many historical seasons.
            Thus, it is to be treated as another tool in our forecasting tool belt that can be tested along with
            other
            statistical or deep learning models. </p>
        <p>We've explored the fundamentals of time series forecasting throughout this book, and now
            you've seen one way to automate most of the manual work we did with statistical and deep learning
            models. I
            highly encourage you to browse through Prophet's documentation for more granular information, as well as
            to
            explore the other libraries for automatic forecasting. Now that you know how to work with one library,
            transitioning to another is very easy. </p>
        <p>In the next chapter, we'll work through a final capstone project and forecast the price of
            beef in Canada. This is a great occasion to apply the forecasting procedure we developed using Prophet,
            as
            well as to experiment with the other models you have learned so far to develop the best solution
            possible.
        </p>
        <p><a id="calibre_link-175"></a><b>394</b></p>
        <p>CHAPTER 19</p>
        <p> <i><b>Automating time series forecasting with
                    Prophet</b></i></p>
        <p> <i><b>19.7</b></i></p>
        <p> <i><b>Exercises</b></i></p>
        <p>Here we'll revisit problems from previous chapters but use Prophet to make forecasts. </p>
        <p>We can then compare the performance of Prophet to the previously built models. </p>
        <p>As always, the solution is available on GitHub: <a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH19">https://github.com/marcopeix/</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH19">TimeSeriesForecastingInPython/tree/master/CH19</a>.
        </p>
        <p> <i><b>19.7.1 Forecast the number of air
                    passengers</b></i></p>
        <p>In chapter 8 we used a dataset that tracks the number of monthly air passengers between 1949
            and 1960. We developed a SARIMA model that achieved a MAPE of 2.85%. </p>
        <p>Use Prophet to forecast the last 12 months of the dataset:</p>
        <p> Does it make sense to add holiday effects? </p>
        <p> Looking at the data, is the seasonality additive or multiplicative? </p>
        <p> Use hyperparameter tuning and cross-validation to find the optimal parameters. </p>
        <p> Fit the model with the optimal parameters and evaluate its predictions for the last 12
            months. Does it achieve a lower MAPE? </p>
        <p> <i><b>19.7.2 Forecast the volume of antidiabetic drug
                    prescriptions</b></i></p>
        <p>In chapter 11 we worked through a capstone project to predict the monthly volume of
            antidiabetic drug prescriptions in Australia. We developed a SARIMA model that achieved a MAPE of 7.9%.
        </p>
        <p>Use Prophet to forecast the last 36 months of the dataset:</p>
        <p> Does it make sense to add holiday effects? </p>
        <p> Looking at the data, is the seasonality additive or multiplicative? </p>
        <p> Use hyperparameter tuning and cross-validation to find the optimal parameters. </p>
        <p> Fit the model with the optimal parameters and evaluate its predictions for the last 36
            months. Does it achieve a lower MAPE? </p>
        <p> <i><b>19.7.3 Forecast the popularity of a keyword on
                    Google Trends</b></i></p>
        <p>Google Trends <a href="https://trends.google.com/trends/">(https://trends.google.com/trends/</a>) is a
            great
            place to
            generate time series datasets. This is where you can see the popular searches on Google around the
            world.
        </p>
        <p>Choose a keyword and a country of your choice, and generate a time series dataset. </p>
        <p>Then use Prophet to predict its popularity in the future. This is a very open-ended project
            with no solutions to it. Take this opportunity to explore the Google Trends tool, and experiment with
            Prophet to learn what works and what does not. </p>
        <p></p>
        <p></p>
        <p><a id="calibre_link-207"></a> <i><b>Summary</b></i></p>
        <p><b>395</b></p>
        <p> <i><b>Summary</b></i></p>
        <p> There are many libraries that automate the forecasting process, such as pmdarima, Prophet,
            NeuralProphet, and PyTorch Forecasting. </p>
        <p> Prophet is one of the most widely known and used libraries in the industry for automatic
            time series forecasting. Knowing how to use it is important for any data scientist doing time series
            forecasting. </p>
        <p> Prophet uses a general additive model that combines a trend component, a sea-</p>
        <p>sonal component, and holiday effects. </p>
        <p> Prophet is not the optimal solution to all problems. It works best on strongly seasonal
            data with multiple historical seasons for training. Therefore, it must be regarded as one of several
            tools
            for forecasting. </p>
        <p><a id="calibre_link-24"></a> <i>Capstone: Forecasting</i></p>
        <p> <i>the monthly average retail</i></p>
        <p> <i>price of steak in Canada</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Developing a forecasting model to predict the </p>
        <p>monthly average retail price of steak in Canada</p>
        <p> Using Prophet's cross-validation functionality</p>
        <p> Developing a SARIMA model and comparing its </p>
        <p>performance to Prophet to determine the </p>
        <p>champion model</p>
        <p>Again, congratulations on making it this far! We have come a long way since the beginning of
            this book. We first defined time series and learned how to forecast them using statistical models that
            generalize as the SARIMAX model. Then we turned to large, high-dimensional datasets and used deep
            learning
            for time series forecasting. In the previous chapter, we covered one of the most popular libraries for
            automating the entire forecasting process: Prophet. We developed two forecasting models using Prophet
            and
            saw how quick and easy it is to generate accurate predictions with few manual steps. </p>
        <p>In this last capstone project, we'll use everything you have learned in this book to
            forecast the monthly average retail price of steak in Canada. At this point, we have a robust
            methodology
            and a wide array of tools to develop a performant forecasting model. </p>
        <p><b>396</b></p>
        <p><a id="calibre_link-176"></a> <i><b>20.1</b></i></p>
        <p> <i><b>Understanding the capstone project</b></i></p>
        <p><b>397</b></p>
        <p> <i><b>20.1</b></i></p>
        <p> <i><b>Understanding the capstone project</b></i></p>
        <p>For this project, we'll use the historical monthly average retail price of food in Canada,
            from 1995 to today. Note that at the time of writing, the data for December 2021</p>
        <p>and onward was not available. The dataset, titled “Monthly average retail prices for food
            and other selected products,” is available for download from Statistics Canada here: <a
                href="http://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000201">www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000201.
            </a></p>
        <p>The price of a basket of goods is an important macroeconomic indicator. This is</p>
        <p>what composes the consumer price index (CPI), which is used to determine if there is an
            inflationary or deflationary period. This in turn allows analysts to assess the effectiveness of the
            economic policy, and it can of course impact programs of government assistance, such as social security.
            If
            the price of goods is expected to rise, the amount reserved for social security should technically
            increase.
        </p>
        <p>The original dataset contains the monthly average retail price of 52 goods, from 1 kilogram
            of round steak to a dozen eggs, 60 grams of deodorant, and gasoline, to name a few. The price is
            reported in
            Canadian dollars for every month starting in 1995 to November 2021. For this project, we'll focus
            specifically on forecasting the price of 1 kg of round steak. </p>
        <p> <i><b>20.1.1 Objective of the capstone project</b></i>
        </p>
        <p>The objective of this capstone project is to create a model that can forecast the monthly
            average retail price of 1 kg of round steak over the next 36 months. If you feel confident, you can
            download
            the dataset and develop a forecasting model. Feel free to use Prophet. </p>
        <p>If you feel you need a little more guidance, here are the steps that need to be</p>
        <p>completed:</p>
        <p>1</p>
        <p>Clean the data so you only have information regarding 1 kg of round steak. </p>
        <p>2</p>
        <p>Rename the columns according to Prophet's convention. </p>
        <p>3</p>
        <p>Format the date correctly. The datestamp only has the year and month, so the</p>
        <p>day must be added. Recall that we are working with monthly averages, so does it</p>
        <p>make sense to add the first day of the month, or the last day of the month? </p>
        <p>4</p>
        <p>Use cross-validation for hyperparameter tuning with Prophet. </p>
        <p>5</p>
        <p>Fit a Prophet model with the optimal parameters. </p>
        <p>6</p>
        <p>Forecast over the test set. </p>
        <p>7</p>
        <p>Evaluate your model using the mean absolute error (MAE). </p>
        <p>8</p>
        <p>Compare your model to a baseline. </p>
        <p>There is one more optional, but highly recommended, step:</p>
        <p>9</p>
        <p>Develop a SARIMA model and compare its performance to Prophet. Did it do</p>
        <p>better? </p>
        <p>You now have all the steps required to successfully complete this project. I highly
            recommend that you try it on your own first. At any point, you can refer to the following</p>
        <p><a id="calibre_link-177"></a><b>398</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>sections for a detailed walkthrough. Also, the entire solution is available on GitHub:</p>
        <p><a href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH20">https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH20.
            </a></p>
        <p>Good luck! </p>
        <p> <i><b>20.2</b></i></p>
        <p> <i><b>Data preprocessing and visualization</b></i></p>
        <p>We'll start by preprocessing the data in order to train a Prophet model. At the same time,
            we'll visualize our time series to deduce some of its properties. </p>
        <p>First we'll import the required libraries. </p>
        <p>import numpy as np</p>
        <p>import pandas as pd</p>
        <p>import matplotlib.pyplot as plt</p>
        <p>from fbprophet import Prophet</p>
        <p>from fbprophet.plot import plot_cross_validation_metric</p>
        <p>from fbprophet.diagnostics import cross_validation, performance_metrics</p>
        <p>from sklearn.metrics import mean_absolute_error</p>
        <p>from itertools import product</p>
        <p>import warnings</p>
        <p>warnings.filterwarnings('ignore')</p>
        <p>I also like to set some general parameters for the figures. Here we'll specify the size and
            remove the grid from the plots. </p>
        <p>plt.rcParams['figure.figsize'] = (10, 7.5)</p>
        <p>plt.rcParams['axes.grid'] = False</p>
        <p>Next, we'll read the data. You can download it from this Statistics Canada (<a
                href="http://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000201">www150</a></p>
        <p><a
                href="http://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000201">.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000201</a>),
            although you are likely to get a more up-to-date version of the dataset since I only had data up to
            November
            2021 when writing this book. If you wish to recreate the results shown here, I suggest you use the CSV
            file
            in the GitHub repository for this chapter (<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH20">https://github.com/marcopeix/</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH20">TimeSeriesForecastingInPython/tree/master/CH20</a>).
        </p>
        <p>df = pd.read_csv('../data/monthly_avg_retail_price_food_canada.csv')</p>
        <p>In its original form, the dataset contains the monthly average retail price of 52 products,
            from January 1995 to November 2021. We wish to specifically forecast the retail price of 1 kg of round
            steak, so we can filter the data accordingly. </p>
        <p>df = df[df['Products'] == 'Round steak, 1 kilogram']</p>
        <p>The next step is to remove unnecessary columns and only keep the REF_DATE col-</p>
        <p>umn, which contains the month and year for the data point, and the VALUE column, which
            contains the average retail price for that month. </p>
        <p><a id="calibre_link-448"></a><img class="calibre2" src="images/000016.png" alt="Image 159" /></p>
        <p> <i><b>20.2</b></i></p>
        <p> <i><b>Data preprocessing and visualization</b></i></p>
        <p><b>399</b></p>
        <p>cols_to_drop = ['GEO', 'DGUID', 'Products', 'UOM', 'UOM_ID', </p>
        <p>'SCALAR_FACTOR', 'SCALAR_ID', 'VECTOR', 'COORDINATE', 'STATUS', </p>
        <p>'SYMBOL', 'TERMINATED', 'DECIMALS']</p>
        <p>df = df.drop(cols_to_drop, axis=1)</p>
        <p>We now have a dataset with 2 columns and 323 rows. This is a good time to visualize our time
            series. The result is shown in figure 20.1. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(df['VALUE'])</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Average retail price of 1kg of round steak (CAD')</p>
        <p>plt.xticks(np.arange(0, 322, 12), np.arange(1995, 2022, 1))</p>
        <p>fig.autofmt_xdate()</p>
        <p>22</p>
        <p>20</p>
        <p>18</p>
        <p>16</p>
        <p>14</p>
        <p>12</p>
        <p>verage retail price of 1kg of round steak (CAD)A</p>
        <p>10</p>
        <p>1</p>
        <p>1995199619971998199920002001200220032004 200520062007 200820092010201
            2012201320142015201620172018201920202021</p>
        <p>Date</p>
        <p>Figure 20.1</p>
        <p>Monthly average retail price of 1 kg of round steak in Canada from January 1995 to November
            2021. There is a clear trend in the data as it increases over time. However, there does not seem to be
            any
            seasonality here. This might be a sign that Prophet is not the best tool for this problem. </p>
        <p><a id="calibre_link-178"></a><b>400</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>Figure 20.1 shows a clear trend in our data but there is no visible seasonality in this time
            series. Thus, Prophet might not be the best tool for this type of problem. However, this is pure
            intuition,
            so we'll test it against a baseline to see if we can successfully forecast our target. </p>
        <p> <i><b>20.3</b></i></p>
        <p> <i><b>Modeling with Prophet</b></i></p>
        <p>We have preprocessed our data and visualized it. The next step is to rename the columns
            according to Prophet's naming convention. The time column must be named</p>
        <p>ds and the value column must be named y. </p>
        <p>df.columns = ['ds', 'y']</p>
        <p>Next, we must format the date correctly. Right now our datestamp only has the year and
            month, but Prophet also expects a day in the format YYYY-MM-DD. Since we are working with monthly
            averages,
            we must add the last day of the month to the datestamp, since we cannot report the average retail price
            of
            January until the very last day of January. </p>
        <p>from pandas.tseries.offsets import MonthEnd</p>
        <p>df['ds'] = pd.to_datetime(df['ds']) + MonthEnd(1)</p>
        <p>Our data is now correctly formatted, so we'll split our dataset into train and test sets.
        </p>
        <p>Our objective is forecasting the future 36 months, so we'll allocate the last 36 data points
            to the test set. The rest is for training. </p>
        <p>train = df[:-36]</p>
        <p>test = df[-36:]</p>
        <p>We can now address hyperparameter tuning. We'll start by defining a list of possible values
            for changepoint_prior_scale and seasonality_prior_scale. We won't include any holiday effects, as they
            likely will not impact the price of goods. Then we'll create a list of all unique combinations. Here
            we'll
            use the mean squared error (MSE) as a selection criterion, because it penalizes large errors, and we
            want
            the best-fitted model. </p>
        <p>param_grid = {</p>
        <p>'changepoint_prior_scale': [0.01, 0.1, 1.0], </p>
        <p>'seasonality_prior_scale': [0.1, 1.0, 10.0]</p>
        <p>}</p>
        <p>params = [dict(zip(param_grid.keys(), v)) for v in </p>
        <p>➥ product(*param_grid.values())]</p>
        <p>mses = []</p>
        <p>Now we must define a list of cutoff dates. Recall that this is a workaround for using
            Prophet with monthly data. The cutoff dates specify the initial training set and the length of the
            testing
            period during cross-validation. </p>
        <p><a id="calibre_link-449"></a> <i><b>20.3</b></i></p>
        <p> <i><b>Modeling with Prophet</b></i></p>
        <p><b>401</b></p>
        <p>In this case, we'll allow for the first 5 years of data to be used as an initial training
            set. </p>
        <p>Then each testing period must have a length of 36 months, since this is our horizon in the
            objective statement. Our cutoff dates thus start in 2001-01-31 and end at the end of the training set,
            which
            is 2018-11-30, and each cutoff date is separated by 36 months. </p>
        <p>cutoffs = pd.date_range(start='2000-01-31', end='2018-11-30', freq='36M')</p>
        <p>We can now test each parameter combination, fit a model, and use cross-validation to measure
            its performance. The parameter combination with the lowest MSE will be selected to generate predictions
            over
            our test set. </p>
        <p>for param in params:</p>
        <p>m = Prophet(**param)</p>
        <p>m.fit(train)</p>
        <p></p>
        <p>df_cv = cross_validation(model=m, horizon='365 days', cutoffs=cutoffs)</p>
        <p>df_p = performance_metrics(df_cv, rolling_window=1)</p>
        <p>mses.append(df_p['mse'].values[0])</p>
        <p></p>
        <p>tuning_results = pd.DataFrame(params)</p>
        <p>tuning_results['mse'] = mses</p>
        <p>best_params = params[np.argmin(mses)]</p>
        <p>print(best_params)</p>
        <p>This indicates that both changepoint_prior_scale and seasonality_prior_scale should be set
            to 1.0. We'll thus define a Prophet model using best_params and fit it on the training set. </p>
        <p>m = Prophet(**best_params)</p>
        <p>m.fit(train); </p>
        <p>Next, we'll use make_future_dataframe to define the forecast horizon. In this case, it is 36
            months. </p>
        <p>future = m.make_future_dataframe(periods=36, freq='M')</p>
        <p>We can now generate predictions. </p>
        <p>forecast = m.predict(future)</p>
        <p>Let's append them to our test set, so it's easier to evaluate the performance and plot the
            forecast against the observed values. </p>
        <p>test[['yhat', 'yhat_lower', 'yhat_upper']] = forecast[['yhat', </p>
        <p>➥ 'yhat_lower', 'yhat_upper']]</p>
        <p>Of course, our model must be evaluated against a benchmark. For this example, we'll simply
            use the last known value of the training set as a prediction for the next 36</p>
        <p><a id="calibre_link-450"></a><img class="calibre2" src="images/000100.png" alt="Image 160" /></p>
        <p><b>402</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>months. We could alternatively use the mean value method, but I would consider the mean in
            recent years only, since there is a clear trend in the data, which means the mean changes over time.
            Using
            the naive seasonal method here is not valid, since there is no clear seasonality in the data. </p>
        <p>test['Baseline'] = train['y'].iloc[-1]</p>
        <p>Everything is set for evaluation. We'll use the MAE to select the best model. This metric is
            chosen for its ease of interpretation. </p>
        <p>baseline_mae = mean_absolute_error(test['y'], test['Baseline'])</p>
        <p>prophet_mae = mean_absolute_error(test['y'], test['yhat'])</p>
        <p>print(prophet_mae)</p>
        <p>print(baseline_mae)</p>
        <p>From this, we obtain an MAE of 0.681 with our baseline, while Prophet achieves an MAE of
            1.163. Therefore, Prophet performs worse than the baseline, which simply uses the last known value as a
            forecast. </p>
        <p>We can visualize the predictions in figure 20.2. </p>
        <p>Actual</p>
        <p>22</p>
        <p>Predictions</p>
        <p>Baseline</p>
        <p>20</p>
        <p>18</p>
        <p>16</p>
        <p>14</p>
        <p>12</p>
        <p>verage retail price of 1kg of round steak (CAD)A</p>
        <p>10</p>
        <p>2016</p>
        <p>2017</p>
        <p>2018</p>
        <p>2019</p>
        <p>2020</p>
        <p>2021</p>
        <p>Date</p>
        <p>Figure 20.2</p>
        <p>Forecasting the monthly average retail price of 1 kg of round steak in Canada. We can see
            that Prophet (shown as a dashed line) tends to overshoot the observed values. </p>
        <p><a id="calibre_link-270"></a> <i><b>20.3</b></i></p>
        <p> <i><b>Modeling with Prophet</b></i></p>
        <p><b>403</b></p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(train['y'])</p>
        <p>ax.plot(test['y'], 'b-', label='Actual')</p>
        <p>ax.plot(test['Baseline'], 'k:', label='Baseline')</p>
        <p>ax.plot(test['yhat'], color='darkorange', ls='--', lw=3, </p>
        <p>➥ label='Predictions')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Average retail price of 1kg of round steak (CAD')</p>
        <p>ax.axvspan(287, 322, color='#808080', alpha=0.1)</p>
        <p>ax.legend(loc='best')</p>
        <p>plt.xticks(np.arange(0, 322, 12), np.arange(1995, 2022, 1))</p>
        <p>plt.fill_between(x=test.index, y1=test['yhat_lower'], </p>
        <p>➥ y2=test['yhat_upper'], color='lightblue')</p>
        <p>plt.xlim(250, 322)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>We can also visualize the components of the model in figure 20.3. </p>
        <p>prophet_components_fig = m.plot_components(forecast)</p>
        <p>Figure 20.3 shows the components of the Prophet model. The top plot shows the trend
            component, which has many changepoints. This is because we allowed the trend to be very flexible by
            setting
            changepoint_prior_scale to 1.0, as it resulted in the best fit during cross-validation. </p>
        <p>The bottom plot shows the yearly seasonal component. This component is likely</p>
        <p>helping Prophet achieve a better fit, but I doubt there is a tangible reason for price of
            goods to decrease toward September. This highlights the curve-fitting procedure of Prophet. It's also a
            great example of a case where domain knowledge could probably help us better fine-tune this parameter.
        </p>
        <p>We have thus found a situation where Prophet is not the ideal solution. In fact, it
            performed worse than our naive forecasting method. We could have anticipated that, since we know that
            Prophet performs best on strongly seasonal data, but we could not be sure until we actually tested it.
        </p>
        <p>The next portion of the project is optional, but I highly recommend that you complete it, as
            it shows a complete solution to a time series forecasting problem. We have tested Prophet and did not
            obtain
            satisfying results, but that does not mean we must give up. Instead, we must search for another solution
            and
            test it. Since we do not have a large dataset, deep learning is not a suitable tool for this problem.
            Therefore, let's try using a SARIMA model. </p>
        <p><a id="calibre_link-179"></a><img src="images/000144.jpg" alt="Image 161" class="calibre2" /></p>
        <p><b>404</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>Figure 20.3</p>
        <p>Components of the Prophet model. The top plot shows the trend component, with many
            changepoints, as we set <b>changepoint_prior_scale</b> to a high value, allowing the
            trend
            to be more flexible. The bottom plot shows the yearly seasonal component. Again, this component likely
            improves the fit of the model, but I doubt there is tangible reason to reduce the price of goods near
            September, for example. </p>
        <p> <i><b>20.4</b></i></p>
        <p> <i><b>Optional: Develop a SARIMA model</b></i></p>
        <p>In the previous section, we used Prophet to forecast the monthly average retail price of 1
            kg of round steak in Canada, but Prophet performed worse than our baseline model. We'll now develop a
            SARIMA
            model to see if it can achieve better performance than our baseline. </p>
        <p>The first step is to import the required libraries. </p>
        <p>from statsmodels.stats.diagnostic import acorr_ljungbox</p>
        <p>from statsmodels.tsa.statespace.sarimax import SARIMAX</p>
        <p>from statsmodels.tsa.stattools import adfuller</p>
        <p>from tqdm import tqdm_notebook</p>
        <p>from typing import Union</p>
        <p>Next, we'll test for stationarity. This will determine the values of the integration order
            <i>d</i> and the seasonal integration order <i>D</i>. Recall that we
            are
            using the ADF test to test for stationarity. </p>
        <p><a id="calibre_link-276"></a> <i><b>20.4</b></i></p>
        <p> <i><b>Optional: Develop a SARIMA model</b></i></p>
        <p><b>405</b></p>
        <p>ad_fuller_result = adfuller(df['y'])</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>Here we get an ADF statistic of 0.31 and a p-value of 0.98. Since the p-value is greater
            than 0.05, we conclude that the series is not stationary. This is expected, since we can clearly see a
            trend
            in the data. </p>
        <p>We'll difference the series once and test again for stationarity. </p>
        <p>y_diff = np.diff(df['y'], n=1)</p>
        <p>ad_fuller_result = adfuller(y_diff)</p>
        <p>print(f'ADF Statistic: {ad_fuller_result[0]}')</p>
        <p>print(f'p-value: {ad_fuller_result[1]}')</p>
        <p>Now we have an ADF-statistic of &ndash;16.78 and a p-value much smaller than 0.05. We thus
            conclude that we have a stationary time series. Therefore, <i>d</i> = 1 and <i class="calibre3">D</i> = 0.
            Recall that SARIMA also requires the frequency <i>m</i>
            to
            be set. Since we have monthly data, the frequency is <i>m</i> = 12. </p>
        <p>Next, we'll use the optimize_SARIMAX function shown in listing 20.1 to find the parameters (
            <i>p</i>, <i>q</i>, <i>P</i>, <i>Q</i>)
            that minimize the Akaike information criterion (AIC). </p>
        <p>Listing 20.1</p>
        <p>Function to select the parameters that minimize the AIC</p>
        <p>def optimize_SARIMAX(endog: Union[pd.Series, list], exog: Union[pd.Series, </p>
        <p>➥ list], order_list: list, d: int, D: int, s: int) -&gt; pd.DataFrame:</p>
        <p></p>
        <p>results = []</p>
        <p></p>
        <p>for order in tqdm_notebook(order_list):</p>
        <p>try: </p>
        <p>model = SARIMAX(</p>
        <p>endog, </p>
        <p>exog, </p>
        <p>order=(order[0], d, order[1]), </p>
        <p>seasonal_order=(order[2], D, order[3], s), </p>
        <p>simple_differencing=False).fit(disp=False)</p>
        <p>except:</p>
        <p>continue</p>
        <p></p>
        <p>aic = model.aic</p>
        <p>results.append([order, model.aic])</p>
        <p></p>
        <p>result_df = pd.DataFrame(results)</p>
        <p>result_df.columns = ['(p,q,P,Q)', 'AIC']</p>
        <p></p>
        <p>#Sort in ascending order, lower AIC is better</p>
        <p>result_df = result_df.sort_values(by='AIC', </p>
        <p>➥ ascending=True).reset_index(drop=True)</p>
        <p></p>
        <p>return result_df</p>
        <p><a id="calibre_link-256"></a><b>406</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>We'll define the range of possible values for <i>p</i>, <i>q</i>, <i class="calibre3">P</i>, and <i>Q</i>,
            generate a list of all
            unique combinations, and run the optimize_SARIMAX function. Note that we do not have exogenous
            variables.
        </p>
        <p>ps = range(1, 4, 1)</p>
        <p>qs = range(1, 4, 1)</p>
        <p>Ps = range(1, 4, 1)</p>
        <p>Qs = range(1, 4, 1)</p>
        <p>order_list = list(product(ps, qs, Ps, Qs))</p>
        <p>d = 1</p>
        <p>D = 0</p>
        <p>s = 12</p>
        <p>SARIMA_result_df = optimize_SARIMAX(train['y'], None, order_list, d, D, s)</p>
        <p>SARIMA_result_df</p>
        <p>Once the search is complete, we'll find that <i>p</i> = 2, <i>q</i> =
            3, <i>P</i> = 1, and <i>Q</i> = 1 is the
            combination that results in the lowest AIC. We can now fit a model using this parameter combination and
            study its residuals in figure 20.4, which turn out to be completely random. </p>
        <p>SARIMA_model = SARIMAX(train['y'], order=(2,1,3), </p>
        <p>➥ seasonal_order=(1,0,1,12), simple_differencing=False)</p>
        <p>SARIMA_model_fit = SARIMA_model.fit(disp=False)</p>
        <p>SARIMA_model_fit.plot_diagnostics(figsize=(10,8)); </p>
        <p>We can further support our conclusion by using the Ljung-Box test. Recall that the null
            hypothesis of the Ljung-Box test is that the data is uncorrelated and independent. </p>
        <p>residuals = SARIMA_model_fit.resid</p>
        <p>lbvalue, pvalue = acorr_ljungbox(residuals, np.arange(1, 11, 1))</p>
        <p>print(pvalue)</p>
        <p>The returned p-values are all greater than 0.05, so we cannot reject the null hypothesis and
            instead conclude that the residuals are indeed random and independent. Our SARIMA model can thus be used
            for
            forecasting. </p>
        <p>We'll generate predictions over the horizon of our test set. </p>
        <p>SARIMA_pred = SARIMA_model_fit.get_prediction(287, 322).predicted_mean</p>
        <p>test['SARIMA_pred'] = SARIMA_pred</p>
        <p>Then we'll evaluate the SARIMA model using the MAE. </p>
        <p>SARIMA_mae = mean_absolute_error(test['y'], test['SARIMA_pred'])</p>
        <p>print(SARIMA_mae)</p>
        <p><a id="calibre_link-451"></a><img src="images/000054.jpg" alt="Image 162" class="calibre2" /></p>
        <p> <i><b>20.4</b></i></p>
        <p> <i><b>Optional: Develop a SARIMA model</b></i></p>
        <p><b>407</b></p>
        <p>Figure 20.4</p>
        <p>Residuals of the SARIMA(2,1,3)(1,0,1)12 model. The top-left plot shows the residuals over
            time, which are completely random with no trend and a fairly constant variance, just like white noise.
        </p>
        <p>The top-right plot shows the distribution of the residuals, which is very close to a normal
            distribution. </p>
        <p>This is further supported by the Q-Q plot at the bottom left. We see a straight line that
            lies on <i><b>y</b></i> = <i><b>x</b></i>, so we can conclude that the
            residuals are normally distributed, like
            white noise. Finally, the correlogram at the bottom right shows no significant coefficients after lag 0,
            which is the same behavior as white noise. We can conclude that the residuals are completely random.
        </p>
        <p>Here we obtain an MAE of 0.678, which is just slightly better than our baseline, which
            achieved an MAE of 0.681. We can visualize the forecasts of the SARIMA model in figure 20.5. </p>
        <p>fig, ax = plt.subplots()</p>
        <p>ax.plot(train['y'])</p>
        <p>ax.plot(test['y'], 'b-', label='Actual')</p>
        <p>ax.plot(test['Baseline'], 'k:', label='Baseline')</p>
        <p>ax.plot(test['SARIMA_pred'], 'r-.', label='SARIMA')</p>
        <p>ax.plot(test['yhat'], color='darkorange', ls='--', lw=3, label='Prophet')</p>
        <p>ax.set_xlabel('Date')</p>
        <p>ax.set_ylabel('Average retail price of 1kg of round steak (CAD)')</p>
        <p><a id="calibre_link-452"></a><img class="calibre2" src="images/000067.png" alt="Image 163" /></p>
        <p><b>408</b></p>
        <p>CHAPTER 20</p>
        <p> <i><b>Capstone: Forecasting the monthly average retail
                    price of steak in Canada</b></i></p>
        <p>ax.axvspan(287, 322, color='#808080', alpha=0.1)</p>
        <p>ax.legend(loc='best')</p>
        <p>plt.xticks(np.arange(0, 322, 12), np.arange(1995, 2022, 1))</p>
        <p>plt.fill_between(x=test.index, y1=test['yhat_lower'], </p>
        <p>➥ y2=test['yhat_upper'], color='lightblue')</p>
        <p>plt.xlim(250, 322)</p>
        <p>fig.autofmt_xdate()</p>
        <p>plt.tight_layout()</p>
        <p>Actual</p>
        <p>Baseline</p>
        <p>22</p>
        <p>SARIMA</p>
        <p>Prophet</p>
        <p>20</p>
        <p>18</p>
        <p>16</p>
        <p>14</p>
        <p>12</p>
        <p>verage retail price of 1kg of round steak (CAD)A</p>
        <p>10</p>
        <p>2016</p>
        <p>2017</p>
        <p>2018</p>
        <p>2019</p>
        <p>2020</p>
        <p>2021</p>
        <p>Date</p>
        <p>Figure 20.5</p>
        <p>Forecasting the monthly average retail price of 1 kg of round steak in Canada. The SARIMA
            model, shown as a dashed and dotted line, achieves the lowest MAE (0.678), but it is only slightly
            better
            than our baseline (0.681), which is shown as a dotted line. </p>
        <p>While SARIMA performed better than Prophet, the difference in performance against the
            benchmark is negligible. This is a situation where we must ask ourselves if it is worth using the more
            complex SARIMA model for such a small difference. We can also investigate further to determine if there
            are
            external variables that could help us</p>
        <p><a id="calibre_link-180"></a> <i><b>20.5</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p><b>409</b></p>
        <p>forecast our target, as it seems that using past values only is not enough to generate
            accurate predictions. </p>
        <p> <i><b>20.5</b></i></p>
        <p> <i><b>Next steps</b></i></p>
        <p>Congratulations on completing this capstone project! This was special and different from
            what we have seen, as it turned out that we were addressing a fairly complex problem, and we could not
            come
            up with a very performant solution. This situation will happen as you tackle different time series
            forecasting problems, and it's where domain knowledge, gathering more data, and using your creativity to
            find external factors that can impact your target come into play. </p>
        <p>Take this opportunity to make this capstone project yours. We studied only one target, but
            there are 52 goods to choose from. Pick another target and see if you can generate predictions that
            perform
            much better than a baseline model. Feel free to also change the forecast horizon. </p>
        <p>If you want to go above and beyond, many government websites have open data, </p>
        <p>making them a gold mine for time series datasets. Here are the links to NYC Open Data and
            Statistics Canada:</p>
        <p> NYC Open Data&mdash;<a
                href="https://opendata.cityofnewyork.us/data/">https://opendata.cityofnewyork.us/data/</a></p>
        <p> Statistics Canada&mdash;<a
                href="http://www150.statcan.gc.ca/n1/en/type/data">www150.statcan.gc.ca/n1/en/type/data</a></p>
        <p>Explore these websites and find a time series dataset you can use to practice your
            forecasting skills. You will likely encounter a challenging problem, forcing you to search for
            solutions,
            and ultimately making you better at time series forecasting. </p>
        <p><a id="calibre_link-25"></a> <i>Going above and beyond</i></p>
        <p> <i><b>This chapter covers</b></i></p>
        <p> Consolidating your learning</p>
        <p> Managing difficult forecasting problems</p>
        <p> Exploring beyond time series forecasting</p>
        <p> Sources of time series datasets</p>
        <p>First of all, congratulations on making it to the end of this book! It has been quite a
            journey to get here, and it required a lot of your time, effort, and attention. </p>
        <p>You have gained a lot of skills for time series forecasting, but there is, of course, a lot
            still to learn. The objective of this chapter is to summarize what you've learned and outline what else
            you
            can achieve with time series data. I'll also encourage you to keep practicing your forecasting skills by
            listing various sources of time series data. </p>
        <p>The real challenge lies ahead of you, as you apply your knowledge to problems, </p>
        <p>either at work or as a side project, where the solutions are unknown to you. It is important
            that you gain confidence in your skills, which can only come from experience and practicing often. It is
            my
            hope that this chapter will inspire you to do so. </p>
        <p><b>410</b></p>
        <p><a id="calibre_link-181"></a> <i><b>21.1</b></i></p>
        <p> <i><b>Summarizing what you've learned</b></i></p>
        <p><b>411</b></p>
        <p> <i><b>21.1</b></i></p>
        <p> <i><b>Summarizing what you've learned</b></i></p>
        <p>Our very first step in time series forecasting was to define a time series as a set of data
            points ordered in time. You also quickly learned that the order of the data must remain untouched for
            our
            forecasting models to make sense. This means that data measured on Monday must always come after Sunday
            and
            before Tuesday. Therefore, </p>
        <p>no shuffling of the data is allowed when splitting it into training and testing sets. </p>
        <p>In chapter 2 we built naive forecasting methods that used very simple statistics or
            heuristics, such as the historical mean, the last known value, or repeating the last season. This is a
            critical step in any forecasting project, as it sets a benchmark for more complex models, revealing
            whether
            they are actually performant models. These benchmarks can also question the use of some advanced models
            since, as you have seen in this book, there are situations where advanced forecasting models do not
            perform
            much better than the baselines. </p>
        <p>Next, in chapter 3 we encountered the random walk model, a situation where we</p>
        <p>cannot apply forecasting models. This is because the value changes by a random number at
            each step, and no forecasting technique can reasonably predict a random number. In such a case, we can
            only
            resort to naive forecasting methods. </p>
        <p> <i><b>21.1.1 Statistical methods for forecasting</b></i>
        </p>
        <p>We then dived into the moving average and autoregressive processes in chapters 4 and 5.
            While real-life time series will rarely be approximated by a pure MA( <i>q</i>) or AR(
            <i>p</i>) model, they are the building blocks of the more complex models that we
            developed
            later on, such as the ARMA( <i>p</i>, <i>q</i>) model. What links all
            these models is that they assume the time series is stationary, meaning that its statistical properties,
            such as the mean, the variance, and autocorrelation, do not change over time. We used the augmented
            Dickey-Fuller (ADF) test to test for stationarity. For this test, the null hypothesis states that the
            series
            is not stationary. Therefore, if we obtain a p-value less than 0.05, we can reject the null hypothesis
            and
            conclude that we have a stationary process. </p>
        <p>While we could use the ACF and PACF plots to find the order <i>q</i> of a
            pure moving average process or the order <i>p</i> of a pure autoregressive process,
            respectively, in chapter 6 the ARMA( <i>p</i>, <i>q</i>) process
            forced us
            to design a general modeling procedure, in which we select the model with the lowest Akaike information
            criterion (AIC). </p>
        <p>Using this model selection criterion allows us to select a model that is not too complex but
            that still fits the data well, hence achieving a balance between overfitting and underfitting. </p>
        <p>Then we studied the model's residuals, which is the difference between the pre-</p>
        <p>dicted and actual values. Ideally the residuals behave like white noise, meaning that they
            are totally random and uncorrelated, which in turn means that our model explains any variance that is
            not
            due to chance. One visual tool that we can use for residual analysis is the quantile-quantile (Q-Q)
            plot,
            which compares the distribution of a sample to another theoretical distribution, in this case, the
            normal
            distribution. If they are identical, we should see a straight line that lies on <i>y</i> =
            <i>x</i>. We also used the</p>
        <p><a id="calibre_link-182"></a><b>412</b></p>
        <p>CHAPTER 21</p>
        <p> <i><b>Going above and beyond</b></i></p>
        <p>Ljung-Box test to determine whether the residuals were independent and uncorrelated. The
            null hypothesis of this test states that the sample is uncorrelated and is independently distributed.
            Therefore, if we obtain a p-value that is larger than 0.05, we fail to reject the null hypothesis and
            conclude that the residuals are random. This is important, because it means that our model has captured
            all
            the information from the data, and only the random variations remain unexplained. </p>
        <p>From this general modeling procedure, we further extended it to much more complex models,
            such as the ARIMA( <i>p</i>, <i>d</i>, <i>q</i>)
            model
            for non-stationary time series in chapter 7. Recall that we used this model to forecast the quarterly
            earnings per share of Johnson &amp; Johnson. </p>
        <p>Then we moved on to the SARIMA( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>, <i
                class="calibre3">Q</i>) <i>m</i> to account for seasonality in time series in
            chapter
            8. Recall that seasonality is the periodic fluctuations we see in the data. For example, the weather is
            hotter in the summer and colder in the winter, or more people drive on the road during the day than
            during
            the night. Using a SARIMA model allowed us to accurately forecast the monthly number of passengers</p>
        <p>for an airline. </p>
        <p>Next, we discovered the SARIMAX( <i>p</i>, <i>d</i>, <i class="calibre3">q</i>)( <i>P</i>, <i>D</i>, <i
                class="calibre3">Q</i>) <i>m</i> model, which added external variables to our model
            in
            chapter 9. Using that model, we were able to forecast the real GDP in the United States. </p>
        <p>Finally, we concluded the statistical forecasting methods in chapter 11 with vector
            autoregression, VAR( <i>p</i>), which allows us to forecast multiple time series in a
            single shot, but only if they Granger-cause one another. Otherwise, the model is invalid. </p>
        <p> <i><b>21.1.2 Deep learning methods for
                    forecasting</b></i></p>
        <p>Complex statistical models for forecasting reach a limit when the dataset becomes too large,
            usually at around 10,000 data points. At that point, statistical methods become very slow to fit and
            start
            losing performance. Furthermore, they fail to model nonlinear relationships in data. </p>
        <p>We thus turned our attention to deep learning, which thrives on large datasets with many
            features. We developed various deep learning models to forecast the hourly traffic on I-94 between
            Minneapolis and St. Paul in Minnesota. Our dataset had more than 17,000 rows of data and six features,
            making it a great opportunity to apply deep learning. </p>
        <p>We started in chapter 14 with a simple linear model that has only an input and an output
            layer, with no hidden layer. Then we built a deep neural network, which adds hidden layers and can model
            nonlinear relationships. </p>
        <p>We moved on to a more complex architecture in chapter 15, with the long short-</p>
        <p>term memory (LSTM) network. This architecture has the added advantage that it keeps
            information from the past in memory, in order to make a prediction for the future. </p>
        <p>We also used a convolutional neural network (CNN) in chapter 16, as they effectively perform
            feature selection using the convolution operation. We used a CNN in conjunc-tion with an LSTM to filter
            our
            time series before feeding it to the LSTM network. </p>
        <p><a id="calibre_link-183"></a> <i><b>21.2</b></i></p>
        <p> <i><b>What if forecasting does not work? </b></i></p>
        <p><b>413</b></p>
        <p>We added one final model to our toolset in chapter 17&mdash;the autoregressive deep</p>
        <p>neural network, which uses its own predictions to make more predictions. This architecture
            is very powerful and is behind some of the state-of-the-art models in time series forecasting, such as
            DeepAR. </p>
        <p>Throughout the entire deep learning section, the models were easily built because we first
            performed data windowing. This crucial step involves formatting the data in such a way that we have
            windows
            with training examples and test examples. This gave us the flexibility to quickly develop models for a
            wide
            variety of use cases, such as single-step forecasting, multi-step forecasting, and multivariate
            forecasting.
        </p>
        <p> <i><b>21.1.3 Automating the forecasting process</b></i>
        </p>
        <p>We put a lot of manual work into developing our models, and we developed our own functions
            to automate the process. However, there are many libraries available that make time series forecasting
            easy
            and fast. </p>
        <p>It's important to note that while these libraries speed up the forecasting process, they
            also add a level of abstraction that removes some of the flexibility and fine-tuning capabilities that
            we
            had available when we developed our own models. Nevertheless, they are great tools for rapid
            prototyping,
            because the time it takes to create a model is very short. </p>
        <p>One such library is Prophet, which is an open source project from Meta and prob-</p>
        <p>ably one of the most widely used forecasting libraries in the industry. However, it is not a
            one-size-fits-all solution. It works best on strongly seasonal data with many historical seasons for
            training. In such cases, it can quickly produce accurate predictions. Since it implements a general
            additive
            model, it can take into account multiple seasonal periods as well as holiday effects and changing
            trends.
            Furthermore, Prophet comes with a suite of utilities to visualize the predictions and the components of
            your
            data, and it includes cross-validation and hyperparameter tuning functions, all within a single library.
        </p>
        <p>This summarizes everything that we have discussed and applied so far. While you</p>
        <p>have all the tools you need to be successful with time series forecasting, you'll also need
            to know how to manage situations where your attempts at predicting the future do not work. </p>
        <p> <i><b>21.2</b></i></p>
        <p> <i><b>What if forecasting does not work? </b></i></p>
        <p>In this book, you learned how to be successful at forecasting time series. We worked through
            a wide variety of situations, from forecasting quarterly earnings per share to predicting the retail
            price
            of steak in Canada. For every scenario, we managed to create a performant forecasting model that was
            better
            than a baseline and generated accurate predictions. However, we might encounter situations where nothing
            seems to work. Thus, it is important to learn how to manage failure. </p>
        <p>There are many reasons why time series forecasting can fail. First, perhaps your data should
            not be analyzed as a time series at all. For example, you might be tasked to</p>
        <p><a id="calibre_link-453"></a><b>414</b></p>
        <p>CHAPTER 21</p>
        <p> <i><b>Going above and beyond</b></i></p>
        <p>forecast the number of sales for the next quarter. While you have access to historical data
            for the number of sales over time, perhaps sales are simply not a function of time. </p>
        <p>Instead, maybe the number of sales is a function of ad spending. In that case, instead of
            considering this problem as a time series, we should consider it as a regression problem, using ad spend
            as
            a feature to predict the number of sales. While this example is simplistic, it shows how reframing the
            problem differently may help you find a solution. </p>
        <p>Another case where time series forecasting will fail is when your data is a random walk.
            Recall from chapter 3 that a random walk is a time series where each step has an equal chance of going
            up or
            down by a random number. Therefore, we are really trying to predict a value that is changing randomly
            over
            time. This is not a reasonable thing to do, since no model can predict a random number. In that case, we
            must resort to using naive forecasting methods, as shown in chapter 2. </p>
        <p>One other possible avenue for solving a difficult forecasting problem is to resample your
            data. For example, suppose you are forecasting the temperature outside. To collect your data, you place
            a
            thermometer outside and record the temperature every minute. We can consider whether working with
            temperature data recorded every minute makes sense. Chances are that the temperature will not vary much
            minute by minute. It may also introduce unnecessary noise if you have a very sensitive thermometer that
            records changes of 0.1 degrees or less. This is a situation where resampling the data will make sense
            and
            allow you to build performant forecasting models. Here you could resample the data so you have a
            temperature
            reading every hour. That way you'll smooth out the time series and be able to uncover a daily
            seasonality.
            Alternatively, you could resample the data daily and uncover a yearly seasonality. </p>
        <p>Thus, you should explore different resampling possibilities with your time series data. This
            idea can also come from your objective. In the temperature forecasting example, it probably does not
            make
            sense to forecast the temperature for the next minute. No one is likely to be interested in that.
            However,
            forecasting the temperature for the next hour or the next day has value. Therefore, resampling the data
            is
            the way to go. </p>
        <p>Finally, if your forecasting efforts fail, you might want to reach out to someone with
            domain knowledge or look for alternative data. Domain knowledge comes with experience, and people with
            expertise in a certain field can better guide data scientists to uncover new solutions. For example, an
            economist knows that gross domestic product and unemployment are linked, but this connection may be
            unknown
            to the data scientist. Thus, a domain expert can help the data scientist uncover a new relationship and
            look
            for unemployment data in order to forecast gross domestic product. </p>
        <p>As you can see, there are different ways of managing difficult forecasting problems. </p>
        <p>In some cases, you might get absolutely stuck, which can mean that you are working on a very
            advanced problem that has not been tackled before. At this point, having an academic partner who can
            lead a
            research team in trying to solve the problem may be the best option. </p>
        <p><a id="calibre_link-184"></a> <i><b>21.3</b></i></p>
        <p> <i><b>Other applications of time series data</b></i></p>
        <p><b>415</b></p>
        <p>There is always value in failure, and you should not feel defeated if a forecast fails. In
            fact, a failed forecast can help you become a better data scientist, because you'll learn to recognize
            which
            problems have a good chance of being solved and which don't. </p>
        <p> <i><b>21.3</b></i></p>
        <p> <i><b>Other applications of time series data</b></i></p>
        <p>This book focused entirely on forecasting techniques where the objective is to predict a
            continuous numerical value. However, we can do more than forecasting with time series data. We can also
            perform classification. </p>
        <p>In time series classification, the goal is to identify whether a time series is coming from
            one particular category. An example application of time series classification is analyzing data from an
            electrocardiogram (ECG), which evaluates the heart's condition. A healthy heart will generate a
            different
            ECG than a heart with issues. Since the data is gathered over time, this is a perfect situation for
            applying
            time series classification in a real-life situation. </p>
        <p>Time series classification</p>
        <p>Time series classification is a task where the objective is to identify whether a time
            series comes from a particular category. </p>
        <p>For example, we could use time series classification to analyze heart-monitoring data and
            determine if it comes from a healthy heart or not. </p>
        <p>We can also use time series data to perform anomaly detection. An anomaly is basically an
            outlier&mdash;a data point that is significantly different from the rest of the data. </p>
        <p>We can see applications of anomaly detection in data monitoring, which in turn is used for
            application maintenance, intrusion detection, credit card fraud, etc. Taking application maintenance as
            an
            example, imagine that a global e-commerce company</p>
        <p>is tracking page visits over time. If the page visit count suddenly falls to zero, it's
            likely there is a problem with the website. An anomaly detection algorithm would notice the event and
            signal
            the maintenance team about a problem. </p>
        <p>Anomaly detection</p>
        <p>Anomaly detection is a task where the objective is to identify the presence of outliers or
            abnormal data. </p>
        <p>For example, we could track the expenses on someone's credit card. If there is suddenly a
            very large expenditure, this may potentially be an outlier, and maybe the person is a victim of fraud.
        </p>
        <p>Anomaly detection is a particularly interesting challenge, because outliers are often rare,
            and there is a risk of generating many false positives. It also adds another layer of complexity, since
            the
            scarcity of the event means that we have few training labels. </p>
        <p><a id="calibre_link-185"></a><b>416</b></p>
        <p>CHAPTER 21</p>
        <p> <i><b>Going above and beyond</b></i></p>
        <p>NOTE</p>
        <p>If you are curious about this type of problem, I recommend reading</p>
        <p>two papers from Microsoft and Yahoo where they expose how they built their</p>
        <p>own frameworks for time series anomaly detection: Hansheng Ren, Bixiong</p>
        <p>Xu, Yujing Wang, et al., “Time-Series Anomaly Detection Service at Microsoft,”
            arXiv:1906.03821v1 (2019), <a
                href="https://arxiv.org/pdf/1906.03821.pdf">https://arxiv.org/pdf/1906.03821.pdf</a>; and Nikolay
            Laptev, Saeed Amizadeh, and Ian Flint, “Generic and Scalable Framework for Automated Time-series Anomaly
            Detection,” <i>KDD '15: Proceedings of</i></p>
        <p> <i>the 21th ACM SIGKDD International Conference on Knowledge Discovery and
                Data</i> <i>Mining</i> (ACM, 2015), <a href="http://mng.bz/pOwE">http://mng.bz/pOwE</a>. </p>
        <p>There are, of course, many more tasks that we can perform with time series data, such as
            clustering, changepoint detection, simulation, or signal processing. I hope that this encourages you to
            further explore what is possible and what is being done. </p>
        <p> <i><b>21.4</b></i></p>
        <p> <i><b>Keep practicing</b></i></p>
        <p>While this book has provided you with many opportunities to apply your knowledge in the form
            of exercises, real-life scenarios in each chapter, and capstone projects, it is important that you keep
            practicing to truly master time series forecasting. You will gain confidence in your skills and
            encounter
            new problems that will inevitably make you better at handling time series data. </p>
        <p>To do so, you'll need to access time series data. The following list identifies some
            websites where you freely access such data:</p>
        <p> <i>“Datasets,” on Papers with Code</i>&mdash;<a
                href="https://paperswithcode.com/datasets?mod=time-series">https://paperswithcode.com/datasets?mod=time-</a>
        </p>
        <p><a href="https://paperswithcode.com/datasets?mod=time-series">series. </a></p>
        <p>A list of close to a hundred datasets (at the time of writing) for time series analysis. You
            can filter them by task, such as anomaly detection, forecasting, classification, etc. You will likely
            encounter datasets used for research papers, which</p>
        <p>are used to test novel techniques and establish state-of-the-art approaches. </p>
        <p> <i>UCI machine learning repository</i>&mdash;<a
                href="https://archive.ics.uci.edu/ml/datasets.php">https://archive.ics.uci.edu/ml/datasets.php. </a>
        </p>
        <p>This a very popular source of data for many machine learning practitioners. </p>
        <p>Click the link for the Time-Series data type, and you'll find 126 time series datasets. You
            can also filter by task, such as classification, regression (forecasting), and clustering. </p>
        <p> <i>NYC Open Data</i>&mdash;<a
                href="https://opendata.cityofnewyork.us/data/">https://opendata.cityofnewyork.us/data/</a>. </p>
        <p>This website catalogs numerous datasets from the city of New York. You can fil-</p>
        <p>ter by domain, such as education, environment, health, transportation, and more. While not
            all of the datasets are time series, you can still find many of them. You could also check whether your
            local city provides openly accessible</p>
        <p>data and work with that as well. </p>
        <p> <i>Statistics Canada</i><a
                href="http://www150.statcan.gc.ca/n1/en/type/data">&mdash;www150.statcan.gc.ca/n1/en/type/data</a>.
        </p>
        <p>This is a Canadian governmental agency that gives free access to a great amount</p>
        <p>of data, including time series data. You can filter by domain, but also by frequency</p>
        <p><a id="calibre_link-302"></a> <i><b>21.4</b></i></p>
        <p> <i><b>Keep practicing</b></i></p>
        <p><b>417</b></p>
        <p>of sampling (daily, weekly, monthly, etc.). Search your own government's web-</p>
        <p>sites to see if you can find a similar resource. </p>
        <p> <i>Google Trends</i>&mdash;<a
                href="https://trends.google.com/trends/">https://trends.google.com/trends/</a>. </p>
        <p>Google Trends gathers data about searches from all around the world. You can</p>
        <p>search for a particular theme and segment by country. You can also set the length of the
            time series, which changes the sampling frequency. For example, </p>
        <p>you can download the last 24 hours of data, which is sampled every 8 minutes. If you
            download the last 5 years, the data is sampled every week. </p>
        <p> <i>Kaggle</i><a
                href="http://www.kaggle.com/datasets?tags=13209-Time+Series+Analysis">&mdash;www.kaggle.com/datasets?tags=13209-Time+Series+Analysis.
            </a></p>
        <p>Kaggle is a popular website among data scientists where companies can host competitions and
            reward top-performing teams. You can also download time series data&mdash;there are over a thousand
            datasets
            at the time of writing. You can</p>
        <p>also find notebooks that use these datasets to inspire you or give you a starting point.
            However, be careful&mdash;anyone can publish a notebook on Kaggle, and their workflow is not always
            correct.
            Note that you'll need to create a free account to download the dataset on your local machine. </p>
        <p>You now have a wide variety of tools and resources for practicing and honing your skills. I
            wish you good luck in your future endeavors, and I hope that you enjoyed reading this book as much as I
            enjoyed writing it. </p>
        <p><a id="calibre_link-186"></a> <i>appendix</i></p>
        <p> <i>Installation instructions</i></p>
        <p> <i><b>Installing Anaconda</b></i></p>
        <p>The code in this book was run on a Windows 10 computer using Jupyter Notebooks</p>
        <p>with Anaconda. I highly recommend using Anaconda, especially if you are on a Windows
            machine, as it automatically installs Python and many libraries that we'll use throughout the book, such
            as
            pandas, numpy, matplotlib, statsmodels, and others. You can install Anaconda's individual edition, which
            is
            free, from their website (<a
                href="http://www.anaconda.com/products/individual">www.anaconda.com/products/individual). </a>It
            comes
            with a graphical installer, making for an easy installation. Note that at the time of writing, Anaconda
            installs Python 3.9. </p>
        <p> <i><b>Python</b></i></p>
        <p>If you follow the recommendation of using Anaconda, you will not need to install Python
            separately. If you do need to install Python separately, you can download it from the official website
            (<a href="http://www.python.org/downloads/">www.python.org/downloads/</a>). The code in this book used
            Python 3.7, but any later version of Python will also work. </p>
        <p> <i><b>Jupyter Notebooks</b></i></p>
        <p>The code in this book was run on Jupyter Notebooks. This allows you to immedi-</p>
        <p>ately see the output of your code, and it's a great tool for learning and exploration. </p>
        <p>It also allows you to write text and display equations. </p>
        <p>Assuming you installed Anaconda, Jupyter Notebook will also install on your</p>
        <p>machine. On Windows, you can press the Windows key and start typing Jupyter</p>
        <p>Notebook. You can then launch the application, which will open your browser. It will display
            a folder structure, and you can navigate to where you want to save your notebooks or where you cloned
            the
            GitHub repository containing the source code. </p>
        <p><b>418</b></p>
        <p><a id="calibre_link-208"></a>APPENDIX</p>
        <p> <i><b>Installation instructions</b></i></p>
        <p><b>419</b></p>
        <p> <i><b>GitHub Repository</b></i></p>
        <p>The entire source code for this book is available on GitH<a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython">ub: https://github.com/mar-</a>
        </p>
        <p><a
                href="https://github.com/marcopeix/TimeSeriesForecastingInPython">copeix/TimeSeriesForecastingInPython</a>.
            At the root of the repository, there is a data folder that contains all the data files used throughout
            the
            entire book. </p>
        <p>The repository is organized by chapter. Each folder contains a notebook that will run all
            the code and generate the figures for that specific chapter. You can also find the solutions to the
            exercises there. Provided that Git is installed, you can clone the repository and access it on your
            local
            machine:</p>
        <p>git clone https:/ /github.com/marcopeix/TimeSeriesForecastingInPython.git</p>
        <p>If Git is not installed, you can download and install it from the Git website (<a
                href="https://git-scm.com/downloads">https://git-</a></p>
        <p><a href="https://git-scm.com/downloads">scm.com/downloads). </a>I then recommend those on
            Windows use Git Bash to run the preceding command. </p>
        <p> <i><b>Installing Prophet</b></i></p>
        <p>In this book, we use the Prophet library, a popular forecasting library that automates most
            of the process. Windows users might have some trouble installing the library, even when using Anaconda.
        </p>
        <p>To install the library, you can run the following commands at your Anaconda prompt: conda
            install libpython m2w64-toolchain -c msys2</p>
        <p>conda install numpy cython matplotlib scipy pandas -c conda-forge</p>
        <p>conda install -c conda-forge pystan</p>
        <p>conda install -c conda-forge fbprophet</p>
        <p> <i><b>Installing libraries in Anaconda</b></i></p>
        <p>If at any time you need to install a particular library while using Anaconda, you can do a
            Google search: conda &lt;package name<a href="https://anaconda.org/conda-forge/&lt;package name&gt;">&gt;.
                The first result should lead you to the https:/</a>/</p>
        <p>anaconda.org/conda-forge/&lt;package name&gt; website, where you will see a list of commands
            that will install the package. Usually, the first command will work, and it will have the format conda
            install -c conda-forge &lt;package name&gt;. </p>
        <p>For example, to install TensorFlow 2.6 with Anaconda, you can run conda install</p>
        <p>-c conda-forge tensorflow at your Anaconda prompt. </p>
        <p><a id="calibre_link-454"></a> </p>
        <p><a id="calibre_link-187"></a> <i>index</i></p>
        <p>Symbols</p>
        <p>compared to SARIMA <a href="#calibre_link-90">176</a><a href="#calibre_link-91">&ndash;178</a></p>
        <p>forecasting with <a href="#calibre_link-88">165&ndash;</a><a href="#calibre_link-89">171</a>
        </p>
        <p>%matplotlib inline function <a href="#calibre_link-100">218</a></p>
        <p>ARLSTM (autoregressive LSTM) model</p>
        <p><a href="#calibre_link-20">320</a><a href="#calibre_link-147">&ndash;328</a></p>
        <p>Numerics</p>
        <p>building models <a href="#calibre_link-145">322&ndash;</a><a href="#calibre_link-188">326</a></p>
        <p>household electric power consumption </p>
        <p>1D convolution <a href="#calibre_link-189">307</a></p>
        <p>forecast <a href="#calibre_link-163">355</a><a href="#calibre_link-164">&ndash;356</a></p>
        <p>introduction to <a href="#calibre_link-144">321&ndash;</a><a href="#calibre_link-145">322</a></p>
        <p>A</p>
        <p>ARMA (autoregressive moving average) process</p>
        <p>defined <a href="#calibre_link-82">142</a></p>
        <p>ACF (autocorrelation function) <a href="#calibre_link-48">41&ndash;</a><a href="#calibre_link-49">42,
            </a><a href="#calibre_link-190">62</a><a href="#calibre_link-191">, 83</a><a href="#calibre_link-192">,
                103</a>
        </p>
        <p>examining <a href="#calibre_link-71">105</a><a href="#calibre_link-72">&ndash;106</a></p>
        <p>acorr_ljungbox function <a href="#calibre_link-193">123</a></p>
        <p>identifying stationary <a href="#calibre_link-72">106</a><a href="#calibre_link-73">&ndash;111</a></p>
        <p>activation function <a href="#calibre_link-194">277</a></p>
        <p>ARMA (p,q) (autoregressive moving average) </p>
        <p>add_changepoints_to_plot method <a href="#calibre_link-170">374</a></p>
        <p>model <a href="#calibre_link-6">61, </a><a href="#calibre_link-195">129, </a><a href="#calibre_link-9">140,
            </a><a href="#calibre_link-196">157, </a><a href="#calibre_link-197">167</a>
        </p>
        <p>add_country_holidays method <a href="#calibre_link-198">385</a></p>
        <p>ArmaProcess function <a href="#calibre_link-199">90</a>, <a href="#calibre_link-200">108</a>
        </p>
        <p>ADF (augmented Dickey-Fuller) test <a href="#calibre_link-47">38, </a><a href="#calibre_link-200">108,
            </a><a href="#calibre_link-201">147, </a></p>
        <p>AR(p) (autoregressive) model <a href="#calibre_link-6">61</a><a href="#calibre_link-64">,
                84</a><a href="#calibre_link-9">, 140</a></p>
        <p><a href="#calibre_link-202">188</a><a href="#calibre_link-203">, 205</a><a href="#calibre_link-103">,
                222</a><a href="#calibre_link-166">, 362</a>, <a href="#calibre_link-181">411</a></p>
        <p><a href="#calibre_link-47">augmented Dickey-Fuller test. </a> <i>See</i>
            ADF (aug-</p>
        <p>adfuller function <a href="#calibre_link-204">67, </a><a href="#calibre_link-205">127</a>
        </p>
        <p>mented Dickey-Fuller) test</p>
        <p>AIC (Akaike information criterion) <a href="#calibre_link-203">205</a><a href="#calibre_link-23">,
                361</a><a href="#calibre_link-181">, 411</a></p>
        <p>auto.arima library <a href="#calibre_link-166">362</a></p>
        <p>introduction to <a href="#calibre_link-74">113&ndash;</a><a href="#calibre_link-75">114</a>
        </p>
        <p><a href="#calibre_link-190">autocorrelation function. <i>See</i></a> ACF
            (autocorrelation </p>
        <p>selecting model using <a href="#calibre_link-75">114</a><a href="#calibre_link-76">&ndash;116</a></p>
        <p>function)</p>
        <p>alpha parameter <a href="#calibre_link-206">21</a></p>
        <p>automating time series forecasting <a href="#calibre_link-23">361&ndash;</a><a
                href="#calibre_link-207">395</a></p>
        <p>Anaconda</p>
        <p>automated forecasting libraries <a href="#calibre_link-166">362</a><a href="#calibre_link-167">&ndash;363</a>
        </p>
        <p>installing <a href="#calibre_link-186">418</a></p>
        <p>Prophet <a href="#calibre_link-167">363</a><a href="#calibre_link-168">&ndash;365</a></p>
        <p>installing libraries in <a href="#calibre_link-208">419</a></p>
        <p>advanced functionality <a href="#calibre_link-169">370</a><a href="#calibre_link-172">&ndash;381</a></p>
        <p>antidiabetic drug prescriptions, forecasting <a href="#calibre_link-13">216&ndash;</a><a
                href="#calibre_link-106">229</a></p>
        <p>basic forecasting with <a href="#calibre_link-168">365</a><a href="#calibre_link-209">&ndash;369</a></p>
        <p>forecasting and evaluating performance <a href="#calibre_link-105">225&ndash;</a><a
                href="#calibre_link-106">229</a></p>
        <p>implementing robust forecasting </p>
        <p>importing required libraries and loading </p>
        <p>process <a href="#calibre_link-172">381&ndash;</a><a href="#calibre_link-174">393</a></p>
        <p>data <a href="#calibre_link-100">218&ndash;</a><a href="#calibre_link-101">219</a></p>
        <p>AutoRegressive class <a href="#calibre_link-210">323</a></p>
        <p>modeling data <a href="#calibre_link-102">220</a><a href="#calibre_link-104">&ndash;224</a>
        </p>
        <p>autoregressive process <a href="#calibre_link-7">81</a><a href="#calibre_link-70">&ndash;100</a></p>
        <p>conducting residual analysis <a href="#calibre_link-104">224</a></p>
        <p>defining <a href="#calibre_link-64">84</a><a href="#calibre_link-65">&ndash;85</a></p>
        <p>performing model selection <a href="#calibre_link-103">222</a><a href="#calibre_link-104">&ndash;224</a>
        </p>
        <p>forecasting <a href="#calibre_link-67">92</a><a href="#calibre_link-68">&ndash;98</a></p>
        <p>visualizing series and components <a href="#calibre_link-101">219</a><a
                href="#calibre_link-102">&ndash;220</a></p>
        <p>predicting average weekly foot traffic <a href="#calibre_link-63">82</a><a
                href="#calibre_link-191">&ndash;83</a></p>
        <p>AR (autoregressive) model <a href="#calibre_link-54">55</a></p>
        <p>stationary</p>
        <p>ARIMA (autoregressive integrated moving aver-</p>
        <p>finding order of <a href="#calibre_link-65">85&ndash;</a><a href="#calibre_link-67">92</a>
        </p>
        <p>age) model <a href="#calibre_link-82">142</a><a href="#calibre_link-83">&ndash;143</a></p>
        <p>partial autocorrelation function <a href="#calibre_link-66">89&ndash;</a><a href="#calibre_link-67">92</a>
        </p>
        <p><b>421</b></p>
        <p><a id="calibre_link-315"></a><b>422</b></p>
        <p>INDEX</p>
        <p>average weekly foot traffic, predicting <a href="#calibre_link-63">82&ndash;</a><a
                href="#calibre_link-191">83</a></p>
        <p>data wrangling <a href="#calibre_link-150">333</a><a href="#calibre_link-153">&ndash;338</a>
        </p>
        <p>ax object <a href="#calibre_link-206">21</a></p>
        <p>datetime data type <a href="#calibre_link-211">336</a></p>
        <p>date_time feature <a href="#calibre_link-212">242</a></p>
        <p>B</p>
        <p>datetime library <a href="#calibre_link-212">242</a></p>
        <p>DateTime object <a href="#calibre_link-213">332</a></p>
        <p>bandwidth usage, forecasting <a href="#calibre_link-214">102</a><a href="#calibre_link-71">&ndash;105</a><a
                href="#calibre_link-79">, 132</a><a href="#calibre_link-80">&ndash;136</a></p>
        <p>decomposition <a href="#calibre_link-31">5</a></p>
        <p>Baseline class <a href="#calibre_link-113">260</a></p>
        <p>deep learning <a href="#calibre_link-15">233</a>&ndash;<a href="#calibre_link-215">286</a>
        </p>
        <p>baseline models <a href="#calibre_link-216">16</a><a href="#calibre_link-38">&ndash;17</a><a
                href="#calibre_link-217">, 71</a></p>
        <p>applying <a href="#calibre_link-108">237</a>&ndash;<a href="#calibre_link-218">245</a></p>
        <p>batches <a href="#calibre_link-219">251</a></p>
        <p>data exploration <a href="#calibre_link-108">237&ndash;</a><a href="#calibre_link-109">241</a></p>
        <p>bps (bits per second) <a href="#calibre_link-214">102</a></p>
        <p>data splitting <a href="#calibre_link-109">241&ndash;</a><a href="#calibre_link-218">245</a>
        </p>
        <p>feature engineering <a href="#calibre_link-109">241</a><a href="#calibre_link-218">&ndash;245</a></p>
        <p>C</p>
        <p>applying baseline models <a href="#calibre_link-113">260&ndash;</a><a href="#calibre_link-116">268</a>
        </p>
        <p>multi-output baseline model <a href="#calibre_link-115">266</a><a href="#calibre_link-116">&ndash;268</a>
        </p>
        <p>call function <a href="#calibre_link-220">324</a></p>
        <p>multi-step baseline models <a href="#calibre_link-114">263</a><a href="#calibre_link-115">&ndash;266</a>
        </p>
        <p>changepoint_prior_scale parameter <a href="#calibre_link-171">379</a></p>
        <p>single-step baseline model <a href="#calibre_link-113">260</a><a href="#calibre_link-221">&ndash;262</a>
        </p>
        <p>CNNs (convolutional neural networks) <a href="#calibre_link-19">305&ndash;</a><a href="#calibre_link-20">320,
            </a></p>
        <p>data windowing <a href="#calibre_link-111">249&ndash;</a><a href="#calibre_link-113">260</a>
        </p>
        <p><a href="#calibre_link-182">412</a></p>
        <p>how deep learning models are trained</p>
        <p>household electric power consumption </p>
        <p><a href="#calibre_link-111">249</a><a href="#calibre_link-112">&ndash;253</a></p>
        <p>forecast <a href="#calibre_link-161">351&ndash;</a><a href="#calibre_link-163">355</a></p>
        <p>implementing DataWindow class <a href="#calibre_link-112">253</a><a href="#calibre_link-113">&ndash;260</a>
        </p>
        <p>implementing <a href="#calibre_link-138">309</a><a href="#calibre_link-142">&ndash;317</a>
        </p>
        <p>household electric power consumption </p>
        <p>as multi-output model <a href="#calibre_link-141">315</a>&ndash;<a href="#calibre_link-142">317</a></p>
        <p>forecast</p>
        <p>as multi-step model <a href="#calibre_link-140">314&ndash;</a><a href="#calibre_link-141">315</a></p>
        <p>modeling <a href="#calibre_link-158">346&ndash;</a><a href="#calibre_link-165">358</a></p>
        <p>as single-step model <a href="#calibre_link-139">310&ndash;</a><a href="#calibre_link-222">313</a></p>
        <p>preparing for modeling <a href="#calibre_link-156">342&ndash;</a><a href="#calibre_link-158">346</a></p>
        <p>introduction to <a href="#calibre_link-137">306</a><a href="#calibre_link-138">&ndash;309</a></p>
        <p>implementing deep neural network</p>
        <p>compile_and_fit function <a href="#calibre_link-119">272, </a><a href="#calibre_link-132">295, </a><a
                href="#calibre_link-138">309</a><a href="#calibre_link-145">,
                322</a><a href="#calibre_link-213">, 332</a></p>
        <p><a href="#calibre_link-122">276</a><a href="#calibre_link-223">&ndash;283</a></p>
        <p>complex time series <a href="#calibre_link-8">101</a><a href="#calibre_link-224">&ndash;139</a></p>
        <p>as multi-output model <a href="#calibre_link-125">282&ndash;</a><a href="#calibre_link-223">283</a></p>
        <p>autoregressive moving average process</p>
        <p>as multi-step model <a href="#calibre_link-124">281&ndash;</a><a href="#calibre_link-125">282</a></p>
        <p>identifying stationary <a href="#calibre_link-72">106</a><a href="#calibre_link-73">&ndash;111</a></p>
        <p>as single-step model <a href="#calibre_link-123">278&ndash;</a><a href="#calibre_link-225">280</a></p>
        <p>introduction to <a href="#calibre_link-71">105&ndash;</a><a href="#calibre_link-72">106</a>
        </p>
        <p>implementing linear model <a href="#calibre_link-118">271</a><a href="#calibre_link-122">&ndash;276</a>
        </p>
        <p>devising modeling procedure <a href="#calibre_link-73">111&ndash;</a><a href="#calibre_link-79">132</a>
        </p>
        <p>implementing multi-output linear </p>
        <p>Akaike information criterion <a href="#calibre_link-74">113&ndash;</a><a href="#calibre_link-76">116</a>
        </p>
        <p>model <a href="#calibre_link-121">275&ndash;</a><a href="#calibre_link-122">276</a></p>
        <p>residual analysis <a href="#calibre_link-76">116&ndash;</a><a href="#calibre_link-226">124</a></p>
        <p>implementing single-step linear model</p>
        <p>forecasting bandwidth usage <a href="#calibre_link-214">102</a><a href="#calibre_link-71">&ndash;105</a>,
        </p>
        <p><a href="#calibre_link-119">272</a></p>
        <p><a href="#calibre_link-79">132</a><a href="#calibre_link-80">&ndash;136</a></p>
        <p>types of models <a href="#calibre_link-107">234&ndash;</a><a href="#calibre_link-108">237</a></p>
        <p>consumption, forecasting <a href="#calibre_link-97">203&ndash;</a><a href="#calibre_link-227">213</a></p>
        <p>when to use <a href="#calibre_link-107">234</a></p>
        <p>Conv1D layer <a href="#calibre_link-189">307, </a><a href="#calibre_link-144">321, </a><a
                href="#calibre_link-228">353</a></p>
        <p>Dense layer <a href="#calibre_link-229">273, </a><a href="#calibre_link-230">296, </a><a
                href="#calibre_link-231">298, </a><a href="#calibre_link-232">311, </a><a href="#calibre_link-144">321,
            </a><a href="#calibre_link-159">349&ndash;</a><a href="#calibre_link-160">350</a></p>
        <p>convolution operation <a href="#calibre_link-19">305</a></p>
        <p>describe method <a href="#calibre_link-109">241</a><a href="#calibre_link-153">, 338</a></p>
        <p>CPI (consumer price index) <a href="#calibre_link-176">397</a></p>
        <p>diff method <a href="#calibre_link-233">43</a><a href="#calibre_link-204">, 67</a></p>
        <p>cpi variable <a href="#calibre_link-92">183</a></p>
        <p>disposable income, forecasting <a href="#calibre_link-97">203&ndash;</a><a href="#calibre_link-227">213</a>
        </p>
        <p>cross_validation function <a href="#calibre_link-234">375</a></p>
        <p>DNNs (deep neural networks) <a href="#calibre_link-17">270</a><a href="#calibre_link-122">,
                276</a><a href="#calibre_link-223">&ndash;283</a></p>
        <p>cumsum method <a href="#calibre_link-235">33</a></p>
        <p>household electric power consumption </p>
        <p>forecast <a href="#calibre_link-160">350&ndash;</a><a href="#calibre_link-161">351</a></p>
        <p>D</p>
        <p>implementing as multi-output model</p>
        <p><a href="#calibre_link-125">282</a><a href="#calibre_link-223">&ndash;283</a></p>
        <p>data conversion <a href="#calibre_link-152">335</a></p>
        <p>implementing as multi-step model <a href="#calibre_link-124">281&ndash;</a><a
                href="#calibre_link-125">282</a></p>
        <p>data exploration <a href="#calibre_link-108">237&ndash;</a><a href="#calibre_link-109">241</a></p>
        <p>implementing as single-step model <a href="#calibre_link-123">278&ndash;</a><a
                href="#calibre_link-225">280</a></p>
        <p>DataFrame data structure <a href="#calibre_link-236">18</a></p>
        <p>data resampling <a href="#calibre_link-152">335&ndash;</a><a href="#calibre_link-153">338</a></p>
        <p>E</p>
        <p>datasets module <a href="#calibre_link-92">183</a></p>
        <p>data splitting <a href="#calibre_link-109">241&ndash;</a><a href="#calibre_link-218">245</a>
        </p>
        <p>early_stopping function <a href="#calibre_link-229">273</a></p>
        <p>DataWindow class <a href="#calibre_link-111">249</a><a href="#calibre_link-112">,
                253&ndash;</a><a href="#calibre_link-113">260, </a><a href="#calibre_link-229">273, </a><a
                href="#calibre_link-132">295, </a><a href="#calibre_link-138">309, </a></p>
        <p>endogenous <a href="#calibre_link-237">181</a></p>
        <p><a href="#calibre_link-145">322, </a><a href="#calibre_link-157">343&ndash;</a><a
                href="#calibre_link-238">345</a></p>
        <p>EPS (earnings per share) <a href="#calibre_link-239">15, </a><a href="#calibre_link-82">142</a></p>
        <p>data windowing <a href="#calibre_link-111">249</a><a href="#calibre_link-113">&ndash;260</a>
        </p>
        <p>evaluate method <a href="#calibre_link-240">261</a></p>
        <p>implementing DataWindow class <a href="#calibre_link-112">253&ndash;</a><a href="#calibre_link-113">260</a>
        </p>
        <p>exogenous predictors or input variables <a href="#calibre_link-11">180</a></p>
        <p>training models <a href="#calibre_link-111">249&ndash;</a><a href="#calibre_link-112">253</a></p>
        <p>external variables, adding to model <a href="#calibre_link-11">180&ndash;</a><a
                href="#calibre_link-241">196</a></p>
        <p><a id="calibre_link-316"></a>INDEX</p>
        <p><b>423</b></p>
        <p>F</p>
        <p>preparing for modeling <a href="#calibre_link-156">342&ndash;</a><a href="#calibre_link-158">346</a></p>
        <p>defining DataWindow class <a href="#calibre_link-157">343</a><a href="#calibre_link-238">&ndash;345</a>
        </p>
        <p>feature engineering</p>
        <p>initial setup <a href="#calibre_link-156">342</a><a href="#calibre_link-157">&ndash;343</a>
        </p>
        <p>deep learning <a href="#calibre_link-109">241</a>&ndash;<a href="#calibre_link-218">245</a>
        </p>
        <p>utility function <a href="#calibre_link-158">346</a></p>
        <p>household electric power consumption </p>
        <p>hyperbolic tangent (tanh) activation function <a href="#calibre_link-130">292</a></p>
        <p>forecast <a href="#calibre_link-153">338</a><a href="#calibre_link-156">&ndash;342</a></p>
        <p>identifying seasonal period <a href="#calibre_link-154">339</a><a href="#calibre_link-155">&ndash;341</a>
        </p>
        <p>I</p>
        <p>removing unnecessary columns <a href="#calibre_link-153">338</a></p>
        <p>splitting and scaling data <a href="#calibre_link-155">341&ndash;</a><a href="#calibre_link-156">342</a>
        </p>
        <p>imputing process <a href="#calibre_link-151">334</a></p>
        <p>filters parameter <a href="#calibre_link-232">311</a></p>
        <p>infl variable <a href="#calibre_link-242">184</a></p>
        <p>first-order differencing <a href="#calibre_link-243">37</a></p>
        <p>__init__ function <a href="#calibre_link-210">323</a></p>
        <p>fit method <a href="#calibre_link-244">367</a></p>
        <p>input gate <a href="#calibre_link-130">292&ndash;</a><a href="#calibre_link-131">294</a></p>
        <p>forget gate <a href="#calibre_link-129">291</a><a href="#calibre_link-130">&ndash;292</a>
        </p>
        <p>integrated series <a href="#calibre_link-83">143</a></p>
        <p>integrated time series <a href="#calibre_link-81">137</a></p>
        <p>G</p>
        <p>integration order <a href="#calibre_link-9">140</a></p>
        <p> <i>Interpretable Machine Learning</i> (Molnar) <a href="#calibre_link-142">317</a></p>
        <p>GDP, forecasting using SARIMAX model</p>
        <p>inverse transform <a href="#calibre_link-47">38</a></p>
        <p><a href="#calibre_link-94">186</a><a href="#calibre_link-95">&ndash;195</a></p>
        <p>isna() method <a href="#calibre_link-151">334</a></p>
        <p>GitHub Repository <a href="#calibre_link-208">419</a></p>
        <p>Granger causality test <a href="#calibre_link-96">201&ndash;</a><a href="#calibre_link-97">203</a></p>
        <p>J</p>
        <p>grangercausalitytests function <a href="#calibre_link-245">207</a></p>
        <p>GRU (gated recurrent unit) <a href="#calibre_link-246">288</a></p>
        <p>Jupyter Notebooks <a href="#calibre_link-186">418</a></p>
        <p>H</p>
        <p>K</p>
        <p>hidden layers <a href="#calibre_link-194">277</a></p>
        <p>kernel <a href="#calibre_link-137">306</a></p>
        <p>historical mean, forecasting <a href="#calibre_link-38">17</a><a href="#calibre_link-247">&ndash;22</a>
        </p>
        <p>implementing historical mean baseline</p>
        <p>L</p>
        <p><a href="#calibre_link-39">19&ndash;</a><a href="#calibre_link-247">22</a></p>
        <p>setting up for baseline implementations</p>
        <p>lags parameter <a href="#calibre_link-248">68</a></p>
        <p><a href="#calibre_link-38">17&ndash;</a><a href="#calibre_link-39">19</a></p>
        <p>last known value</p>
        <p>holidays_prior_scale parameter <a href="#calibre_link-171">379</a></p>
        <p>multi-step baseline models <a href="#calibre_link-114">263</a><a href="#calibre_link-249">&ndash;264</a>
        </p>
        <p>horizon parameter <a href="#calibre_link-250">72</a></p>
        <p>naive prediction of the future <a href="#calibre_link-41">25</a><a href="#calibre_link-42">&ndash;26</a>
        </p>
        <p>household electric power consumption, </p>
        <p>LDA (linear discriminant analysis) <a href="#calibre_link-218">245</a></p>
        <p>forecasting <a href="#calibre_link-21">329&ndash;</a><a href="#calibre_link-165">358</a></p>
        <p>linear model <a href="#calibre_link-17">270</a></p>
        <p>data wrangling and preprocessing</p>
        <p>Ljung-Box test <a href="#calibre_link-251">120&ndash;</a><a href="#calibre_link-77">121</a><a
                href="#calibre_link-252">, 150</a><a href="#calibre_link-253">,
                170</a><a href="#calibre_link-254">, 191</a><a href="#calibre_link-255">, 208</a><a
                href="#calibre_link-104">, 224</a>, </p>
        <p><a href="#calibre_link-150">333&ndash;</a><a href="#calibre_link-153">338</a></p>
        <p><a href="#calibre_link-256">406</a><a href="#calibre_link-182">, 412</a></p>
        <p>data conversion <a href="#calibre_link-152">335</a></p>
        <p>look-ahead bias <a href="#calibre_link-36">12</a></p>
        <p>data resampling <a href="#calibre_link-152">335</a><a href="#calibre_link-153">&ndash;338</a></p>
        <p>LSTMCell class <a href="#calibre_link-220">324</a></p>
        <p>dealing with missing data <a href="#calibre_link-151">334</a></p>
        <p>LSTMCell layer <a href="#calibre_link-210">323</a></p>
        <p>feature engineering <a href="#calibre_link-153">338</a><a href="#calibre_link-156">&ndash;342</a></p>
        <p>LSTM layer <a href="#calibre_link-230">296, </a><a href="#calibre_link-140">314, </a><a
                href="#calibre_link-144">321, </a><a href="#calibre_link-161">351</a></p>
        <p>identifying seasonal period <a href="#calibre_link-154">339</a><a href="#calibre_link-155">&ndash;341</a>
        </p>
        <p>LSTM (long short-term memory) model <a href="#calibre_link-18">287&ndash;</a><a
                href="#calibre_link-257">304</a></p>
        <p>removing unnecessary columns <a href="#calibre_link-153">338</a></p>
        <p>autoregressive <a href="#calibre_link-20">320&ndash;</a><a href="#calibre_link-147">328</a>
        </p>
        <p>splitting and scaling data <a href="#calibre_link-155">341&ndash;</a><a href="#calibre_link-156">342</a>
        </p>
        <p>household electric power consumption </p>
        <p>modeling <a href="#calibre_link-158">346&ndash;</a><a href="#calibre_link-165">358</a></p>
        <p>forecast <a href="#calibre_link-161">351</a><a href="#calibre_link-162">, 354</a><a
                href="#calibre_link-163">&ndash;355</a></p>
        <p>ARLSTM model <a href="#calibre_link-163">355&ndash;</a><a href="#calibre_link-164">356</a>
        </p>
        <p>implementing <a href="#calibre_link-132">295&ndash;</a><a href="#calibre_link-135">302</a>
        </p>
        <p>baseline models <a href="#calibre_link-158">346&ndash;</a><a href="#calibre_link-159">349</a></p>
        <p>as multi-output model <a href="#calibre_link-134">299</a><a href="#calibre_link-135">&ndash;302</a></p>
        <p>CNNs <a href="#calibre_link-161">351&ndash;</a><a href="#calibre_link-228">353</a></p>
        <p>as multi-step model <a href="#calibre_link-133">297</a><a href="#calibre_link-134">&ndash;299</a></p>
        <p>combining CNN with LSTM <a href="#calibre_link-162">354&ndash;</a><a href="#calibre_link-163">355</a></p>
        <p>as single-step model <a href="#calibre_link-132">295</a><a href="#calibre_link-133">&ndash;297</a></p>
        <p>DNNs <a href="#calibre_link-160">350</a><a href="#calibre_link-161">&ndash;351</a></p>
        <p>overview of <a href="#calibre_link-128">290</a><a href="#calibre_link-132">&ndash;295</a>
        </p>
        <p>linear model <a href="#calibre_link-159">349</a><a href="#calibre_link-160">&ndash;350</a>
        </p>
        <p>forget gate <a href="#calibre_link-129">291</a><a href="#calibre_link-130">&ndash;292</a>
        </p>
        <p>LSTM model <a href="#calibre_link-161">351</a></p>
        <p>input gate <a href="#calibre_link-130">292&ndash;</a><a href="#calibre_link-131">294</a></p>
        <p>selecting best model <a href="#calibre_link-164">356&ndash;</a><a href="#calibre_link-165">358</a></p>
        <p>output gate <a href="#calibre_link-131">294&ndash;</a><a href="#calibre_link-132">295</a>
        </p>
        <p>objective of project <a href="#calibre_link-149">331&ndash;</a><a href="#calibre_link-213">332</a></p>
        <p>recurrent neural networks <a href="#calibre_link-246">288</a>&ndash;<a href="#calibre_link-128">290</a>
        </p>
        <p>overview of project <a href="#calibre_link-148">330</a><a href="#calibre_link-213">&ndash;332</a></p>
        <p>lstm_rnn layer <a href="#calibre_link-210">323</a></p>
        <p><a id="calibre_link-317"></a><b>424</b></p>
        <p>INDEX</p>
        <p>M</p>
        <p>implementing LSTM model as <a href="#calibre_link-133">297&ndash;</a><a href="#calibre_link-134">299</a>
        </p>
        <p>predicting last known value <a href="#calibre_link-114">263&ndash;</a><a href="#calibre_link-249">264</a>
        </p>
        <p>m1 variable <a href="#calibre_link-242">184</a></p>
        <p>repeating input sequence <a href="#calibre_link-249">264&ndash;</a><a href="#calibre_link-115">266</a>
        </p>
        <p>MAE (mean absolute error) <a href="#calibre_link-258">77</a><a href="#calibre_link-68">, 98,
            </a><a href="#calibre_link-80">136, </a><a href="#calibre_link-240">261, </a><a
                href="#calibre_link-127">285, </a></p>
        <p><a href="#calibre_link-231">298, </a><a href="#calibre_link-143">318, </a><a href="#calibre_link-147">328,
            </a><a href="#calibre_link-156">342, </a><a href="#calibre_link-259">368,
            </a><a href="#calibre_link-176">397</a></p>
        <p>N</p>
        <p>make_dataset function <a href="#calibre_link-260">257</a></p>
        <p>make_future_dataframe method <a href="#calibre_link-244">367</a></p>
        <p>naive prediction of the future <a href="#calibre_link-3">14</a><a href="#calibre_link-261">&ndash;29</a>
        </p>
        <p>MA (moving average) model <a href="#calibre_link-54">55</a><a href="#calibre_link-57">,
                63</a></p>
        <p>defining baseline model <a href="#calibre_link-216">16</a><a href="#calibre_link-38">&ndash;17</a></p>
        <p>mape function <a href="#calibre_link-262">20</a></p>
        <p>forecasting historical mean <a href="#calibre_link-38">17&ndash;</a><a href="#calibre_link-247">22</a>
        </p>
        <p>MAPE (mean absolute percentage error) <a href="#calibre_link-39">19</a><a href="#calibre_link-90">,
                176</a>,
            implementing historical mean baseline <a href="#calibre_link-39">19&ndash;</a><a
                href="#calibre_link-247">22</a></p>
        <p><a href="#calibre_link-263">194, </a><a href="#calibre_link-264">212, </a><a href="#calibre_link-259">368</a>
        </p>
        <p>setting up for baseline implementations</p>
        <p>MA(q) (moving average) model <a href="#calibre_link-6">61, </a><a href="#calibre_link-7">81,
            </a><a href="#calibre_link-8">101, </a><a href="#calibre_link-9">140</a></p>
        <p><a href="#calibre_link-38">17</a><a href="#calibre_link-39">&ndash;19</a></p>
        <p>matplotlib library <a href="#calibre_link-206">21, </a><a href="#calibre_link-186">418</a>
        </p>
        <p>forecasting last year's mean <a href="#calibre_link-40">23&ndash;</a><a href="#calibre_link-41">25</a>
        </p>
        <p>max_epochs parameter <a href="#calibre_link-229">273</a></p>
        <p>implementing naive seasonal forecast <a href="#calibre_link-42">26</a><a
                href="#calibre_link-43">&ndash;28</a></p>
        <p>mean_absolute_error function <a href="#calibre_link-258">77</a></p>
        <p>predicting using last known value <a href="#calibre_link-41">25&ndash;</a><a href="#calibre_link-42">26</a>
        </p>
        <p>mean_squared_error function <a href="#calibre_link-265">51, </a><a href="#calibre_link-266">74, </a><a
                href="#calibre_link-267">96</a></p>
        <p>naive seasonal forecast, implementing <a href="#calibre_link-42">26</a><a
                href="#calibre_link-43">&ndash;28</a></p>
        <p>mean squared error (MSE) <a href="#calibre_link-35">11</a><a href="#calibre_link-265">,
                51</a><a href="#calibre_link-266">, 74</a><a href="#calibre_link-268">, 93</a><a
                href="#calibre_link-240">, 261</a>, NeuralProphet website <a href="#calibre_link-166">362</a></p>
        <p><a href="#calibre_link-118">271, </a><a href="#calibre_link-156">342, </a><a href="#calibre_link-178">400</a>
        </p>
        <p>non-stationary time series <a href="#calibre_link-9">140</a><a href="#calibre_link-269">&ndash;155</a>
        </p>
        <p>method parameter <a href="#calibre_link-250">72</a></p>
        <p>autoregressive integrated moving average </p>
        <p>Model class <a href="#calibre_link-210">323</a></p>
        <p>model <a href="#calibre_link-82">142&ndash;</a><a href="#calibre_link-83">143</a></p>
        <p>Molnar, Christof <a href="#calibre_link-142">317</a></p>
        <p>forecasting <a href="#calibre_link-84">145&ndash;</a><a href="#calibre_link-85">154</a></p>
        <p>monitor parameter <a href="#calibre_link-229">273</a></p>
        <p>modifying general modeling procedure</p>
        <p>monthly air passengers, forecasting number </p>
        <p><a href="#calibre_link-83">143</a><a href="#calibre_link-84">&ndash;145</a></p>
        <p>of <a href="#calibre_link-87">163</a><a href="#calibre_link-91">&ndash;178</a></p>
        <p>numpy library <a href="#calibre_link-39">19, </a><a href="#calibre_link-235">33</a><a
                href="#calibre_link-204">, 67</a><a href="#calibre_link-205">, 127</a><a href="#calibre_link-150">,
                333</a><a href="#calibre_link-168">, 365</a><a href="#calibre_link-186">, 418</a></p>
        <p>monthly average retail price, forecasting <a href="#calibre_link-24">396&ndash;</a><a
                href="#calibre_link-180">409</a></p>
        <p>data preprocessing and visualization <a href="#calibre_link-177">398</a><a
                href="#calibre_link-178">&ndash;400</a></p>
        <p>O</p>
        <p>developing SARIMA model <a href="#calibre_link-179">404&ndash;</a><a href="#calibre_link-180">409</a></p>
        <p>modeling with Prophet <a href="#calibre_link-178">400&ndash;</a><a href="#calibre_link-270">403</a></p>
        <p>optimize_ARIMA function <a href="#calibre_link-271">149, </a><a href="#calibre_link-272">168</a></p>
        <p>objective of project <a href="#calibre_link-176">397</a>&ndash;<a href="#calibre_link-177">398</a></p>
        <p>optimize_ARMA function <a href="#calibre_link-273">115, </a><a href="#calibre_link-271">149</a></p>
        <p>overview of project <a href="#calibre_link-176">397&ndash;</a><a href="#calibre_link-177">398</a></p>
        <p>optimize_SARIMA function <a href="#calibre_link-272">168</a></p>
        <p>moving average (MA) model <a href="#calibre_link-54">55</a><a href="#calibre_link-57">,
                63</a></p>
        <p>optimize_SARIMAX function <a href="#calibre_link-202">188, </a><a href="#calibre_link-274">223, </a><a
                href="#calibre_link-275">390, </a><a href="#calibre_link-276">405</a></p>
        <p>moving average (MA(q)) model <a href="#calibre_link-6">61, </a><a href="#calibre_link-7">81,
            </a><a href="#calibre_link-8">101, </a><a href="#calibre_link-9">140</a></p>
        <p>optimize_VAR function <a href="#calibre_link-203">205</a></p>
        <p>moving average process <a href="#calibre_link-6">61</a><a href="#calibre_link-62">&ndash;80</a></p>
        <p>out-of-sample forecasting <a href="#calibre_link-38">17</a></p>
        <p>defining <a href="#calibre_link-57">63&ndash;</a><a href="#calibre_link-248">68</a></p>
        <p>output gate <a href="#calibre_link-131">294</a><a href="#calibre_link-132">&ndash;295</a>
        </p>
        <p>forecasting <a href="#calibre_link-59">69&ndash;</a><a href="#calibre_link-60">78</a></p>
        <p>out_steps parameter <a href="#calibre_link-210">323</a></p>
        <p>identifying order of <a href="#calibre_link-58">64</a><a href="#calibre_link-248">&ndash;68</a></p>
        <p>MSE (mean squared error) <a href="#calibre_link-35">11</a><a href="#calibre_link-265">,
                51</a><a href="#calibre_link-266">, 74</a><a href="#calibre_link-268">, 93</a><a
                href="#calibre_link-240">, 261</a>, P</p>
        <p><a href="#calibre_link-118">271, </a><a href="#calibre_link-156">342, </a><a href="#calibre_link-178">400</a>
        </p>
        <p>multi-output model</p>
        <p>PACF (partial autocorrelation function) <a href="#calibre_link-66">89&ndash;</a><a
                href="#calibre_link-67">92</a></p>
        <p>deep learning <a href="#calibre_link-115">266&ndash;</a><a href="#calibre_link-116">268</a>
        </p>
        <p>padding <a href="#calibre_link-277">308</a></p>
        <p>implementing <a href="#calibre_link-121">275</a><a href="#calibre_link-122">&ndash;276</a>
        </p>
        <p>pandas library <a href="#calibre_link-38">17, </a><a href="#calibre_link-278">53, </a><a
                href="#calibre_link-186">418</a></p>
        <p>implementing CNNs as <a href="#calibre_link-141">315</a><a href="#calibre_link-142">&ndash;317</a></p>
        <p>pandas.Timedelta class <a href="#calibre_link-234">375</a></p>
        <p>implementing DNNs as <a href="#calibre_link-125">282&ndash;</a><a href="#calibre_link-223">283</a></p>
        <p>patience parameter <a href="#calibre_link-229">273</a></p>
        <p>implementing LSTM model as <a href="#calibre_link-134">299&ndash;</a><a href="#calibre_link-135">302</a>
        </p>
        <p>performance_metrics function <a href="#calibre_link-279">377</a></p>
        <p>multiple time series <a href="#calibre_link-12">197&ndash;</a><a href="#calibre_link-99">215</a></p>
        <p>plot_acf function <a href="#calibre_link-248">68</a><a href="#calibre_link-280">, 88</a><a
                href="#calibre_link-281">, 109</a></p>
        <p>forecasting real disposable income and real </p>
        <p>plot_components method <a href="#calibre_link-282">371</a></p>
        <p>consumption <a href="#calibre_link-97">203&ndash;</a><a href="#calibre_link-227">213</a></p>
        <p>plot_diagnostics function <a href="#calibre_link-245">207</a></p>
        <p>vector autoregression model <a href="#calibre_link-283">199&ndash;</a><a href="#calibre_link-96">201</a>
        </p>
        <p>plot_diagnostics method <a href="#calibre_link-193">123</a><a href="#calibre_link-252">,
                150</a><a href="#calibre_link-104">, 224</a></p>
        <p>designing modeling procedure for <a href="#calibre_link-96">201</a><a href="#calibre_link-97">&ndash;203</a>
        </p>
        <p>plot method <a href="#calibre_link-221">262, </a><a href="#calibre_link-229">273, </a><a
                href="#calibre_link-230">296, </a><a href="#calibre_link-232">311, </a><a href="#calibre_link-284">325,
            </a><a href="#calibre_link-285">347, </a><a href="#calibre_link-169">370</a></p>
        <p>Granger causality test <a href="#calibre_link-96">201</a><a href="#calibre_link-97">&ndash;203</a></p>
        <p>plot_pacf function <a href="#calibre_link-286">91</a><a href="#calibre_link-287">, 110</a>
        </p>
        <p>MultiStepLastBaseline class <a href="#calibre_link-114">263</a></p>
        <p>plot_weekly method <a href="#calibre_link-288">372</a></p>
        <p>multi-step model <a href="#calibre_link-114">263&ndash;</a><a href="#calibre_link-115">266</a></p>
        <p>plot_yearly method <a href="#calibre_link-288">372</a></p>
        <p>implementing <a href="#calibre_link-120">274</a><a href="#calibre_link-121">&ndash;275</a>
        </p>
        <p>Pmdarima website <a href="#calibre_link-166">362</a></p>
        <p>implementing CNNs as <a href="#calibre_link-140">314</a><a href="#calibre_link-141">&ndash;315</a></p>
        <p>pop variable <a href="#calibre_link-242">184</a></p>
        <p>implementing DNNs as <a href="#calibre_link-124">281&ndash;</a><a href="#calibre_link-125">282</a></p>
        <p>predict method <a href="#calibre_link-244">367</a></p>
        <p><a id="calibre_link-318"></a>INDEX</p>
        <p><b>425</b></p>
        <p>product function <a href="#calibre_link-75">114</a></p>
        <p>RNN layer <a href="#calibre_link-210">323</a></p>
        <p>Prophet <a href="#calibre_link-167">363&ndash;</a><a href="#calibre_link-168">365</a></p>
        <p>RNNs (recurrent neural networks) <a href="#calibre_link-246">288&ndash;</a><a href="#calibre_link-128">290,
            </a></p>
        <p>advanced functionality <a href="#calibre_link-169">370</a><a href="#calibre_link-172">&ndash;381</a></p>
        <p><a href="#calibre_link-19">305</a></p>
        <p>cross-validation and performance </p>
        <p>rolling_forecast function <a href="#calibre_link-250">72, </a><a href="#calibre_link-79">132, </a><a
                href="#calibre_link-289">192, </a><a href="#calibre_link-290">210,
            </a><a href="#calibre_link-105">225</a></p>
        <p>metrics <a href="#calibre_link-170">374</a><a href="#calibre_link-291">&ndash;378</a></p>
        <p>rolling forecasts <a href="#calibre_link-217">71</a></p>
        <p>hyperparameter tuning <a href="#calibre_link-171">379</a><a href="#calibre_link-172">&ndash;381</a></p>
        <p>rolling_window parameter <a href="#calibre_link-279">377</a></p>
        <p>visualization capabilities <a href="#calibre_link-169">370</a><a href="#calibre_link-170">&ndash;374</a>
        </p>
        <p>basic forecasting with <a href="#calibre_link-168">365&ndash;</a><a href="#calibre_link-209">369</a></p>
        <p>S</p>
        <p>implementing robust forecasting process</p>
        <p><a href="#calibre_link-172">381&ndash;</a><a href="#calibre_link-174">393</a></p>
        <p>SARIMA(p,d,q)(P,D,Q)m (seasonal autoregres-</p>
        <p>comparing to SARIMA <a href="#calibre_link-173">389</a><a href="#calibre_link-174">&ndash;393</a></p>
        <p>sive integrated moving average) model <a href="#calibre_link-10">156</a></p>
        <p>predicting popularity searches on </p>
        <p>SARIMA (seasonal autoregressive integrated </p>
        <p>Google <a href="#calibre_link-172">381</a><a href="#calibre_link-292">&ndash;388</a></p>
        <p>moving average) model</p>
        <p>installing <a href="#calibre_link-208">419</a></p>
        <p>compared to ARIMA <a href="#calibre_link-90">176</a><a href="#calibre_link-91">&ndash;178</a></p>
        <p>monthly average retail price forecast <a href="#calibre_link-178">400</a><a
                href="#calibre_link-270">&ndash;403</a></p>
        <p>examining <a href="#calibre_link-196">157</a><a href="#calibre_link-86">&ndash;160</a></p>
        <p>website <a href="#calibre_link-166">362</a></p>
        <p>forecasting with <a href="#calibre_link-89">171&ndash;</a><a href="#calibre_link-293">175</a></p>
        <p>Prophet class <a href="#calibre_link-244">367</a></p>
        <p>monthly average retail price forecast</p>
        <p>Python <a href="#calibre_link-186">418</a></p>
        <p><a href="#calibre_link-179">404&ndash;</a><a href="#calibre_link-180">409</a></p>
        <p>PyTorch Forecasting website <a href="#calibre_link-166">362</a></p>
        <p>Prophet vs. <a href="#calibre_link-173">389&ndash;</a><a href="#calibre_link-174">393</a>
        </p>
        <p>SARIMAX function <a href="#calibre_link-217">71</a><a href="#calibre_link-271">, 149</a><a
                href="#calibre_link-294">, 182</a></p>
        <p>Q</p>
        <p>SARIMAX model <a href="#calibre_link-11">180</a><a href="#calibre_link-294">,
                182&ndash;</a><a href="#calibre_link-94">186</a></p>
        <p>caveat for <a href="#calibre_link-93">185</a><a href="#calibre_link-94">&ndash;186</a></p>
        <p>qqplot function <a href="#calibre_link-77">121</a></p>
        <p>exogenous variables <a href="#calibre_link-92">183</a><a href="#calibre_link-242">&ndash;184</a></p>
        <p>Q-Q (quantile-quantile) plot <a href="#calibre_link-73">111</a>, <a href="#calibre_link-295">118&ndash;</a><a
                href="#calibre_link-251">120, </a><a href="#calibre_link-181">411</a></p>
        <p>forecasting real GDP using <a href="#calibre_link-94">186</a><a href="#calibre_link-95">&ndash;195</a>
        </p>
        <p>seasonality <a href="#calibre_link-10">156&ndash;</a><a href="#calibre_link-296">179</a></p>
        <p>R</p>
        <p>forecasting number of monthly air </p>
        <p>passengers <a href="#calibre_link-87">163</a><a href="#calibre_link-91">&ndash;178</a></p>
        <p>random.seed method <a href="#calibre_link-235">33</a></p>
        <p>comparing performance of methods</p>
        <p>random walk process <a href="#calibre_link-4">30</a><a href="#calibre_link-297">&ndash;58</a></p>
        <p><a href="#calibre_link-90">176&ndash;</a><a href="#calibre_link-91">178</a></p>
        <p>forecasting random walks <a href="#calibre_link-51">47&ndash;</a><a href="#calibre_link-298">54</a></p>
        <p>forecasting with an ARIMA model</p>
        <p>next timestep <a href="#calibre_link-53">52&ndash;</a><a href="#calibre_link-298">54</a></p>
        <p><a href="#calibre_link-88">165&ndash;</a><a href="#calibre_link-89">171</a></p>
        <p>on long horizon <a href="#calibre_link-52">48</a><a href="#calibre_link-53">&ndash;52</a>
        </p>
        <p>forecasting with SARIMA model <a href="#calibre_link-89">171&ndash;</a><a href="#calibre_link-293">175</a>
        </p>
        <p>identifying random walks <a href="#calibre_link-45">35&ndash;</a><a href="#calibre_link-51">47</a></p>
        <p>identifying seasonal patterns in time series</p>
        <p>autocorrelation function <a href="#calibre_link-48">41</a><a href="#calibre_link-49">&ndash;42</a></p>
        <p><a href="#calibre_link-86">160&ndash;</a><a href="#calibre_link-87">163</a></p>
        <p>stationarity <a href="#calibre_link-46">36</a><a href="#calibre_link-47">&ndash;38</a></p>
        <p>naive seasonal forecast <a href="#calibre_link-42">26&ndash;</a><a href="#calibre_link-43">28</a></p>
        <p>testing for stationarity <a href="#calibre_link-47">38</a><a href="#calibre_link-48">&ndash;41</a></p>
        <p>SARIMA model <a href="#calibre_link-196">157&ndash;</a><a href="#calibre_link-86">160</a>
        </p>
        <p>introduction to <a href="#calibre_link-299">31&ndash;</a><a href="#calibre_link-45">35</a>
        </p>
        <p>seasonality_mode parameter <a href="#calibre_link-171">379</a></p>
        <p>simulating <a href="#calibre_link-44">32</a><a href="#calibre_link-45">&ndash;35</a></p>
        <p>seasonality_prior_scale parameter <a href="#calibre_link-171">379</a></p>
        <p>read_csv method <a href="#calibre_link-38">17, </a><a href="#calibre_link-50">45, </a><a
                href="#calibre_link-150">333</a></p>
        <p>second-order differencing <a href="#calibre_link-243">37</a></p>
        <p>realcons variable <a href="#calibre_link-92">183</a></p>
        <p>self parameter <a href="#calibre_link-210">323</a></p>
        <p>realdpi variable <a href="#calibre_link-92">183</a></p>
        <p>Sequential model <a href="#calibre_link-229">273</a><a href="#calibre_link-230">, 296</a><a
                href="#calibre_link-232">, 311</a></p>
        <p>realgdp variable <a href="#calibre_link-92">183</a></p>
        <p>shift method <a href="#calibre_link-278">53</a></p>
        <p>realgovt variable <a href="#calibre_link-92">183</a></p>
        <p>shuffling <a href="#calibre_link-219">251</a></p>
        <p>realint variable <a href="#calibre_link-242">184</a></p>
        <p>single-step model</p>
        <p>realinv variable <a href="#calibre_link-92">183</a></p>
        <p>deep learning <a href="#calibre_link-113">260</a>&ndash;<a href="#calibre_link-221">262</a>
        </p>
        <p>recursive_forecast function <a href="#calibre_link-300">193</a></p>
        <p>implementing <a href="#calibre_link-119">272&ndash;</a><a href="#calibre_link-120">274</a>
        </p>
        <p>ReLU (Rectified Linear Unit) activation </p>
        <p>implementing CNNs as <a href="#calibre_link-139">310&ndash;</a><a href="#calibre_link-222">313</a></p>
        <p>function <a href="#calibre_link-123">278</a></p>
        <p>implementing DNNs as <a href="#calibre_link-123">278&ndash;</a><a href="#calibre_link-225">280</a></p>
        <p>RepeatBaseline class <a href="#calibre_link-249">264</a></p>
        <p>implementing LSTM model as <a href="#calibre_link-132">295</a><a href="#calibre_link-133">&ndash;297</a>
        </p>
        <p>resid property <a href="#calibre_link-77">121</a></p>
        <p>single-step models <a href="#calibre_link-107">234</a></p>
        <p>residual analysis <a href="#calibre_link-76">116</a><a href="#calibre_link-77">&ndash;121</a></p>
        <p>sklearn API <a href="#calibre_link-244">367</a></p>
        <p>antidiabetic drug prescriptions forecast <a href="#calibre_link-104">224</a></p>
        <p>sklearn library <a href="#calibre_link-265">51, </a><a href="#calibre_link-267">96</a></p>
        <p>Ljung-Box test <a href="#calibre_link-251">120</a><a href="#calibre_link-77">&ndash;121</a>
        </p>
        <p>sklearn package <a href="#calibre_link-266">74</a></p>
        <p>performing <a href="#calibre_link-77">121</a><a href="#calibre_link-226">&ndash;124</a></p>
        <p>split_to_inputs_labels function <a href="#calibre_link-301">255</a></p>
        <p>quantile-quantile plot <a href="#calibre_link-295">118&ndash;</a><a href="#calibre_link-251">120</a></p>
        <p>standard_normal method <a href="#calibre_link-235">33</a></p>
        <p><a id="calibre_link-319"></a><b>426</b></p>
        <p>INDEX</p>
        <p>stationarity</p>
        <p>sources of time series data <a href="#calibre_link-185">416</a><a href="#calibre_link-302">&ndash;417</a>
        </p>
        <p>autoregressive moving average process <a href="#calibre_link-72">106</a><a
                href="#calibre_link-73">&ndash;111</a></p>
        <p>statistical methods for <a href="#calibre_link-181">411</a><a href="#calibre_link-182">&ndash;412</a></p>
        <p>finding order of stationary autoregressive </p>
        <p>steps in <a href="#calibre_link-32">8&ndash;</a><a href="#calibre_link-35">11</a></p>
        <p>process <a href="#calibre_link-65">85</a><a href="#calibre_link-67">&ndash;92</a></p>
        <p>collecting new data <a href="#calibre_link-35">11</a></p>
        <p>introduction to <a href="#calibre_link-46">36</a><a href="#calibre_link-47">&ndash;38</a>
        </p>
        <p>deploying to production <a href="#calibre_link-35">11</a></p>
        <p>testing for <a href="#calibre_link-47">38&ndash;</a><a href="#calibre_link-48">41</a></p>
        <p>determining what must be forecast to achieve </p>
        <p>stationary time series <a href="#calibre_link-46">36</a></p>
        <p>goals <a href="#calibre_link-33">9</a></p>
        <p>statsmodels library <a href="#calibre_link-49">42</a><a href="#calibre_link-204">, 67</a><a
                href="#calibre_link-286">, 91</a><a href="#calibre_link-200">, 108</a><a href="#calibre_link-303">,
                161</a><a href="#calibre_link-92">, 183</a><a href="#calibre_link-97">, 203</a>, developing
            forecasting
            model <a href="#calibre_link-34">10&ndash;</a><a href="#calibre_link-35">11</a></p>
        <p><a href="#calibre_link-245">207, </a><a href="#calibre_link-186">418</a></p>
        <p>gathering data <a href="#calibre_link-34">10</a></p>
        <p>STL function <a href="#calibre_link-303">161</a></p>
        <p>monitoring <a href="#calibre_link-35">11</a></p>
        <p>strides <a href="#calibre_link-189">307</a></p>
        <p>setting goals <a href="#calibre_link-33">9</a></p>
        <p>sum() method <a href="#calibre_link-151">334</a></p>
        <p>setting horizon of forecast <a href="#calibre_link-34">10</a></p>
        <p>summary method <a href="#calibre_link-93">185</a></p>
        <p>timestamp method <a href="#calibre_link-212">242</a></p>
        <p>to_numeric function <a href="#calibre_link-152">335</a></p>
        <p>T</p>
        <p>train_len parameter <a href="#calibre_link-250">72</a></p>
        <p>transpose method <a href="#calibre_link-220">324</a></p>
        <p>tanh (hyperbolic tangent) activation function <a href="#calibre_link-130">292</a></p>
        <p>tbilrate variable <a href="#calibre_link-242">184</a><a href="#calibre_link-283">, 199</a>
        </p>
        <p>U</p>
        <p>time series <a href="#calibre_link-304">4</a></p>
        <p>timeseries_dataset_from_array function <a href="#calibre_link-305">256</a></p>
        <p>unemp variable <a href="#calibre_link-242">184</a></p>
        <p>time series decomposition <a href="#calibre_link-303">161</a></p>
        <p>units parameter <a href="#calibre_link-232">311, </a><a href="#calibre_link-210">323</a></p>
        <p>time series forecasting <a href="#calibre_link-2">3</a><a href="#calibre_link-37">&ndash;13</a><a
                href="#calibre_link-25">, 410</a><a href="#calibre_link-302">&ndash;417</a></p>
        <p>automating <a href="#calibre_link-183">413</a></p>
        <p>V</p>
        <p>automating with Prophet <a href="#calibre_link-23">361&ndash;</a><a href="#calibre_link-207">395</a></p>
        <p>compared to other regression tasks <a href="#calibre_link-36">12</a><a href="#calibre_link-37">&ndash;13</a>
        </p>
        <p>VARMAX function <a href="#calibre_link-98">214</a></p>
        <p>time series have order <a href="#calibre_link-36">12</a></p>
        <p>VAR (vector autoregression) model <a href="#calibre_link-283">199</a><a
                href="#calibre_link-96">&ndash;201</a></p>
        <p>time series sometimes do not have features <a href="#calibre_link-37">13</a></p>
        <p>designing modeling procedure for <a href="#calibre_link-96">201&ndash;</a><a href="#calibre_link-97">203</a>
        </p>
        <p>complex <a href="#calibre_link-8">101&ndash;</a><a href="#calibre_link-224">139</a></p>
        <p>Granger causality test <a href="#calibre_link-96">201&ndash;</a><a href="#calibre_link-97">203</a></p>
        <p>components of <a href="#calibre_link-31">5</a><a href="#calibre_link-32">&ndash;8</a></p>
        <p>components of time series <a href="#calibre_link-31">5&ndash;</a><a href="#calibre_link-32">8</a></p>
        <p>W</p>
        <p>deep learning for <a href="#calibre_link-15">233&ndash;</a><a href="#calibre_link-306">247,
            </a><a href="#calibre_link-182">412&ndash;</a><a href="#calibre_link-183">413</a></p>
        <p>features <a href="#calibre_link-37">13</a></p>
        <p>warmup function <a href="#calibre_link-210">323</a></p>
        <p>filtering with convolutional neural </p>
        <p>white noise <a href="#calibre_link-44">32</a></p>
        <p>networks <a href="#calibre_link-19">305&ndash;</a><a href="#calibre_link-307">319</a></p>
        <p>widget_sales_diff variable <a href="#calibre_link-204">67</a></p>
        <p>if forecasting doesn't work <a href="#calibre_link-183">413</a><a href="#calibre_link-184">&ndash;415</a>
        </p>
        <p>window parameter <a href="#calibre_link-250">72</a></p>
        <p>multiple time series <a href="#calibre_link-12">197</a><a href="#calibre_link-99">&ndash;215</a></p>
        <p>non-stationary time series <a href="#calibre_link-9">140&ndash;</a><a href="#calibre_link-269">155</a>
        </p>
        <p>Y</p>
        <p>order <a href="#calibre_link-36">12</a></p>
        <p>other applications of time series data <a href="#calibre_link-184">415</a><a
                href="#calibre_link-185">&ndash;416</a></p>
        <p>y_pred vector <a href="#calibre_link-262">20</a></p>
        <p>overview <a href="#calibre_link-304">4</a><a href="#calibre_link-32">&ndash;8</a></p>
        <p>y_true vector <a href="#calibre_link-262">20</a></p>
        <p><a id="calibre_link-455"></a><img src="images/000101.png" alt="Image 164" class="calibre2" /></p>
        <p><img src="images/000037.png" alt="Image 165" class="calibre2" /></p>
        <p><img src="images/000004.png" alt="Image 166" class="calibre2" /></p>
        <p><img src="images/000044.png" alt="Image 167" class="calibre2" /></p>
        <p>RELATED MANNING TITLES</p>
        <p> <i>Deep Learning with Python, Second Edition</i></p>
        <p>by François Chollet</p>
        <p>ISBN 9781617296864</p>
        <p>504 pages, $59.99 </p>
        <p>October 2021</p>
        <p> <i>Math and Architectures of Deep Learning</i></p>
        <p>by Krishnendu Chaudhury</p>
        <p>ISBN 9781617296482</p>
        <p>535 pages (estimated), $69.99 </p>
        <p>Early 2023 (estimated)</p>
        <p> <i>For ordering information, go to www.manning.com</i></p>
        <p><a id="calibre_link-456"></a><img src="images/000001.png" alt="Image 168" class="calibre2" /></p>
        <p><img src="images/000035.png" alt="Image 169" class="calibre2" /></p>
        <p><img src="images/000155.png" alt="Image 170" class="calibre2" /></p>
        <p><img src="images/000132.png" alt="Image 171" class="calibre2" /></p>
        <p>RELATED MANNING TITLES</p>
        <p> <i>Inside Deep Learning</i></p>
        <p>by Edward Raff </p>
        <p>Foreword by Kirk Borne</p>
        <p>ISBN 9781617298639</p>
        <p>600 pages, $59.99 </p>
        <p>April 2022</p>
        <p> <i>Deep Learning Patterns and Practices</i></p>
        <p>by Andrew Ferlitsch</p>
        <p>ISBN 9781617298264</p>
        <p>472 pages, $59.99 </p>
        <p>August 2021</p>
        <p> <i>For ordering information, go to www.manning.com</i></p>
        <p><a id="calibre_link-457"></a> <i><b>Core concepts for time
                    series forecasting (continued from inside front cover)</b></i></p>
        <p>Core concept</p>
        <p>Chapter</p>
        <p>Section</p>
        <p>SARIMA model</p>
        <p>8</p>
        <p>8.1</p>
        <p>Frequency of seasonality</p>
        <p>8</p>
        <p>8.1</p>
        <p>Time series decomposition</p>
        <p>8</p>
        <p>8.2</p>
        <p>Forecasting with SARIMA</p>
        <p>8</p>
        <p>8.3</p>
        <p>SARIMAX model</p>
        <p>9</p>
        <p>9.1</p>
        <p>Caveat of SARIMAX</p>
        <p>9</p>
        <p>9.1.2</p>
        <p>Forecasting with SARIMAX</p>
        <p>9</p>
        <p>9.2</p>
        <p>Vector autoregression model (VAR)</p>
        <p>10</p>
        <p>10.1</p>
        <p>Granger causality test</p>
        <p>10</p>
        <p>10.2.1</p>
        <p>Forecasting with VAR</p>
        <p>10</p>
        <p>10.3</p>
        <p>Types of deep learning models</p>
        <p>12</p>
        <p>12.2</p>
        <p>Data windowing</p>
        <p>13</p>
        <p>13.1</p>
        <p>Deep neural network</p>
        <p>14</p>
        <p>14.2</p>
        <p>Long short-term memory (LSTM)</p>
        <p>15</p>
        <p>15.2</p>
        <p>Convolutional neural network (CNN)</p>
        <p>16</p>
        <p>16.1</p>
        <p>Autoregressive LSTM</p>
        <p>17</p>
        <p>17.1</p>
        <p>Working with Prophet</p>
        <p>19</p>
        <p>19.2</p>
        <p><a id="calibre_link-458"></a><img src="images/000105.png" alt="Image 172" class="calibre2" /></p>
        <p><img src="images/000115.jpg" alt="Image 173" class="calibre2" /></p>
        <p><img src="images/000164.png" alt="Image 174" class="calibre2" /></p>
        <p>DATA SCIENCE / MACHINE LEARNING</p>
        <p>Time Series Forecasting in Python</p>
        <p>“Th e importance of time </p>
        <p>series analysis cannot be </p>
        <p>overstated. Th</p>
        <p>is book provides </p>
        <p>Marco Peixeiro</p>
        <p>key techniques to deal with </p>
        <p>You can predict the future&mdash;with a little help from </p>
        <p>time series data in real-world </p>
        <p>Python, deep learning, and time series data! Time series </p>
        <p>forecasting is a technique for modeling time-centric data </p>
        <p>applications. Indispensable. </p>
        <p>&mdash;Amar</p>
        <p>” </p>
        <p>to identify upcoming events. New Python libraries and power-</p>
        <p>esh Rajasekharan, IBM </p>
        <p>ful deep learning tools make accurate time series forecasts </p>
        <p>easier than ever before. </p>
        <p>“Marco Peixeiro presents </p>
        <p>concepts clearly using </p>
        <p>Time Series Forecasting in Python teaches you how to get </p>
        <p>interesting examples and </p>
        <p>immediate, meaningful predictions from time-based data </p>
        <p>illustrative plots. You'll be up </p>
        <p>such as logs, customer analytics, and other event streams. In </p>
        <p>and running quickly using </p>
        <p>this accessible book, you'll learn statistical and deep learning </p>
        <p>methods for time series forecasting, fully demonstrated with </p>
        <p>the power of Python. </p>
        <p>&mdash;Ariel Andres</p>
        <p>” </p>
        <p>annotated Python code. Develop your skills with projects like </p>
        <p>predicting the future volume of drug prescriptions, and you'll </p>
        <p>MD Financial Management </p>
        <p>soon be ready to build your own accurate, insightful forecasts. </p>
        <p>“What caught my attention </p>
        <p>What's Inside</p>
        <p>were the practical examples </p>
        <p>immediately applicable to </p>
        <p>● Create models for seasonal eff ects and external variables</p>
        <p>real life. He explains complex </p>
        <p>● Multivariate forecasting models to predict multiple </p>
        <p>topics without the excess of </p>
        <p>time series</p>
        <p>mathematical formalism. </p>
        <p>● Deep learning for large datasets</p>
        <p>&mdash;Simone Sguazza</p>
        <p>” </p>
        <p>● Automate the forecasting process</p>
        <p>University of Applied Sciences and </p>
        <p>Arts of Southern Switzerland</p>
        <p>For data scientists familiar with Python and TensorFlow. </p>
        <p>Marco Peixeiro is a seasoned data science instructor who has </p>
        <p>worked as a data scientist for one of Canada's largest banks. </p>
        <p>See first page</p>
        <p>Register this print book to get free access to all ebook formats. </p>
        <p>Visit https://www.manning.com/freebook</p>
        <p><b>ISBN-13: 978-1-61729-988-9</b></p>
        <p>M A N N I N G</p>
        <p><a name="outline"></a></p>
        <h1 class="calibre5">Document Outline</h1>
        <ul class="calibre6">
            <li class="calibre7"><a href="#calibre_link-308">Time Series Forecasting in Python</a></li>
            <li class="calibre7"><a href="#calibre_link-309">brief contents</a></li>
            <li class="calibre7"><a href="#calibre_link-310">contents</a></li>
            <li class="calibre7"><a href="#calibre_link-26">preface</a></li>
            <li class="calibre7"><a href="#calibre_link-27">acknowledgments</a></li>
            <li class="calibre7"><a href="#calibre_link-28">about this book</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-28">Who should read this book? </a></li>
                    <li class="calibre7"><a href="#calibre_link-311">How this book is organized: A roadmap</a></li>
                    <li class="calibre7"><a href="#calibre_link-312">About the code</a></li>
                    <li class="calibre7"><a href="#calibre_link-312">liveBook discussion forum</a></li>
                    <li class="calibre7"><a href="#calibre_link-313">Author online</a></li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-29">about the author</a></li>
            <li class="calibre7"><a href="#calibre_link-30">about the cover illustration</a></li>
            <li class="calibre7"><a href="#calibre_link-1">Part 1&mdash;Time waits for no one</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-2">1 Understanding time series forecasting</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-304">1.1 Introducing time series</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-31">1.1.1 Components of a time
                                            series</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-32">1.2 Bird's-eye view of time series
                                    forecasting</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-33">1.2.1 Setting a goal</a></li>
                                    <li class="calibre7"><a href="#calibre_link-33">1.2.2 Determining what must be
                                            forecast to achieve your goal</a></li>
                                    <li class="calibre7"><a href="#calibre_link-34">1.2.3 Setting the horizon of the
                                            forecast</a></li>
                                    <li class="calibre7"><a href="#calibre_link-34">1.2.4 Gathering the data</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-34">1.2.5 Developing a forecasting
                                            model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-35">1.2.6 Deploying to
                                            production</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-35">1.2.7 Monitoring</a></li>
                                    <li class="calibre7"><a href="#calibre_link-35">1.2.8 Collecting new data</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-36">1.3 How time series forecasting is
                                    different
                                    from other regression tasks</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-36">1.3.1 Time series have an
                                            order</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-37">1.3.2 Time series sometimes do
                                            not
                                            have features</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-37">1.4 Next steps</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-3">2 A naive prediction of the future</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-216">2.1 Defining a baseline model</a></li>
                            <li class="calibre7"><a href="#calibre_link-38">2.2 Forecasting the historical mean</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-38">2.2.1 Setup for baseline
                                            implementations</a></li>
                                    <li class="calibre7"><a href="#calibre_link-39">2.2.2 Implementing the
                                            historical
                                            mean baseline</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-40">2.3 Forecasting last year's mean</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-41">2.4 Predicting using the last known
                                    value</a></li>
                            <li class="calibre7"><a href="#calibre_link-42">2.5 Implementing the naive seasonal
                                    forecast</a></li>
                            <li class="calibre7"><a href="#calibre_link-43">2.6 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-261">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-4">3 Going on a random walk</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-299">3.1 The random walk process</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-44">3.1.1 Simulating a random walk
                                            process</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-45">3.2 Identifying a random walk</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-46">3.2.1 Stationarity</a></li>
                                    <li class="calibre7"><a href="#calibre_link-47">3.2.2 Testing for
                                            stationarity</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-48">3.2.3 The autocorrelation
                                            function</a></li>
                                    <li class="calibre7"><a href="#calibre_link-49">3.2.4 Putting it all
                                            together</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-50">3.2.5 Is GOOGL a random walk?
                                        </a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-51">3.3 Forecasting a random walk</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-52">3.3.1 Forecasting on a long
                                            horizon</a></li>
                                    <li class="calibre7"><a href="#calibre_link-53">3.3.2 Forecasting the next
                                            timestep</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-54">3.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-55">3.5 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-55">3.5.1 Simulate and forecast a
                                            random
                                            walk</a></li>
                                    <li class="calibre7"><a href="#calibre_link-56">3.5.2 Forecast the daily closing
                                            price of GOOGL</a></li>
                                    <li class="calibre7"><a href="#calibre_link-56">3.5.3 Forecast the daily closing
                                            price of a stock of your choice</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-56">Summary</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-5">Part 2&mdash;Forecasting with statistical models</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-6">4 Modeling a moving average process</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-57">4.1 Defining a moving average
                                    process</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-58">4.1.1 Identifying the order of a
                                            moving average process</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-59">4.2 Forecasting a moving average
                                    process</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-60">4.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-61">4.4 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-61">4.4.1 Simulate an MA(2) process
                                            and
                                            make forecasts</a></li>
                                    <li class="calibre7"><a href="#calibre_link-62">4.4.2 Simulate an MA(q) process
                                            and
                                            make forecasts</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-62">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-7">5 Modeling an autoregressive process</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-63">5.1 Predicting the average weekly foot
                                    traffic in a retail store</a></li>
                            <li class="calibre7"><a href="#calibre_link-64">5.2 Defining the autoregressive
                                    process</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-65">5.3 Finding the order of a stationary
                                    autoregressive process</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-66">5.3.1 The partial
                                            autocorrelation
                                            function (PACF)</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-67">5.4 Forecasting an autoregressive
                                    process</a></li>
                            <li class="calibre7"><a href="#calibre_link-68">5.5 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-69">5.6 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-69">5.6.1 Simulate an AR(2) process
                                            and
                                            make forecasts</a></li>
                                    <li class="calibre7"><a href="#calibre_link-70">5.6.2 Simulate an AR(p) process
                                            and
                                            make forecasts</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-70">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-8">6 Modeling complex time series</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-214">6.1 Forecasting bandwidth usage for
                                    data
                                    centers</a></li>
                            <li class="calibre7"><a href="#calibre_link-71">6.2 Examining the autoregressive moving
                                    average process</a></li>
                            <li class="calibre7"><a href="#calibre_link-72">6.3 Identifying a stationary ARMA
                                    process</a></li>
                            <li class="calibre7"><a href="#calibre_link-73">6.4 Devising a general modeling
                                    procedure</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-74">6.4.1 Understanding the Akaike
                                            information criterion (AIC)</a></li>
                                    <li class="calibre7"><a href="#calibre_link-75">6.4.2 Selecting a model using
                                            the
                                            AIC</a></li>
                                    <li class="calibre7"><a href="#calibre_link-76">6.4.3 Understanding residual
                                            analysis</a></li>
                                    <li class="calibre7"><a href="#calibre_link-77">6.4.4 Performing residual
                                            analysis</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-78">6.5 Applying the general modeling
                                    procedure</a></li>
                            <li class="calibre7"><a href="#calibre_link-79">6.6 Forecasting bandwidth usage</a></li>
                            <li class="calibre7"><a href="#calibre_link-80">6.7 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-81">6.8 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-81">6.8.1 Make predictions on the
                                            simulated ARMA(1,1) process</a></li>
                                    <li class="calibre7"><a href="#calibre_link-81">6.8.2 Simulate an ARMA(2,2)
                                            process
                                            and make forecasts</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-314">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-9">7 Forecasting non-stationary time series</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-82">7.1 Defining the autoregressive
                                    integrated
                                    moving average model</a></li>
                            <li class="calibre7"><a href="#calibre_link-83">7.2 Modifying the general modeling
                                    procedure
                                    to account for non-stationary series</a></li>
                            <li class="calibre7"><a href="#calibre_link-84">7.3 Forecasting a non-stationary times
                                    series</a></li>
                            <li class="calibre7"><a href="#calibre_link-85">7.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-85">7.5 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-85">7.5.1 Apply the ARIMA(p,d,q)
                                            model
                                            on the datasets from chapters 4, 5, and 6</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-269">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-10">8 Accounting for seasonality</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-196">8.1 Examining the SARIMA(p,d,q)(P,D,Q)m
                                    model</a></li>
                            <li class="calibre7"><a href="#calibre_link-86">8.2 Identifying seasonal patterns in a
                                    time
                                    series</a></li>
                            <li class="calibre7"><a href="#calibre_link-87">8.3 Forecasting the number of monthly
                                    air
                                    passengers</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-88">8.3.1 Forecasting with an
                                            ARIMA(p,d,q) model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-89">8.3.2 Forecasting with a
                                            SARIMA(p,d,q)(P,D,Q)m model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-90">8.3.3 Comparing the performance
                                            of
                                            each forecasting method</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-91">8.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-91">8.5 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-91">8.5.1 Apply the
                                            SARIMA(p,d,q)(P,D,Q)m model on the Johnson &amp; Johnson dataset</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-296">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-11">9 Adding external variables to our model</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-294">9.1 Examining the SARIMAX model</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-92">9.1.1 Exploring the exogenous
                                            variables of the US macroeconomics dataset</a></li>
                                    <li class="calibre7"><a href="#calibre_link-93">9.1.2 Caveat for using
                                            SARIMAX</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-94">9.2 Forecasting the real GDP using the
                                    SARIMAX model</a></li>
                            <li class="calibre7"><a href="#calibre_link-95">9.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-95">9.4 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-95">9.4.1 Use all exogenous
                                            variables in
                                            a SARIMAX model to predict the real GDP</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-241">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-12">10 Forecasting multiple time series</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-283">10.1 Examining the VAR model</a></li>
                            <li class="calibre7"><a href="#calibre_link-96">10.2 Designing a modeling procedure for
                                    the
                                    VAR(p) model</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-96">10.2.1 Exploring the Granger
                                            causality test</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-97">10.3 Forecasting real disposable income
                                    and
                                    real consumption</a></li>
                            <li class="calibre7"><a href="#calibre_link-98">10.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-98">10.5 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-98">10.5.1 Use a VARMA model to
                                            predict
                                            realdpi and realcons</a></li>
                                    <li class="calibre7"><a href="#calibre_link-99">10.5.2 Use a VARMAX model to
                                            predict
                                            realdpi and realcons</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-99">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-13">11 Capstone: Forecasting the number of
                            antidiabetic
                            drug prescriptions in Australia</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-100">11.1 Importing the required libraries
                                    and
                                    loading the data</a></li>
                            <li class="calibre7"><a href="#calibre_link-101">11.2 Visualizing the series and its
                                    components</a></li>
                            <li class="calibre7"><a href="#calibre_link-102">11.3 Modeling the data</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-103">11.3.1 Performing model
                                            selection</a></li>
                                    <li class="calibre7"><a href="#calibre_link-104">11.3.2 Conducting residual
                                            analysis</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-105">11.4 Forecasting and evaluating the
                                    model's
                                    performance</a></li>
                            <li class="calibre7"><a href="#calibre_link-106">Next steps</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-14">Part 3&mdash;Large-scale forecasting with deep
                    learning</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-15">12 Introducing deep learning for time series
                            forecasting</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-107">12.1 When to use deep learning for time
                                    series forecasting</a></li>
                            <li class="calibre7"><a href="#calibre_link-107">12.2 Exploring the different types of
                                    deep
                                    learning models</a></li>
                            <li class="calibre7"><a href="#calibre_link-108">12.3 Getting ready to apply deep
                                    learning
                                    for forecasting</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-108">12.3.1 Performing data
                                            exploration</a></li>
                                    <li class="calibre7"><a href="#calibre_link-109">12.3.2 Feature engineering and
                                            data
                                            splitting</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-110">12.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-110">12.5 Exercise</a></li>
                            <li class="calibre7"><a href="#calibre_link-306">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-16">13 Data windowing and creating baselines for
                            deep
                            learning</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-111">13.1 Creating windows of data</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-111">13.1.1 Exploring how deep
                                            learning
                                            models are trained for time series forecasting</a></li>
                                    <li class="calibre7"><a href="#calibre_link-112">13.1.2 Implementing the
                                            DataWindow
                                            class</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-113">13.2 Applying baseline models</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-113">13.2.1 Single-step baseline
                                            model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-114">13.2.2 Multi-step baseline
                                            models</a></li>
                                    <li class="calibre7"><a href="#calibre_link-115">13.2.3 Multi-output baseline
                                            model</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-116">13.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-117">13.4 Exercises</a></li>
                            <li class="calibre7"><a href="#calibre_link-117">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-17">14 Baby steps with deep learning</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-118">14.1 Implementing a linear model</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-119">14.1.1 Implementing a
                                            single-step
                                            linear model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-120">14.1.2 Implementing a
                                            multi-step
                                            linear model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-121">14.1.3 Implementing a
                                            multi-output
                                            linear model</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-122">14.2 Implementing a deep neural
                                    network</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-123">14.2.1 Implementing a deep
                                            neural
                                            network as a single-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-124">14.2.2 Implementing a deep
                                            neural
                                            network as a multi-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-125">14.2.3 Implementing a deep
                                            neural
                                            network as a multi-output model</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-126">14.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-127">14.4 Exercises</a></li>
                            <li class="calibre7"><a href="#calibre_link-127">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-18">15 Remembering the past with LSTM</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-246">15.1 Exploring the recurrent neural
                                    network
                                    (RNN)</a></li>
                            <li class="calibre7"><a href="#calibre_link-128">15.2 Examining the LSTM
                                    architecture</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-129">15.2.1 The forget gate</a></li>
                                    <li class="calibre7"><a href="#calibre_link-130">15.2.2 The input gate</a></li>
                                    <li class="calibre7"><a href="#calibre_link-131">15.2.3 The output gate</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-132">15.3 Implementing the LSTM
                                    architecture</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-132">15.3.1 Implementing an LSTM as
                                            a
                                            single-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-133">15.3.2 Implementing an LSTM as
                                            a
                                            multi-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-134">15.3.3 Implementing an LSTM as
                                            a
                                            multi-output model</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-135">15.4 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-136">15.5 Exercises</a></li>
                            <li class="calibre7"><a href="#calibre_link-257">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-19">16 Filtering a time series with CNN</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-137">16.1 Examining the convolutional neural
                                    network (CNN)</a></li>
                            <li class="calibre7"><a href="#calibre_link-138">16.2 Implementing a CNN</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-139">16.2.1 Implementing a CNN as a
                                            single-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-140">16.2.2 Implementing a CNN as a
                                            multi-step model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-141">16.2.3 Implementing a CNN as a
                                            multi-output model</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-142">16.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-143">16.4 Exercises</a></li>
                            <li class="calibre7"><a href="#calibre_link-307">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-20">17 Using predictions to make more
                            predictions</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-144">17.1 Examining the ARLSTM
                                    architecture</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-145">17.2 Building an autoregressive LSTM
                                    model</a></li>
                            <li class="calibre7"><a href="#calibre_link-146">17.3 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-147">17.4 Exercises</a></li>
                            <li class="calibre7"><a href="#calibre_link-147">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-21">18 Capstone: Forecasting the electric power
                            consumption of a household</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-148">18.1 Understanding the capstone
                                    project</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-149">18.1.1 Objective of this
                                            capstone
                                            project</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-150">18.2 Data wrangling and
                                    preprocessing</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-151">18.2.1 Dealing with missing
                                            data</a></li>
                                    <li class="calibre7"><a href="#calibre_link-152">18.2.2 Data conversion</a></li>
                                    <li class="calibre7"><a href="#calibre_link-152">18.2.3 Data resampling</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-153">18.3 Feature engineering</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-153">18.3.1 Removing unnecessary
                                            columns</a></li>
                                    <li class="calibre7"><a href="#calibre_link-154">18.3.2 Identifying the seasonal
                                            period</a></li>
                                    <li class="calibre7"><a href="#calibre_link-155">18.3.3 Splitting and scaling
                                            the
                                            data</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-156">18.4 Preparing for modeling with deep
                                    learning</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-156">18.4.1 Initial setup</a></li>
                                    <li class="calibre7"><a href="#calibre_link-157">18.4.2 Defining the DataWindow
                                            class</a></li>
                                    <li class="calibre7"><a href="#calibre_link-158">18.4.3 Utility function to
                                            train
                                            our models</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-158">18.5 Modeling with deep learning</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-158">18.5.1 Baseline models</a></li>
                                    <li class="calibre7"><a href="#calibre_link-159">18.5.2 Linear model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-160">18.5.3 Deep neural network</a>
                                    </li>
                                    <li class="calibre7"><a href="#calibre_link-161">18.5.4 Long short-term memory
                                            (LSTM) model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-161">18.5.5 Convolutional neural
                                            network
                                            (CNN)</a></li>
                                    <li class="calibre7"><a href="#calibre_link-162">18.5.6 Combining a CNN with an
                                            LSTM</a></li>
                                    <li class="calibre7"><a href="#calibre_link-163">18.5.7 The autoregressive LSTM
                                            model</a></li>
                                    <li class="calibre7"><a href="#calibre_link-164">18.5.8 Selecting the best
                                            model</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-165">18.6 Next steps</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-22">Part 4&mdash;Automating forecasting at scale</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-23">19 Automating time series forecasting with
                            Prophet</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-166">19.1 Overview of the automated
                                    forecasting
                                    libraries</a></li>
                            <li class="calibre7"><a href="#calibre_link-167">19.2 Exploring Prophet</a></li>
                            <li class="calibre7"><a href="#calibre_link-168">19.3 Basic forecasting with Prophet</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-169">19.4 Exploring Prophet's advanced
                                    functionality</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-169">19.4.1 Visualization
                                            capabilities</a></li>
                                    <li class="calibre7"><a href="#calibre_link-170">19.4.2 Cross-validation and
                                            performance metrics</a></li>
                                    <li class="calibre7"><a href="#calibre_link-171">19.4.3 Hyperparameter
                                            tuning</a>
                                    </li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-172">19.5 Implementing a robust forecasting
                                    process with Prophet</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-172">19.5.1 Forecasting project:
                                            Predicting the popularity of “chocolate” searches on Google</a></li>
                                    <li class="calibre7"><a href="#calibre_link-173">19.5.2 Experiment: Can SARIMA
                                            do
                                            better? </a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-174">19.6 Next steps</a></li>
                            <li class="calibre7"><a href="#calibre_link-175">19.7 Exercises</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-175">19.7.1 Forecast the number of
                                            air
                                            passengers</a></li>
                                    <li class="calibre7"><a href="#calibre_link-175">19.7.2 Forecast the volume of
                                            antidiabetic drug prescriptions</a></li>
                                    <li class="calibre7"><a href="#calibre_link-175">19.7.3 Forecast the popularity
                                            of a
                                            keyword on Google Trends</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-207">Summary</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-24">20 Capstone: Forecasting the monthly average
                            retail
                            price of steak in Canada</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-176">20.1 Understanding the capstone
                                    project</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-176">20.1.1 Objective of the
                                            capstone
                                            project</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-177">20.2 Data preprocessing and
                                    visualization</a></li>
                            <li class="calibre7"><a href="#calibre_link-178">20.3 Modeling with Prophet</a></li>
                            <li class="calibre7"><a href="#calibre_link-179">20.4 Optional: Develop a SARIMA
                                    model</a>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-180">20.5 Next steps</a></li>
                        </ul>
                    </li>
                    <li class="calibre7"><a href="#calibre_link-25">21 Going above and beyond</a>
                        <ul class="calibre9">
                            <li class="calibre7"><a href="#calibre_link-181">21.1 Summarizing what you've
                                    learned</a>
                                <ul class="calibre9">
                                    <li class="calibre7"><a href="#calibre_link-181">21.1.1 Statistical methods for
                                            forecasting</a></li>
                                    <li class="calibre7"><a href="#calibre_link-182">21.1.2 Deep learning methods
                                            for
                                            forecasting</a></li>
                                    <li class="calibre7"><a href="#calibre_link-183">21.1.3 Automating the
                                            forecasting
                                            process</a></li>
                                </ul>
                            </li>
                            <li class="calibre7"><a href="#calibre_link-183">21.2 What if forecasting does not work?
                                </a></li>
                            <li class="calibre7"><a href="#calibre_link-184">21.3 Other applications of time series
                                    data</a></li>
                            <li class="calibre7"><a href="#calibre_link-185">21.4 Keep practicing</a></li>
                        </ul>
                    </li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-186">Appendix&mdash;Installation instructions</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-186">Installing Anaconda</a></li>
                    <li class="calibre7"><a href="#calibre_link-186">Python</a></li>
                    <li class="calibre7"><a href="#calibre_link-186">Jupyter Notebooks</a></li>
                    <li class="calibre7"><a href="#calibre_link-208">GitHub Repository</a></li>
                    <li class="calibre7"><a href="#calibre_link-208">Installing Prophet</a></li>
                    <li class="calibre7"><a href="#calibre_link-208">Installing libraries in Anaconda</a></li>
                </ul>
            </li>
            <li class="calibre7"><a href="#calibre_link-187">index</a>
                <ul class="calibre8">
                    <li class="calibre7"><a href="#calibre_link-187">Symbols</a></li>
                    <li class="calibre7"><a href="#calibre_link-187">Numerics</a></li>
                    <li class="calibre7"><a href="#calibre_link-187">A</a></li>
                    <li class="calibre7"><a href="#calibre_link-315">B</a></li>
                    <li class="calibre7"><a href="#calibre_link-315">C</a></li>
                    <li class="calibre7"><a href="#calibre_link-315">D</a></li>
                    <li class="calibre7"><a href="#calibre_link-315">E</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">F</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">G</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">H</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">I</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">J</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">K</a></li>
                    <li class="calibre7"><a href="#calibre_link-316">L</a></li>
                    <li class="calibre7"><a href="#calibre_link-317">M</a></li>
                    <li class="calibre7"><a href="#calibre_link-317">N</a></li>
                    <li class="calibre7"><a href="#calibre_link-317">O</a></li>
                    <li class="calibre7"><a href="#calibre_link-317">P</a></li>
                    <li class="calibre7"><a href="#calibre_link-318">Q</a></li>
                    <li class="calibre7"><a href="#calibre_link-318">R</a></li>
                    <li class="calibre7"><a href="#calibre_link-318">S</a></li>
                    <li class="calibre7"><a href="#calibre_link-319">T</a></li>
                    <li class="calibre7"><a href="#calibre_link-319">U</a></li>
                    <li class="calibre7"><a href="#calibre_link-319">V</a></li>
                    <li class="calibre7"><a href="#calibre_link-319">W</a></li>
                    <li class="calibre7"><a href="#calibre_link-319">Y</a></li>
                </ul>
            </li>
        </ul>
        <p></p>


    </div>

</body>

</html>